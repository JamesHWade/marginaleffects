---
output: github_document
---

# `marginaleffects`

### Marginal effects using `R`, automatic differentiation, and the delta method

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![R-CMD-check](https://github.com/vincentarelbundock/fastmargins/workflows/R-CMD-check/badge.svg)](https://github.com/vincentarelbundock/fastmargins/actions)
<!-- badges: end -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

This package is still experimental. *Use with caution!*

## What?

The `marginaleffects` package allows `R` users to compute and plot "marginal effects" for a *wide* variety of models. 

A "marginal effect" is a measure of the association between a change in the regressors, and a change in the response variable. More formally, [the `margins` vignette](https://cran.r-project.org/web/packages/margins/index.html) defines "marginal effects" as follows:

> "Marginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data."

Marginal effects are extremely useful, because they are intuitive and easy to interpret. They are often the main quantity of interest in an empirical analysis. Unfortunately, they can be often be quite difficult to compute:

> In ordinary least squares regression with no interactions or higher-order term, the estimated slope coefficients are marginal effects. In other cases and for generalized linear models, the coefficients are not marginal effects at least not on the scale of the response variable.

To calculate marginal effects, we take derivatives of the regression equation. This can be challenging, especially when our models are non-linear, or when regressors are transformed or interacted. Computing the variance is even more difficult. The `marginaleffects` package hopes to do most of this hard work for you.

# Why?

Many `R` packages advertise their ability to compute "marginal effects." However, most of them do *not* actually compute marginal effects *as defined above* (the term is ambiguously defined in the statistical literature and used differently across fields). Instead, they compute related quantities such as "Estimated Marginal Means" or "Differences in Predicted Probabilities." The rare packages which actually compute marginal effects are typically limited in the model types they support, and in the range of transformations they allow (interactions, polynomials, etc.).

The main package in the `R` ecosystem to compute marginal effects is [the fantastic, trailblazing, and powerful `margins`](https://cran.r-project.org/web/packages/margins/index.html) by [Thomas J. Leeper.](https://thomasleeper.com/) The `marginaleffects` package is (essentially) a clone of `margins`. 

So why did I write a new package?

* _Speed:_ In a typical case shown below, `marginaleffects` is over 400x faster (55 seconds vs. 130 *milli*seconds).
* _Efficiency:_ Between 5 and 60x smaller memory footprint.
* _Extensibility:_ Adding support for new models often requires less than 10 lines of new code.
* `ggplot2` support: Plot your (conditional) marginal effects using `ggplot2`.
* _Tidy:_ The results produced by `marginaleffects` follow "tidy" principles. They are easy to process and program with. 
* _Active development_.

Downsides of `marginaleffects` include:

* No support for weights or simultation-based inference.
* Possibly fragile handling of transformations in model formulas (not sure).
* More dependencies.
* Newer package with a much smaller (i.e., non-existent) user base.

## How?

By using [the `numDeriv` package](https://cran.r-project.org/web/packages/numDeriv/index.html) to compute gradients and jacobians. That's it. That's the secret sauce.

## Supported models

This table shows the list of models supported by `marginaleffect`, and shows which numerical results have been checked against alternative software packages: Stata's `margins` command and R's `margins` package. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tibble)
library(kableExtra)

tab <- 
'"Model",           "Effect", "Variance", "Stata",      "margins"
"stats::lm",        "✓"      , "✓"        , "✓"         , "✓"
"stats::glm",       "✓"      , "✓"        , "✓"         , "✓"
"stats::loess",     "✓"      ,            ,             , "✓"
"AER::ivreg",       "✓"      , "✓"        , "✓"         , "✓"
"betareg::betareg", "✓"      , "✓"        , "✓"         , "✓"
"fixest::feols",    "✓"      , "✓"        ,             ,
"fixest::feglm",    "✓"      , "✓"        ,             ,
"ivreg::ivreg",     "✓"      , "✓"        , "✓"         , "✓"
"lme4::lmer",       "✓"      , "✓"        ,             , "dydx only"
"lme4::glmer",      "✓"      , "✓"        ,             , "dydx only"
"MASS::polr",       "✓"      ,            , "✓"         ,
"ordinal::clm",     "✓"      ,            ,             , "✓"
"survey::svyglm",   "✓"      , "✓"        ,             , "✓"
"survey::svyglm",   "✓"      , "✓"        ,             , "✓"'
tab <- read.csv(text = tab)
colnames(tab) <- c("Model", "Support: Effect", "Support: Std.Errors", "Validity: Stata", "Validity: margins")
kbl(tab, format = "pipe") %>% kable_styling() 
#%>% add_header_above(c(" " = 1, "Support" = 2, "Validity" = 2))
```

## Installation

You can install the latest version of `marginaleffects` from Github:

```{r, eval=FALSE}
remotes::install_github("vincentarelbundock/marginaleffects")
```

## Getting started

First, we load the library, download the [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/) data from the [`Rdatasets` archive](https://vincentarelbundock.github.io/Rdatasets/articles/data.html), and estimate a GLM model:

```{r}
library(marginaleffects)

dat <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv")
dat$large_penguin <- ifelse(dat$body_mass_g > median(dat$body_mass_g, na.rm = TRUE), 1, 0)

mod <- glm(large_penguin ~ bill_length_mm + flipper_length_mm + species, 
           data = dat, family = binomial)
```

The `marginaleffects` function computes a distinct estimate of the marginal effect and of the standard error for each regressor ("term"), for each unit of observation ("rowid"). You can browse view and manipulate the full results with functions like `head`, as you would any `data.frame`:

```{r}
mfx <- marginaleffects(mod)

head(mfx)
```

Notice that the results are presented in "tidy" format: each row of the original dataset gets a unique `rowid` value, each unit-level marginal effect appears on a distinct row, and metadata appears neatly in separate columns. This makes it easy to operate on the results programmatically.

We can obtain similar (but arguably messier) results with the `margins` package:

```{r}
library(margins)

mar <- margins(mod)
mar <- data.frame(mar)

head(mar, 2)
```

## Average Marginal Effects

A dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. Many analysts like to report the "Average Marginal Effect", that is, the average of all the observation-specific marginal effects. These are easy to compute based on the full `data.frame` shown above, but the `summary` function is convenient:

```{r}
summary(mfx)
```

If the `emmeans` package is installed, `summary.marginaleffects` will try to display contrasts for logical, factor, and character variables.

You can also extract average marginal effects using `tidy` and `glance` methods which conform to the [`broom` package specification](https://broom.tidymodels.org/):

```{r}
tidy(mfx)

glance(mfx)
```

## Typical Marginal Effects

Sometimes, we are not interested in *all* the unit-specific marginal effects, but would rather look at the estimated marginal effects for certain "typical" individuals. The `typical` function helps us build datasets full of "typical" rows. For example, to generate very unhappy individuals with or without kids: 

```{r}
typical(mod, 
        at = list(flipper_length_mm = 180, 
                  species = c("Adelie", "Gentoo")))
```

This dataset can then be used in `marginaleffects` to compute marginal effects for those (fictional) individuals:

```{r}
nd <- typical(mod, 
              at = list(flipper_length_mm = 180, 
                        species = c("Adelie", "Gentoo")))
marginaleffects(mod, newdata = nd)
```

When a variable is omitted from the `at` list, `typical` will automatically select the median (or mode) of the missing variable.

## Counterfactual Marginal Effects

The `typical` function allowed us look at completely fictional individual. The `counterfactual` lets us compute the marginal effects for the actual observations in our dataset, but with a few manipulated values. For example, this code will create a `data.frame` twice as long as the original `dat`, where each observation is repeated with different values of the `kids` variable:

```{r}
nd <- counterfactual(mod, at = list(flipper_length_mm = c(160, 180)))
```

We see that the rows 1, 2, and 3 of the original dataset have been replicated twice, with different values of the `kids` variable:

```{r}
nd[nd$rowid %in% 1:3,]
```

Again, we can use this to compute average (or median) marginal effects over the counterfactual individuals:

```{r, message=FALSE, warning=FALSE}
library(dplyr)

marginaleffects(mod, newdata = nd) %>%
    group_by(term) %>%
    summarize(across(dydx:std.error, median))
```

## Tables

Average marginal effects are easy to display in a regression table using packages like `modelsummary`:

```{r, echo = FALSE}
options(modelsummary_default = "markdown")
```

```{r}
library(modelsummary)

# fit models and store them in a named list
mod <- list(
    "Logit" = glm(large_penguin ~ flipper_length_mm, data = dat, family = binomial),
    "OLS" = lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = dat))

# apply the `marginaleffects` function to all the models using `lapply`
mfx <- lapply(mod, marginaleffects)

# build a table
modelsummary(mfx)
```

You can also display models with contrasts using `modelsummary`'s `group` argument:

```{r}
mod <- list(
    "Logit" = glm(large_penguin ~ species, data = dat, family = binomial),
    "OLS" = lm(body_mass_g ~ flipper_length_mm + species, data = dat))

mfx <- lapply(mod, marginaleffects)

modelsummary(mfx, group = term + contrast ~ model)
```

## Plots (`ggplot2`)

Since the output of the `marginaleffects` function is "tidy", it is very easy to use the `data.frame` that this function produces directly to draw plots with any drawing package you like. In addition, the `marginaleffects` package also offers functions to draw frequently used plots with `ggplot2`.

The first is a simple `plot` command to draw the average marginal effects:

```{r, out.width = "60%"}
mod <- lm(mpg ~ hp + wt + drat, data = mtcars)
mfx <- marginaleffects(mod)

plot(mfx)
```

The second is a `plot_cme` function to draw "Conditional Marginal Effects." This is useful when a model includes interaction terms and we want to plot how the marginal effect of a variable changes as the value of a "condition" (or "moderator") variable changes:

```{r, out.width = "60%"}
mod <- lm(mpg ~ hp * wt + drat, data = mtcars)

plot_cme(mod, effect = "hp", condition = "wt")
```

## Benchmarks

Here are two *very* naive benchmarks to compare the speed of `marginaleffects` and `margins`. Computing the unit-level marginal effects and standard errors in a logistic regression model with 1500 observations is over 400 times faster with `marginaleffects`. Calculating only the marginal effects is about 50% faster. 

Simulate data and fit model:

``` r
N <- 1500
dat <- data.frame(
    x2 = rnorm(N),
    x1 = rnorm(N),
    x3 = rnorm(N),
    x4 = rnorm(N),
    e = rnorm(N))
dat$y <- rbinom(N, 1, plogis(
    dat$x1 + dat$x2 + dat$x3 + dat$x4 + dat$x3 * dat$x4))

mod <- glm(y ~ x1 + x2 + x3 * x4, data = dat, family = binomial)
```

Marginal effects and standard errors:

``` r
b1 = bench::mark(
    margins::margins(mod, unit_ses = TRUE),
    marginaleffects(mod),
    check = FALSE,
    max_iterations = 3)

b1
#> # A tibble: 2 x 6
#>   expression                                  min   median `itr/sec` mem_alloc
#>   <bch:expr>                             <bch:tm> <bch:tm>     <dbl> <bch:byt>
#> 1 margins::margins(mod, unit_ses = TRUE)    54.6s    54.6s    0.0183    1.39GB
#> 2 marginaleffects(mod)                           123.4ms  129.5ms    7.60     21.83MB
#> # … with 1 more variable: gc/sec <dbl>
```

Marginal effects only:

``` r
b2 = bench::mark(
    margins::margins(mod, unit_ses = FALSE),
    marginaleffects(mod, variance = NULL),
    check = FALSE,
    max_iterations = 3)

b2
#> # A tibble: 2 x 6
#>   expression                                   min   median `itr/sec` mem_alloc
#>   <bch:expr>                              <bch:tm> <bch:tm>     <dbl> <bch:byt>
#> 1 margins::margins(mod, unit_ses = FALSE)   92.7ms  101.3ms      9.74   34.14MB
#> 2 marginaleffects(mod, variance = NULL)            66.1ms   67.7ms     14.3     5.22MB
#> # … with 1 more variable: gc/sec <dbl>
```

## Extending `marginaleffects` to support new models

In most cases, extending `marginaleffects` to support new models is easy. Imagine you want to add support for an object called "`model`" of class "`Example`" with N observations. 

#### *Step 1:* Check if `marginaleffects` default functions work:

```r
# returns a named vector of coefficients
get_coef(model)

# returns a named vector of predictions 
# returns a named matrix of size NxK for models with K levels (e.g., multinomial logit)
get_predict(model)

# returns a named square matrix of size equal to the number of coefficients
stats::vcov(model)

# returns a new model object with different stored coefficients 
# calling predict(model) and predict(model_new) should produce different results
model_new <- reset_coefs(model, rep(0, length(get_coef(model))))
```

If all of these functions work out-of-the-box, there's a good chance your model will be supported automatically. If they do *not* work, move to...

#### *Step 2:* Define the missing methods.

Find the class name of your model by calling:

```r
class(model)
```

Then, create functions (methods) called `get_coef.EXAMPLE`, `get_predict.EXAMPLE`, `vcov.EXAMPLE`, and `reset_coefs.EXAMPLE`, with the "EXAMPLE" replace by the name your model class.

#### *Step 3:* Add tests

Create a file called `tests/testthat/test-PKGNAME.R` and write a few tests. Ideally, we would like to compare the results obtained by `marginaleffect` to an external source, like the `margins` package or `Stata`.

#### *Step 4:* Finalize

Add your new model class to the lists of supported models:

* In the `sanity_dydx_model` function of the `R/sanity.R` file.
* In the supported models table of the `README.Rmd` file.
