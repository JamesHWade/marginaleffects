---
output: github_document
title: "fastmargins: Fast Marginal Effects"
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Proof of concept

This is just a proof of concept. Please do not use in serious applications.

# Installation

You can install the released version of fastmargins from Github:

```{r, eval=FALSE}
remotes::install_github("vincentarelbundock/fastmargins")
```

# Examples

## Logit regression with a multiplicative interaction term

Load the library, simulate data, and estimate a logistic regression model with interaction terms:

```{r}
library(fastmargins)

N <- 1000
dat <- data.frame(
    x2 = rnorm(N),
    x1 = rnorm(N),
    x3 = rnorm(N),
    x4 = rnorm(N),
    e = rnorm(N))
dat$y <- rbinom(N, 1, plogis(
    dat$x1 + dat$x2 + dat$x3 + dat$x4 + dat$x3 * dat$x4))

mod <- glm(y ~ x1 + x2 + x3 * x4, data = dat, family = binomial)

coef(mod)
```

Compute unit-level marginal effects and variances:

```{r}
res <- mfx(mod)
head(res)
```

Notice that the results are presented in `tidy` format: each row of the original dataset gets a unique `rowid` value, each unit-level marginal effect appears on a distinct row, and metadata appears neatly in separate columns. This makes it easy to operate on the results programmatically.

We can obtain similar results with the `margins` package, but the two packages use slightly different numerical approximation strategies, so the results will differ very slightly:

```{r}
library(margins)

mar <- margins(mod, unit_ses = TRUE)

head(data.frame(mar), 2)

cor(mar$dydx_x1, res[res$term == "x1", "dydx"])
cor(mar$SE_dydx_x1, res[res$term == "x1", "std.error"])

cor(mar$dydx_x3, res[res$term == "x3", "dydx"])
cor(mar$SE_dydx_x3, res[res$term == "x3", "std.error"])
```

## Conditional marginal effects with `ggplot2`

```{r, echo = FALSE}
library(ggplot2)
theme_set(theme_minimal())
```

We can use the `newdata` argument to do a "counterfactual" analysis:

```{r}

source("R/counterfactual.R")
tmp <- mtcars
tmp$gear <- as.factor(tmp$gear)
tmp$am <- as.logical(tmp$am)
mod <- lm(mpg ~ hp + drat + gear + am, tmp)

# error
nd <- counterfactual(mod, 
                     at = list(gear = "6", am = c(TRUE, FALSE)))
                    
# correct
nd <- counterfactual(mod, 
                     at = list(gear = 5, am = c(TRUE, FALSE)))


mfx(mod, newdata = nd) |> head()
```

The nice thing about the `tidy` output format is that we can pipe the output of the `mfx` function directly to `ggplot2`:

```{r}
library(tidyverse)

mfx(mod, newdata = counterfactuals) |>
    mutate(conf.low = dydx - 1.96 * std.error,
           conf.high = dydx + 1.96 * std.error) |>
    ggplot(aes(x = x4, 
               y = dydx, 
               ymin = conf.low, 
               ymax = conf.high)) +
    geom_ribbon(alpha = .1) +
    geom_line() + 
    facet_wrap(~term)
```

# Benchmarks

Here are a couple naive benchmarks to compare the speed of computation with the `fastmargins` and `margins` packages. Since unit-level standard errors can be expensive to compute, we run the benchmarks with and without standard errors.

## Marginal effects and standard errors (unit-level)

In this naive benchmark, computing marginal effects with their unit-level standard errors is over 300x faster.

```{r}
b1 = bench::mark(
    margins(mod, unit_ses = TRUE),
    mfx(mod, variance = vcov(mod)),
    check = FALSE,
    max_iterations = 3)
b1
```

## Marginal effects only

In this naive benchmark, computing marginal effects *without* their unit-level standard errors is over 40% faster.

```{r}
b2 = bench::mark(
    margins(mod, unit_ses = FALSE),
    mfx(mod, variance = NULL),
    check = FALSE,
    max_iterations = 3)
b2
```
