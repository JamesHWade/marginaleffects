@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}

@article{RafGre2020,
  title={Semantic and cognitive tools to aid statistical science: replace confidence and significance by compatibility and surprise},
  author={Rafi, Zad and Greenland, Sander},
  journal={BMC medical research methodology},
  volume={20},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@article{Rot2021,
  title={Rothman Responds to “Surprise!”},
  author={Rothman, Kenneth J},
  journal={American Journal of Epidemiology},
  volume={190},
  number={2},
  pages={194--195},
  year={2021},
  publisher={Oxford University Press}
}

@article{ColEdwGre2021,
  title={Surprise!},
  author={Cole, Stephen R and Edwards, Jessie K and Greenland, Sander},
  journal={American Journal of Epidemiology},
  volume={190},
  number={2},
  pages={191--193},
  year={2021},
  publisher={Oxford University Press}
}


 @article{AngBat2022, title={A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification}, url={http://arxiv.org/abs/2107.07511}, DOI={10.48550/arXiv.2107.07511}, abstractNote={Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.}, note={arXiv:2107.07511 [cs, math, stat]}, number={arXiv:2107.07511}, publisher={arXiv}, author={Angelopoulos, Anastasios N. and Bates, Stephen}, year={2022}, month={Sep} }
 