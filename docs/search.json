[
  {
    "objectID": "vignettes/comparisons.html",
    "href": "vignettes/comparisons.html",
    "title": "Comparisons",
    "section": "",
    "text": "In this vignette, we introduce “comparisons”, defined as:\n\nCompare the predictions made by a model for different regressor values (e.g., college graduates vs. others): contrasts, differences, risk ratios, odds, etc.\n\nThe comparisons() function is extremely flexible, and it allows users to estimate a vast array of quantities of interest. To describe those quantities, we will break the problem up in 4 steps:\n\nQuantity\nGrid\nAverage\nHypothesis\n\nThese steps can be combined and mixed and matched to define and compute many different estimands.\n\n\nConsider a logistic regression model estimated using the Titanic mortality data:\n\nlibrary(marginaleffects)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ndat$PClass[dat$PClass == \"*\"] &lt;- NA\nmod &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\n\n\nThe question that interests us is:\n\nHow does the probability of survival (outcome) change if a passenger travels in 1st class vs. 3rd class?\n\nSince we are comparing two predicted outcomes, we will use comparisons(). To indicate that our focal variable is PClass and that we are interested in the comparison between 1st and 3rd class, we will use the variables argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n\n\n\n\nIn GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above.\nBy default, comparisons() will compute estimates for every row of the original dataset that was used to fit a model. There are 1313 observations in the titanic dataset. Therefore, if we just execute the code in the previous section, we will obtain 1313 estimates of the difference between the probability of survival in 3rd and 1st class:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.496     0.0610 -8.13  &lt; 0.001 51.0 -0.616 -0.376\n#&gt;  PClass 3rd - 1st   -0.472     0.1247 -3.79  &lt; 0.001 12.7 -0.716 -0.228\n#&gt;  PClass 3rd - 1st   -0.353     0.0641 -5.51  &lt; 0.001 24.7 -0.478 -0.227\n#&gt;  PClass 3rd - 1st   -0.493     0.0583 -8.45  &lt; 0.001 55.0 -0.607 -0.379\n#&gt;  PClass 3rd - 1st   -0.445     0.1452 -3.07  0.00216  8.9 -0.730 -0.161\n#&gt; --- 746 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;  PClass 3rd - 1st   -0.377     0.0703 -5.36  &lt; 0.001 23.5 -0.515 -0.239\n#&gt;  PClass 3rd - 1st   -0.384     0.0726 -5.30  &lt; 0.001 23.0 -0.527 -0.242\n#&gt;  PClass 3rd - 1st   -0.412     0.0821 -5.02  &lt; 0.001 20.9 -0.573 -0.251\n#&gt;  PClass 3rd - 1st   -0.399     0.0773 -5.16  &lt; 0.001 22.0 -0.550 -0.247\n#&gt;  PClass 3rd - 1st   -0.361     0.0661 -5.47  &lt; 0.001 24.4 -0.490 -0.232\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Survived, PClass, SexCode, Age \n#&gt; Type:  response\n\nNotice that the contrast between 3rd and 1st is different from row to row. This reflects the fact that, in our model, moving from 1st to 3rd would have a different effect on the predicted probability of survival for different individuals.\nWe can be more specific in our query. Instead of using the empirical distribution as our “grid”, we can specify exactly where we want to evaluate the comparison in the predictor space, by using the newdata argument and the datagrid() function. For example, say I am interested in:\n\nThe effect of moving from 1st to 3rd class on the probability of survival for a 50 year old man and a 50 year old woman.\n\nI can type:\n\ncmp &lt;- comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\ncmp\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nWe now know that moving from 1st to 3rd changes by -0.184 the probability of survival for 50 year old men (SexCode=0), and by -0.511 the probability of survival for 50 year old women (SexCode=1).\n\n\n\nAgain, by default comparisons() estimates quantities for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an “average contrast”:\n\navg_comparisons(mod,                          # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.396     0.0425 -9.3   &lt;0.001 66.0 -0.479 -0.312\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nAlternatively, we could also take the average, but just of the two estimates that we computed above for the 50 year old man and 50 year old woman.\n\navg_comparisons(mod,                           # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.348     0.0676 -5.15   &lt;0.001 21.8 -0.48 -0.215\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that this is exactly the same as the average in the estimates from the previous section, which we had stored as cmp:\n\ncmp$estimate\n#&gt; [1] -0.1844289 -0.5113098\n\nmean(cmp$estimate)\n#&gt; [1] -0.3478694\n\n\n\n\nFinally, imagine we are interested in this question:\n\nDoes moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?\n\nTo answer this, we use the hypothesis argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1), # Step 2: Grid\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result maps directly onto the estimates we had above. It is the difference in the contrast for 50-men and 50-women:\n\ndiff(cmp$estimate)\n#&gt; [1] -0.3268809\n\nThis result can be interpreted as a “difference-in-differences”: Moving from 1st to 3rd has a much larger negative effect on the probability of survival for a 50 year old woman than for a 50 year old man. This difference is statistically significant.\nWe can do a similar comparison, but instead of fixing a conditional grid, we can average over subgroups of the empirical distribution, using the by argument:\n\navg_comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  by = \"SexCode\",                              # Step 3: Average\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  b1=b2    0.162     0.0845 1.91   0.0558 4.2 -0.00402  0.327\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nNow we show how to use the base R predict() function to compute some of the same quantities as above. This exercise may be clarifying for some users.\n\ngrid_50_1_3 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"3rd\")\ngrid_50_1_1 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"1st\")\ngrid_50_0_3 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"3rd\")\ngrid_50_0_1 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"1st\")\n\n\nyhat_50_1_3 &lt;- predict(mod, newdata = grid_50_1_3, type = \"response\")\nyhat_50_1_1 &lt;- predict(mod, newdata = grid_50_1_1, type = \"response\")\nyhat_50_0_3 &lt;- predict(mod, newdata = grid_50_0_3, type = \"response\")\nyhat_50_0_1 &lt;- predict(mod, newdata = grid_50_0_1, type = \"response\")\n\n## prediction on a grid\npredictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = \"3rd\"))\n#&gt; \n#&gt;  Age SexCode PClass Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   50       1    3rd    0.446    0.661 0.6 0.235  0.679\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, Survived, Age, SexCode, PClass \n#&gt; Type:  invlink(link)\nyhat_50_1_3\n#&gt;         1 \n#&gt; 0.4463379\n\n## contrast on a grid\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1))\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nyhat_50_0_3 - yhat_50_0_1\n#&gt;          1 \n#&gt; -0.1844289\nyhat_50_1_3 - yhat_50_1_1\n#&gt;          1 \n#&gt; -0.5113098\n\n## difference-in-differences \ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1),\n  hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)\n#&gt;         1 \n#&gt; 0.3268809\n\n## average of the empirical distribution of contrasts\navg_comparisons(mod, variables = list(PClass = c(\"1st\", \"3rd\")), by = \"SexCode\")\n#&gt; \n#&gt;    Term              Contrast SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass mean(3rd) - mean(1st)       0   -0.334     0.0570 -5.86   &lt;0.001 27.7 -0.446 -0.222\n#&gt;  PClass mean(3rd) - mean(1st)       1   -0.496     0.0623 -7.95   &lt;0.001 49.0 -0.618 -0.374\n#&gt; \n#&gt; Columns: term, contrast, SexCode, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\ngrid_empirical_1_3 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_1_1 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"1st\")\ngrid_empirical_0_3 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_0_1 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"1st\")\nyhat_empirical_0_1 &lt;- predict(mod, newdata = grid_empirical_0_1, type = \"response\")\nyhat_empirical_0_3 &lt;- predict(mod, newdata = grid_empirical_0_3, type = \"response\")\nyhat_empirical_1_1 &lt;- predict(mod, newdata = grid_empirical_1_1, type = \"response\")\nyhat_empirical_1_3 &lt;- predict(mod, newdata = grid_empirical_1_3, type = \"response\")\nmean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)\n#&gt; [1] -0.3341426\nmean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)\n#&gt; [1] -0.4956673\n\n\n\n\n\n\n\nConsider a simple model with a logical and a factor variable:\n\nlibrary(marginaleffects)\n\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\n\nThe comparisons function automatically computes contrasts for each level of the categorical variables, relative to the baseline category (FALSE for logicals, and the reference level for factors), while holding all other values at their observed values. The avg_comparisons() does the same, but then marginalizes by taking the average of unit-level estimates:\n\ncmp &lt;- avg_comparisons(mod)\ncmp\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 % 97.5 %\n#&gt;   am  TRUE - FALSE     2.56       1.30  1.97   0.0485  4.4   0.0167   5.10\n#&gt;   cyl 6 - 4           -6.16       1.54 -4.01   &lt;0.001 14.0  -9.1661  -3.15\n#&gt;   cyl 8 - 4          -10.07       1.45 -6.93   &lt;0.001 37.8 -12.9136  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe summary printed above says that moving from the reference category 4 to the level 6 on the cyl factor variable is associated with a change of -6.156 in the adjusted prediction. Similarly, the contrast from FALSE to TRUE on the am variable is equal to 2.560.\nWe can obtain different contrasts by using the comparisons() function. For example:\n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0 -9.17  -3.15\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93  &lt; 0.001 37.8 -12.91  -7.22\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01   &lt;0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93   &lt;0.001 37.8 -12.91  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nFor comparison, this code produces the same results using the emmeans package:\n\nlibrary(emmeans)\nemm &lt;- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast    estimate   SE df t.ratio p.value\n#&gt;  cyl6 - cyl4    -6.16 1.54 28  -4.009  0.0012\n#&gt;  cyl8 - cyl4   -10.07 1.45 28  -6.933  &lt;.0001\n#&gt;  cyl8 - cyl6    -3.91 1.47 28  -2.660  0.0331\n#&gt; \n#&gt; Results are averaged over the levels of: am \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\nemm &lt;- emmeans(mod, specs = \"am\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast     estimate  SE df t.ratio p.value\n#&gt;  TRUE - FALSE     2.56 1.3 28   1.973  0.0585\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n\nNote that these commands also work on for other types of models, such as GLMs, on different scales:\n\nmod_logit &lt;- glm(am ~ factor(gear), data = mtcars, family = binomial)\n\navg_comparisons(mod_logit)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  gear    4 - 3    0.667   1.36e-01 4.9e+00   &lt;0.001 20.0   0.4  0.933\n#&gt;  gear    5 - 3    1.000   9.95e-06 1.0e+05   &lt;0.001  Inf   1.0  1.000\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod_logit, type = \"link\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  gear    4 - 3     21.3       4578 0.00464    0.996 0.0  -8951   8994\n#&gt;  gear    5 - 3     41.1       9156 0.00449    0.996 0.0 -17904  17986\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n\n\n\nFor categorical predictors, Scholbeck et al. 2023 recommend that analysts report what they call the “observation-wise categorical marginal effects.” They describe the procedure as follows:\n\nRecall that the common definition of categorical MEs is based on first changing all observations’ value of \\(x_j\\) to each category and then computing the difference in predictions when changing it to the reference category. However, one is often interested in prediction changes if aspects of an actual observation change. We therefore propose an observation-wise categorical ME. We first select a single reference category \\(c_h\\). For each observation whose feature value \\(x_j \\neq c_h\\), we predict once with the observed value \\(x_j\\) and once where \\(x_j\\) has been replaced by \\(c_h\\).\n\nTo achieve this with marginaleffects, we proceed in three simple steps:\n\nUse the factor() function to set the reference level of the categorical variable.\nUse the newdata argument to take the subset of data where the observed \\(x_j\\) is different from the reference level we picked in 1.\nApply the avg_comparisons() with the \"revreference\" option.\n\n\ndat &lt;- transform(mtcars, cyl = factor(cyl, levels = c(6, 4, 8)))\n\nmod &lt;- glm(vs ~ mpg * factor(cyl), data = dat, family = binomial)\n\navg_comparisons(mod,\n  variables = list(cyl = \"revreference\"),\n  newdata = subset(dat, cyl != 6))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4   -0.323     0.2170 -1.49    0.137   2.9 -0.748  0.103\n#&gt;   cyl    6 - 8    0.561     0.0357 15.69   &lt;0.001 181.9  0.491  0.631\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\n\nAll functions of the marginaleffects package attempt to treat character predictors as factor predictors. However, using factors instead of characters when modeling is strongly encouraged, because they are much safer and faster. This is because factors hold useful information about the full list of levels, which makes them easier to track and handle internally by marginaleffects. Users are strongly encouraged to convert their character variables to factor before fitting their models and using slopes functions.\n\n\n\nWe can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the hp variable by 1 unit (default) or by 5 units about the original value:\n\nmod &lt;- lm(mpg ~ hp, data = mtcars)\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp       +1  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = 5))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp       +5   -0.341     0.0506 -6.74   &lt;0.001 35.9 -0.44 -0.242\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions for a change in the regressor between two arbitrary values:\n\navg_comparisons(mod, variables = list(hp = c(90, 110)))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp 110 - 90    -1.36      0.202 -6.74   &lt;0.001 35.9 -1.76 -0.968\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:\n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp  Q3 - Q1     -5.7      0.845 -6.74   &lt;0.001 35.9 -7.35  -4.04\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n#&gt; \n#&gt;  Term                Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   &lt;0.001 35.9 -6.04  -3.32\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"2sd\"))\n#&gt; \n#&gt;  Term            Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd) - (x - sd)    -9.36       1.39 -6.74   &lt;0.001 35.9 -12.1  -6.64\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n#&gt; \n#&gt;  Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp Max - Min    -19.3       2.86 -6.74   &lt;0.001 35.9 -24.9  -13.7\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\n\nIn some contexts we are interested in whether the “effect” of a variable changes, as a function of another variable. A very simple strategy to tackle this question is to estimate a model with a multiplicative interaction like this one:\n\nmod &lt;- lm(mpg ~ am * factor(cyl), data = mtcars)\n\nCalling avg_comparisons() with the by argument shows that the estimated comparisons differ based on cyl:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\")\n#&gt; \n#&gt;  Term          Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;    am mean(1) - mean(0)   4     5.18       2.05 2.521   0.0117 6.4  1.15   9.20\n#&gt;    am mean(1) - mean(0)   6     1.44       2.32 0.623   0.5336 0.9 -3.10   5.98\n#&gt;    am mean(1) - mean(0)   8     0.35       2.32 0.151   0.8799 0.2 -4.19   4.89\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nHowever, using the hypothesis argument for pairwise contrasts between the above comparisons reveals that the heterogeneity is not statistically significant:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  4 - 6     3.73       3.09 1.206    0.228 2.1 -2.33   9.80\n#&gt;  4 - 8     4.82       3.09 1.559    0.119 3.1 -1.24  10.89\n#&gt;  6 - 8     1.09       3.28 0.333    0.739 0.4 -5.33   7.51\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nIn other contexts, we are interested in a “cross-contrast” or “cross-comparisons”; we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the variables argument, and set the cross=TRUE:\n\navg_comparisons(mod, variables = c(\"cyl\", \"am\"), cross = TRUE)\n#&gt; \n#&gt;  Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 % C: am C: cyl\n#&gt;     -2.33       2.48 -0.942  0.34596 1.5  -7.19   2.52 1 - 0  6 - 4\n#&gt;     -7.50       2.77 -2.709  0.00674 7.2 -12.93  -2.07 1 - 0  8 - 4\n#&gt; \n#&gt; Columns: term, contrast_am, contrast_cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nThis section compares 4 quantities:\n\nUnit-Level Contrasts\nAverage Contrast\nContrast at the Mean\nContrast Between Marginal Means\n\nThe ideas discussed in this section focus on contrasts, but they carry over directly to analogous types of marginal effects.\n\n\nIn models with interactions or non-linear components (e.g., link function), the value of a contrast or marginal effect can depend on the value of all the predictors in the model. As a result, contrasts and marginal effects are fundamentally unit-level quantities. The effect of a 1 unit increase in \\(X\\) can be different for Mary or John. Every row of a dataset has a different contrast and marginal effect.\nThe mtcars dataset has 32 rows, so the comparisons() function produces 32 contrast estimates:\n\nlibrary(marginaleffects)\nmod &lt;- glm(vs ~ factor(gear) + mpg, family = binomial, data = mtcars)\ncmp &lt;- comparisons(mod, variables = \"mpg\")\nnrow(cmp)\n#&gt; [1] 32\n\n\n\n\nBy default, the slopes() and comparisons() functions compute marginal effects and contrasts for every row of the original dataset. These unit-level estimates can be of great interest, as discussed in another vignette. Nevertheless, one may want to focus on one-number summaries: the avg_*() functions or the by argument compute the “Average Marginal Effect” or “Average Contrast,” by taking the mean of all the unit-level estimates.\n\navg_comparisons(mod, variables = \"mpg\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, variables = \"mpg\", by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nwhich are equivalent to:\n\nmean(cmp$estimate)\n#&gt; [1] 0.06091377\n\nWe could also show the full distribution of contrasts across our dataset with a histogram:\n\nlibrary(ggplot2)\n\ncmp &lt;- comparisons(mod, variables = \"gear\")\n\nggplot(cmp, aes(estimate)) +\n    geom_histogram(bins = 30) +\n    facet_wrap(~contrast, scale = \"free_x\") +\n    labs(x = \"Distribution of unit-level contrasts\")\n\n\n\n\nThis graph displays the effect of a change of 1 unit in the gear variable, for each individual in the observed data.\n\n\n\nAn alternative which used to be very common but has now fallen into a bit of disfavor is to compute “Contrasts at the mean.” The idea is to create a “synthetic” or “hypothetical” individual (row of the dataset) whose characteristics are completely average. Then, we compute and report the contrast for this specific hypothetical individual.\nThis can be achieved by setting newdata=\"mean\" or to newdata=datagrid(), both of which fix variables to their means or modes:\n\ncomparisons(mod, variables = \"mpg\", newdata = \"mean\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % gear  mpg\n#&gt;   mpg       +1    0.155     0.0539 2.88  0.00399 8.0 0.0495  0.261    3 20.1\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, gear, mpg \n#&gt; Type:  response\n\nContrasts at the mean can differ substantially from average contrasts.\nThe advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.\n\n\n\nYet another type of contrast is the “Contrast between marginal means.” This type of contrast is closely related to the “Contrast at the mean”, with a few wrinkles. It is the default approach used by the emmeans package for R.\nRoughly speaking, the procedure is as follows:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nThe contrast obtained through this approach has two critical characteristics:\n\nIt is the contrast for a synthetic individual with perfectly average qualities on every (numeric) predictor.\nIt is a weighted average of unit-level contrasts, where weights assume a perfectly balanced dataset across every categorical predictor.\n\nWith respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted as if the data were perfectly balanced?\nFor example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.\nIf the contrasts between marginal means is really the quantity of interest, it is easy to use the comparisons() to estimate contrasts between marginal means. The newdata determines the values of the predictors at which we want to compute contrasts. We can set newdata=\"marginalmeans\" to emulate the emmeans behavior. For example, here we compute contrasts in a model with an interaction:\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\nmod &lt;- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)\n\navg_comparisons(\n    mod,\n    newdata = \"marginalmeans\",\n    variables = c(\"species\", \"island\"))\n#&gt; \n#&gt;     Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  island  Dream - Biscoe       -0.434      0.451 -0.962    0.336   1.6 -1.318  0.450\n#&gt;  island  Torgersen - Biscoe    0.060      0.467  0.128    0.898   0.2 -0.856  0.976\n#&gt;  species Chinstrap - Adelie   10.563      0.418 25.272   &lt;0.001 465.7  9.744 11.382\n#&gt;  species Gentoo - Adelie       5.792      0.798  7.257   &lt;0.001  41.2  4.228  7.356\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWhich is equivalent to this in emmeans:\n\nemm &lt;- emmeans(\n    mod,\n    specs = c(\"species\", \"island\"))\ncontrast(emm, method = \"trt.vs.ctrl1\")\n#&gt;  contrast                            estimate    SE  df t.ratio p.value\n#&gt;  Chinstrap Biscoe - Adelie Biscoe      nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Biscoe - Adelie Biscoe          5.792 0.798 331   7.257  &lt;.0001\n#&gt;  Adelie Dream - Adelie Biscoe          -0.434 0.451 331  -0.962  0.7573\n#&gt;  Chinstrap Dream - Adelie Biscoe       nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Dream - Adelie Biscoe           5.358 1.035 331   5.177  &lt;.0001\n#&gt;  Adelie Torgersen - Adelie Biscoe       0.060 0.467 331   0.128  0.9987\n#&gt;  Chinstrap Torgersen - Adelie Biscoe   nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Torgersen - Adelie Biscoe       5.852 1.070 331   5.468  &lt;.0001\n#&gt; \n#&gt; Results are averaged over the levels of: sex \n#&gt; P value adjustment: dunnettx method for 5 tests\n\nThe emmeans section of the Alternative Software vignette shows further examples.\nThe excellent vignette of the emmeans package discuss the same issues in a slightly different (and more positive) way:\n\nThe point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.\n\n\nAll this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace.\n\n\n\n\n\nConsider a model with an interaction term. What happens to the dependent variable when the hp variable increases by 10 units?\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt, data = mtcars)\n\nplot_comparisons(\n    mod,\n    variables = list(hp = 10),\n    condition = \"wt\")\n\n\n\n\n\n\n\nSo far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:\n\ncomparison\ntransform\n\nConsider the case of a model with a single predictor \\(x\\). To compute average contrasts, we proceed as follows:\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x\\): \\(\\hat{y}_x\\)\nCompute adjusted predictions for each row of the dataset for the observed values \\(x + 1\\): \\(\\hat{y}_{x+1}\\)\ncomparison: Compute unit-level contrasts by taking the difference between (or some other function of) adjusted predictions: \\(\\hat{y}_{x+1} - \\hat{y}_x\\)\nCompute the average contrast by taking the mean of unit-level contrasts: \\(1/N \\sum_{i=1}^N \\hat{y}_{x+1} - \\hat{y}_x\\)\ntransform: Transform the average contrast or return them as-is.\n\nThe comparison argument of the comparisons() function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with hi value of \\(x\\), and predictions with a lo value of \\(x\\): function(hi, lo) hi-lo.\nThe transform argument of the comparisons() function applies a custom transformation to the unit-level contrasts.\nThe transform argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without transform.\n\n\n\nThe default contrast calculate by the comparisons() function is a (untransformed) difference between two adjusted predictions. For instance, to estimate the effect of a change of 1 unit, we do:\n\nlibrary(marginaleffects)\n\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\n\n## construct data\n\nmtcars_minus &lt;- mtcars_plus &lt;- mtcars\nmtcars_minus$mpg &lt;- mtcars_minus$mpg - 0.5\nmtcars_plus$mpg &lt;- mtcars_plus$mpg + 0.5\n\n## adjusted predictions\nyhat_minus &lt;- predict(mod, newdata = mtcars_minus, type = \"response\")\nyhat_plus &lt;- predict(mod, newdata = mtcars_plus, type = \"response\")\n\n## unit-level contrasts\ncon &lt;- yhat_plus - yhat_minus\n\n## average contrasts\nmean(con)\n#&gt; [1] 0.05540227\n\nWe can use the avg_comparisons() function , or the by argument to obtain the same results:\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nGoing back to our Titanic example:\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\nIn this case, a contrast is a difference between predicted probabilities. We can compute that contrast for different types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Contrast PClass Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % SexCode  Age\n#&gt;  SexCode    1 - 0    1st    0.483     0.0631 7.65   &lt;0.001 45.5 0.359  0.606   0.381 30.4\n#&gt;  SexCode    1 - 0    3rd    0.335     0.0634 5.29   &lt;0.001 22.9 0.211  0.459   0.381 30.4\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, predicted_lo, predicted_hi, predicted, Survived, SexCode, Age \n#&gt; Type:  response\n\nOne we can notice above, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. Being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?\nTo answer this question, we can compute a difference-in-difference using the hypothesis argument (see the Hypothesis vignette for details). For example, using b1 and b2 to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:\n\ncomparisons(\n  titanic,\n  hypothesis = \"b1 - b2 = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  b1-b2=0    0.148     0.0894 1.65   0.0987 3.3 -0.0276  0.323\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNow, let’s say we consider more types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;     Term Contrast PClass   Age Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 % 97.5 % SexCode\n#&gt;  SexCode    1 - 0    1st  0.17   0.1081      0.122  0.883   0.3774   1.4 -0.1319  0.348   0.381\n#&gt;  SexCode    1 - 0    1st 71.00   0.8795      0.057 15.437   &lt;0.001 176.2  0.7679  0.991   0.381\n#&gt;  SexCode    1 - 0    3rd  0.17   0.0805      0.157  0.513   0.6081   0.7 -0.2272  0.388   0.381\n#&gt;  SexCode    1 - 0    3rd 71.00   0.4265      0.203  2.101   0.0356   4.8  0.0287  0.824   0.381\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, Age, predicted_lo, predicted_hi, predicted, Survived, SexCode \n#&gt; Type:  response\n\nWith these results, we could compute a triple difference:\n\ncomparisons(\n  titanic,\n  hypothesis = \"(b1 - b3) - (b2 - b4) = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;               Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b1-b3)-(b2-b4)=0   -0.425      0.359 -1.19    0.236 2.1 -1.13  0.278\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nInstead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, the adjrr function the Stata software package can compute “adjusted risk ratios”, which are ratios of adjusted predictions. To do this in R, we use the comparison argument:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result is the average adjusted risk ratio for an increment of 1, that is, the adjusted predictions when the mpg are incremented by 0.5, divided by the adjusted predictions when mpg is decremented by 0.5.\nThe comparison accepts different values for common types of contrasts: ‘difference’, ‘ratio’, ‘lnratio’, ‘ratioavg’, ‘lnratioavg’, ‘lnoravg’, ‘differenceavg’. These strings are shortcuts for functions that accept two vectors of adjusted predictions and returns a single vector of contrasts. For example, these two commands yield identical results:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, comparison = function(hi, lo) hi / lo)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:\n\navg_comparisons(mod, comparison = function(hi, lo) sqrt(hi) / log(lo + 10))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1    0.275     0.0252 10.9   &lt;0.001 89.4 0.225  0.324\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe same arguments work in the plotting function plot_comparisons() as well, which allows us to plot various custom contrasts. Here is a comparison of Adjusted Risk Ratio and Adjusted Risk Difference in a model of the probability of survival aboard the Titanic:\n\nlibrary(ggplot2)\nlibrary(patchwork)\ntitanic &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ntitanic &lt;- read.csv(titanic)\nmod_titanic &lt;- glm(\n    Survived ~ Sex * PClass + Age + I(Age^2),\n    family = binomial,\n    data = titanic)\n\navg_comparisons(mod_titanic)\n#&gt; \n#&gt;    Term      Contrast Estimate Std. Error     z Pr(&gt;|z|)     S    2.5 %  97.5 %\n#&gt;  Age    +1            -0.00639    0.00107  -6.0   &lt;0.001  28.9 -0.00848 -0.0043\n#&gt;  PClass 2nd - 1st     -0.20578    0.03954  -5.2   &lt;0.001  22.3 -0.28328 -0.1283\n#&gt;  PClass 3rd - 1st     -0.40428    0.03958 -10.2   &lt;0.001  78.9 -0.48187 -0.3267\n#&gt;  Sex    male - female -0.48468    0.03004 -16.1   &lt;0.001 192.2 -0.54355 -0.4258\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np1 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survival | Age + 1) / P(Survival | Age)\")\n\np2 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\") +\n    ylab(\"Adjusted Risk Difference\\nP(Survival | Age + 1) - P(Survival | Age)\")\n\np1 + p2\n\n\n\n\nBy default, the standard errors around contrasts are computed using the delta method on the scale determined by the type argument (e.g., “link” or “response”). Some analysts may prefer to proceed differently. For example, in Stata, the adjrr computes adjusted risk ratios (ARR) in two steps:\n\nCompute the natural log of the ratio between the mean of adjusted predictions with \\(x+1\\) and the mean of adjusted predictions with \\(x\\).\nExponentiate the estimate and confidence interval bounds.\n\nStep 1 is easy to achieve with the comparison argument described above. Step 2 can be achieved with the transform argument:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that we can use the lnratioavg shortcut instead of defining the function ourselves.\nThe order of operations in previous command was:\n\nCompute the custom unit-level log ratios\nExponentiate them\nTake the average using the avg_comparisons()\n\nThere is a very subtle difference between the procedure above and this code:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nSince the exp function is now passed to the transform argument of the comparisons() function, the exponentiation is now done only after unit-level contrasts have been averaged. This is what Stata appears to do under the hood, and the results are slightly different.\n\ncomparisons(\n    mod,\n    comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nNote that equivalent results can be obtained using shortcut strings in the comparison argument: “ratio”, “lnratio”, “lnratioavg”.\n\ncomparisons(\n    mod,\n    comparison = \"lnratioavg\",\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg mean(+1)     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nAll the same arguments apply to the plotting functions of the marginaleffects package as well. For example we can plot the Adjusted Risk Ratio in a model with a quadratic term:\n\nlibrary(ggplot2)\ndat_titanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\nmod2 &lt;- glm(Survived  ~ Age, data = dat_titanic, family = binomial)\nplot_comparisons(\n    mod2,\n    variables = list(\"Age\" = 10),\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survived = 1 | Age + 10) / P(Survived = 1 | Age)\")\n\n\n\n\n\n\n\nBy default, the comparisons() function computes a “forward” difference. For example, if we ask comparisons() to estimate the effect of a 10-unit change in predictor x on outcome y, comparisons() will compare the predicted values with x and x+10.\n\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (mtcars$hp - min(mtcars$hp)) / (max(mtcars$hp) - min(mtcars$hp)) + 1\nmod &lt;- lm(mpg ~ log(new_hp), data = dat)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = 10))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp      +10     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:\n\nforward_diff &lt;- \\(x) data.frame(x, x + 10)\nbackward_diff &lt;- \\(x) data.frame(x - 10, x)\ncenter_diff &lt;- \\(x) data.frame(x - 5, x + 5)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = forward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = backward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -6.51      0.744 -8.74   &lt;0.001 58.6 -7.97  -5.05\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = center_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -4.06      0.464 -8.74   &lt;0.001 58.6 -4.97  -3.15\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that the last “centered” difference gives the same results as the default comparisons() call.\n\n\n\nWith hurdle models, we can fit two separate models simultaneously:\n\nA model that predicts if the outcome is zero or not zero\nIf the outcome is not zero, a model that predicts what the value of the outcome is\n\nWe can calculate predictions and marginal effects for each of these hurdle model processes, but doing so requires some variable transformation since the stages of these models use different link functions.\nThe hurdle_lognormal() family in brms uses logistic regression (with a logit link) for the hurdle part of the model and lognormal regression (where the outcome is logged before getting used in the model) for the non-hurdled part. Let’s look at an example of predicting GDP per capita (which is distributed exponentially) using life expectancy. We’ll add some artificial zeros so that we can work with a hurdle stage of the model.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(gapminder)\n\n## Build some 0s into the GDP column\nset.seed(1234)\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap0 = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero)\n\nmod &lt;- brm(\n  bf(gdpPercap0 ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = 4, cores = 4, seed = 1234)\n\nWe have two different sets of coefficients here for the two different processes. The hurdle part (hu) uses a logit link, and the non-hurdle part (mu) uses an identity link. However, that’s a slight misnomer—a true identity link would show the coefficients on a non-logged dollar value scale. Because we’re using a lognormal family, GDP per capita is pre-logged, so the “original” identity scale is actually logged dollars.\n\nsummary(mod)\n\n\n#&gt;  Family: hurdle_lognormal \n#&gt;   Links: mu = identity; sigma = identity; hu = logit \n#&gt; Formula: gdpPercap0 ~ lifeExp \n#&gt;          hu ~ lifeExp\n#&gt;    Data: gapminder (Number of observations: 1680) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept        3.47      0.09     3.29     3.65 1.00     4757     3378\n#&gt; hu_Intercept     3.16      0.40     2.37     3.96 1.00     2773     2679\n#&gt; lifeExp          0.08      0.00     0.08     0.08 1.00     5112     3202\n#&gt; hu_lifeExp      -0.10      0.01    -0.12    -0.08 1.00     2385     2652\n#&gt; ...\n\nWe can get predictions for the hu part of the model on the link (logit) scale:\n\npredictions(mod, dpar = \"hu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40   -0.817 -1.03 -0.604\n#&gt;       60   -2.805 -3.06 -2.555\n#&gt;       80   -4.790 -5.34 -4.275\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\n\n…or on the response (percentage point) scale:\n\npredictions(mod, dpar = \"hu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate   2.5 % 97.5 %\n#&gt;       40  0.30630 0.26231 0.3534\n#&gt;       60  0.05703 0.04466 0.0721\n#&gt;       80  0.00824 0.00478 0.0137\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can also get slopes for the hu part of the model on the link (logit) or response (percentage point) scales:\n\nslopes(mod, dpar = \"hu\", type = \"link\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp Estimate  2.5 %  97.5 %\n#&gt;  lifeExp      40  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      60  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      80  -0.0993 -0.116 -0.0837\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  link\n\nslopes(mod, dpar = \"hu\", type = \"response\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp  Estimate    2.5 %    97.5 %\n#&gt;  lifeExp      40 -0.021080 -0.02592 -0.016590\n#&gt;  lifeExp      60 -0.005322 -0.00615 -0.004562\n#&gt;  lifeExp      80 -0.000812 -0.00115 -0.000543\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWorking with the mu part of the model is trickier. Switching between type = \"link\" and type = \"response\" doesn’t change anything, since the outcome is pre-logged:\n\npredictions(mod, dpar = \"mu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\npredictions(mod, dpar = \"mu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nFor predictions, we need to exponentiate the results to scale them back up to dollar amounts. We can do this by post-processing the results (e.g. with dplyr::mutate(predicted = exp(predicted))), or we can use the transform argument in predictions() to pass the results to exp() after getting calculated:\n\npredictions(mod, dpar = \"mu\", \n            newdata = datagrid(lifeExp = seq(40, 80, 20)),\n            transform = exp)\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40      744   694    801\n#&gt;       60     3581  3449   3718\n#&gt;       80    17215 16110  18410\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can pass transform = exp to plot_predictions() too:\n\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"link\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       title = \"Hurdle part (hu)\",\n       subtitle = \"Logit-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"response\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       subtitle = \"Percentage point-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       title = \"Non-hurdle part (mu)\",\n       subtitle = \"Log-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  transform = exp,\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       subtitle = \"Dollar-scale predictions\")\n\n\n\n\nFor marginal effects, we need to transform the predictions before calculating the instantaneous slopes. We also can’t use the slopes() function directly—we need to use comparisons() and compute the numerical derivative ourselves (i.e. predict gdpPercap at lifeExp of 40 and 40.001 and calculate the slope between those predictions). We can use the comparison argument to pass the pair of predicted values to exp() before calculating the slopes:\n\n## step size of the numerical derivative\neps &lt;- 0.001\n\ncomparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  # rescale the elements of the slope\n  # (exp(40.001) - exp(40)) / exp(0.001)\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps\n)\n#&gt; \n#&gt;     Term Contrast lifeExp Estimate  2.5 % 97.5 %\n#&gt;  lifeExp   +0.001      40     58.4   55.8     61\n#&gt;  lifeExp   +0.001      60    280.9  266.6    296\n#&gt;  lifeExp   +0.001      80   1349.5 1222.6   1490\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWe can visually confirm that these are the instantaneous slopes at each of these levels of life expectancy:\n\npredictions_data &lt;- predictions(\n  mod,\n  newdata = datagrid(lifeExp = seq(30, 80, 1)),\n  dpar = \"mu\",\n  transform = exp) |&gt;\n  select(lifeExp, prediction = estimate)\n\nslopes_data &lt;- comparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps) |&gt;\n  select(lifeExp, estimate) |&gt;\n  left_join(predictions_data, by = \"lifeExp\") |&gt;\n  # Point-slope formula: (y - y1) = m(x - x1)\n  mutate(intercept = estimate * (-lifeExp) + prediction)\n\nggplot(predictions_data, aes(x = lifeExp, y = prediction)) +\n  geom_line(size = 1) + \n  geom_abline(data = slopes_data, aes(slope = estimate, intercept = intercept), \n              size = 0.5, color = \"red\") +\n  geom_point(data = slopes_data) +\n  geom_label(data = slopes_data, aes(label = paste0(\"Slope: \", round(estimate, 1))),\n             nudge_x = -1, hjust = 1) +\n  theme_minimal()\n\n\n\n\nWe now have this in the experiments section\n\n\n\n\nokabeito &lt;- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\n\nset.seed(1024)\nn &lt;- 200\nd &lt;- data.frame(\n  y = rnorm(n),\n  cond = as.factor(sample(0:1, n, TRUE)),\n  episode = as.factor(sample(0:4, n, TRUE)))\n\nmodel1 &lt;- lm(y ~ cond * episode, data = d)\n\np &lt;- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point()\n\n\n\n\n## do episodes 1 and 2 differ when `cond=0`\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = \"black\") +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0))    # grid\n#&gt; \n#&gt;     Term Contrast cond Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 % episode\n#&gt;  episode    2 - 1    0    0.241      0.396 0.609    0.542 0.9 -0.535   1.02       0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cond, predicted_lo, predicted_hi, predicted, y, episode \n#&gt; Type:  response\n\n## do cond=0 and cond=1 differ when episode = 1\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = \"cond\",              # comparison of interest\n  newdata = datagrid(episode = 1)) # grid\n#&gt; \n#&gt;  Term Contrast episode Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % cond\n#&gt;  cond    1 - 0       1    0.546      0.347 1.57    0.115 3.1 -0.134   1.23    0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, episode, predicted_lo, predicted_hi, predicted, y, cond \n#&gt; Type:  response\n\n## Is the difference between episode 1 and 2 larger in cond=0 or cond=1? \n## try this without the `hypothesis` argument to see what we are comparing more clearly\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  annotate(\"rect\", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = \"green\") +\n  annotate(\"rect\", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = \"orange\")  +\n  ggtitle(\"Is the green box taller than the orange box?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0:1),  # grid\n  hypothesis = \"b1 = b2\")          # hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.413      0.508 0.812    0.417 1.3 -0.583   1.41\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/comparisons.html#simple-example-titanic",
    "href": "vignettes/comparisons.html#simple-example-titanic",
    "title": "Comparisons",
    "section": "",
    "text": "Consider a logistic regression model estimated using the Titanic mortality data:\n\nlibrary(marginaleffects)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ndat$PClass[dat$PClass == \"*\"] &lt;- NA\nmod &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\n\n\nThe question that interests us is:\n\nHow does the probability of survival (outcome) change if a passenger travels in 1st class vs. 3rd class?\n\nSince we are comparing two predicted outcomes, we will use comparisons(). To indicate that our focal variable is PClass and that we are interested in the comparison between 1st and 3rd class, we will use the variables argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n\n\n\n\nIn GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above.\nBy default, comparisons() will compute estimates for every row of the original dataset that was used to fit a model. There are 1313 observations in the titanic dataset. Therefore, if we just execute the code in the previous section, we will obtain 1313 estimates of the difference between the probability of survival in 3rd and 1st class:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.496     0.0610 -8.13  &lt; 0.001 51.0 -0.616 -0.376\n#&gt;  PClass 3rd - 1st   -0.472     0.1247 -3.79  &lt; 0.001 12.7 -0.716 -0.228\n#&gt;  PClass 3rd - 1st   -0.353     0.0641 -5.51  &lt; 0.001 24.7 -0.478 -0.227\n#&gt;  PClass 3rd - 1st   -0.493     0.0583 -8.45  &lt; 0.001 55.0 -0.607 -0.379\n#&gt;  PClass 3rd - 1st   -0.445     0.1452 -3.07  0.00216  8.9 -0.730 -0.161\n#&gt; --- 746 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;  PClass 3rd - 1st   -0.377     0.0703 -5.36  &lt; 0.001 23.5 -0.515 -0.239\n#&gt;  PClass 3rd - 1st   -0.384     0.0726 -5.30  &lt; 0.001 23.0 -0.527 -0.242\n#&gt;  PClass 3rd - 1st   -0.412     0.0821 -5.02  &lt; 0.001 20.9 -0.573 -0.251\n#&gt;  PClass 3rd - 1st   -0.399     0.0773 -5.16  &lt; 0.001 22.0 -0.550 -0.247\n#&gt;  PClass 3rd - 1st   -0.361     0.0661 -5.47  &lt; 0.001 24.4 -0.490 -0.232\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Survived, PClass, SexCode, Age \n#&gt; Type:  response\n\nNotice that the contrast between 3rd and 1st is different from row to row. This reflects the fact that, in our model, moving from 1st to 3rd would have a different effect on the predicted probability of survival for different individuals.\nWe can be more specific in our query. Instead of using the empirical distribution as our “grid”, we can specify exactly where we want to evaluate the comparison in the predictor space, by using the newdata argument and the datagrid() function. For example, say I am interested in:\n\nThe effect of moving from 1st to 3rd class on the probability of survival for a 50 year old man and a 50 year old woman.\n\nI can type:\n\ncmp &lt;- comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\ncmp\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nWe now know that moving from 1st to 3rd changes by -0.184 the probability of survival for 50 year old men (SexCode=0), and by -0.511 the probability of survival for 50 year old women (SexCode=1).\n\n\n\nAgain, by default comparisons() estimates quantities for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an “average contrast”:\n\navg_comparisons(mod,                          # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.396     0.0425 -9.3   &lt;0.001 66.0 -0.479 -0.312\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nAlternatively, we could also take the average, but just of the two estimates that we computed above for the 50 year old man and 50 year old woman.\n\navg_comparisons(mod,                           # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.348     0.0676 -5.15   &lt;0.001 21.8 -0.48 -0.215\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that this is exactly the same as the average in the estimates from the previous section, which we had stored as cmp:\n\ncmp$estimate\n#&gt; [1] -0.1844289 -0.5113098\n\nmean(cmp$estimate)\n#&gt; [1] -0.3478694\n\n\n\n\nFinally, imagine we are interested in this question:\n\nDoes moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?\n\nTo answer this, we use the hypothesis argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1), # Step 2: Grid\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result maps directly onto the estimates we had above. It is the difference in the contrast for 50-men and 50-women:\n\ndiff(cmp$estimate)\n#&gt; [1] -0.3268809\n\nThis result can be interpreted as a “difference-in-differences”: Moving from 1st to 3rd has a much larger negative effect on the probability of survival for a 50 year old woman than for a 50 year old man. This difference is statistically significant.\nWe can do a similar comparison, but instead of fixing a conditional grid, we can average over subgroups of the empirical distribution, using the by argument:\n\navg_comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  by = \"SexCode\",                              # Step 3: Average\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  b1=b2    0.162     0.0845 1.91   0.0558 4.2 -0.00402  0.327\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nNow we show how to use the base R predict() function to compute some of the same quantities as above. This exercise may be clarifying for some users.\n\ngrid_50_1_3 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"3rd\")\ngrid_50_1_1 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"1st\")\ngrid_50_0_3 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"3rd\")\ngrid_50_0_1 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"1st\")\n\n\nyhat_50_1_3 &lt;- predict(mod, newdata = grid_50_1_3, type = \"response\")\nyhat_50_1_1 &lt;- predict(mod, newdata = grid_50_1_1, type = \"response\")\nyhat_50_0_3 &lt;- predict(mod, newdata = grid_50_0_3, type = \"response\")\nyhat_50_0_1 &lt;- predict(mod, newdata = grid_50_0_1, type = \"response\")\n\n## prediction on a grid\npredictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = \"3rd\"))\n#&gt; \n#&gt;  Age SexCode PClass Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   50       1    3rd    0.446    0.661 0.6 0.235  0.679\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, Survived, Age, SexCode, PClass \n#&gt; Type:  invlink(link)\nyhat_50_1_3\n#&gt;         1 \n#&gt; 0.4463379\n\n## contrast on a grid\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1))\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nyhat_50_0_3 - yhat_50_0_1\n#&gt;          1 \n#&gt; -0.1844289\nyhat_50_1_3 - yhat_50_1_1\n#&gt;          1 \n#&gt; -0.5113098\n\n## difference-in-differences \ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1),\n  hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)\n#&gt;         1 \n#&gt; 0.3268809\n\n## average of the empirical distribution of contrasts\navg_comparisons(mod, variables = list(PClass = c(\"1st\", \"3rd\")), by = \"SexCode\")\n#&gt; \n#&gt;    Term              Contrast SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass mean(3rd) - mean(1st)       0   -0.334     0.0570 -5.86   &lt;0.001 27.7 -0.446 -0.222\n#&gt;  PClass mean(3rd) - mean(1st)       1   -0.496     0.0623 -7.95   &lt;0.001 49.0 -0.618 -0.374\n#&gt; \n#&gt; Columns: term, contrast, SexCode, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\ngrid_empirical_1_3 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_1_1 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"1st\")\ngrid_empirical_0_3 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_0_1 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"1st\")\nyhat_empirical_0_1 &lt;- predict(mod, newdata = grid_empirical_0_1, type = \"response\")\nyhat_empirical_0_3 &lt;- predict(mod, newdata = grid_empirical_0_3, type = \"response\")\nyhat_empirical_1_1 &lt;- predict(mod, newdata = grid_empirical_1_1, type = \"response\")\nyhat_empirical_1_3 &lt;- predict(mod, newdata = grid_empirical_1_3, type = \"response\")\nmean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)\n#&gt; [1] -0.3341426\nmean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)\n#&gt; [1] -0.4956673"
  },
  {
    "objectID": "vignettes/comparisons.html#predictor-types",
    "href": "vignettes/comparisons.html#predictor-types",
    "title": "Comparisons",
    "section": "",
    "text": "Consider a simple model with a logical and a factor variable:\n\nlibrary(marginaleffects)\n\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\n\nThe comparisons function automatically computes contrasts for each level of the categorical variables, relative to the baseline category (FALSE for logicals, and the reference level for factors), while holding all other values at their observed values. The avg_comparisons() does the same, but then marginalizes by taking the average of unit-level estimates:\n\ncmp &lt;- avg_comparisons(mod)\ncmp\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 % 97.5 %\n#&gt;   am  TRUE - FALSE     2.56       1.30  1.97   0.0485  4.4   0.0167   5.10\n#&gt;   cyl 6 - 4           -6.16       1.54 -4.01   &lt;0.001 14.0  -9.1661  -3.15\n#&gt;   cyl 8 - 4          -10.07       1.45 -6.93   &lt;0.001 37.8 -12.9136  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe summary printed above says that moving from the reference category 4 to the level 6 on the cyl factor variable is associated with a change of -6.156 in the adjusted prediction. Similarly, the contrast from FALSE to TRUE on the am variable is equal to 2.560.\nWe can obtain different contrasts by using the comparisons() function. For example:\n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0 -9.17  -3.15\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93  &lt; 0.001 37.8 -12.91  -7.22\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01   &lt;0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93   &lt;0.001 37.8 -12.91  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nFor comparison, this code produces the same results using the emmeans package:\n\nlibrary(emmeans)\nemm &lt;- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast    estimate   SE df t.ratio p.value\n#&gt;  cyl6 - cyl4    -6.16 1.54 28  -4.009  0.0012\n#&gt;  cyl8 - cyl4   -10.07 1.45 28  -6.933  &lt;.0001\n#&gt;  cyl8 - cyl6    -3.91 1.47 28  -2.660  0.0331\n#&gt; \n#&gt; Results are averaged over the levels of: am \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\nemm &lt;- emmeans(mod, specs = \"am\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast     estimate  SE df t.ratio p.value\n#&gt;  TRUE - FALSE     2.56 1.3 28   1.973  0.0585\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n\nNote that these commands also work on for other types of models, such as GLMs, on different scales:\n\nmod_logit &lt;- glm(am ~ factor(gear), data = mtcars, family = binomial)\n\navg_comparisons(mod_logit)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  gear    4 - 3    0.667   1.36e-01 4.9e+00   &lt;0.001 20.0   0.4  0.933\n#&gt;  gear    5 - 3    1.000   9.95e-06 1.0e+05   &lt;0.001  Inf   1.0  1.000\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod_logit, type = \"link\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  gear    4 - 3     21.3       4578 0.00464    0.996 0.0  -8951   8994\n#&gt;  gear    5 - 3     41.1       9156 0.00449    0.996 0.0 -17904  17986\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n\n\n\nFor categorical predictors, Scholbeck et al. 2023 recommend that analysts report what they call the “observation-wise categorical marginal effects.” They describe the procedure as follows:\n\nRecall that the common definition of categorical MEs is based on first changing all observations’ value of \\(x_j\\) to each category and then computing the difference in predictions when changing it to the reference category. However, one is often interested in prediction changes if aspects of an actual observation change. We therefore propose an observation-wise categorical ME. We first select a single reference category \\(c_h\\). For each observation whose feature value \\(x_j \\neq c_h\\), we predict once with the observed value \\(x_j\\) and once where \\(x_j\\) has been replaced by \\(c_h\\).\n\nTo achieve this with marginaleffects, we proceed in three simple steps:\n\nUse the factor() function to set the reference level of the categorical variable.\nUse the newdata argument to take the subset of data where the observed \\(x_j\\) is different from the reference level we picked in 1.\nApply the avg_comparisons() with the \"revreference\" option.\n\n\ndat &lt;- transform(mtcars, cyl = factor(cyl, levels = c(6, 4, 8)))\n\nmod &lt;- glm(vs ~ mpg * factor(cyl), data = dat, family = binomial)\n\navg_comparisons(mod,\n  variables = list(cyl = \"revreference\"),\n  newdata = subset(dat, cyl != 6))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4   -0.323     0.2170 -1.49    0.137   2.9 -0.748  0.103\n#&gt;   cyl    6 - 8    0.561     0.0357 15.69   &lt;0.001 181.9  0.491  0.631\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\n\nAll functions of the marginaleffects package attempt to treat character predictors as factor predictors. However, using factors instead of characters when modeling is strongly encouraged, because they are much safer and faster. This is because factors hold useful information about the full list of levels, which makes them easier to track and handle internally by marginaleffects. Users are strongly encouraged to convert their character variables to factor before fitting their models and using slopes functions.\n\n\n\nWe can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the hp variable by 1 unit (default) or by 5 units about the original value:\n\nmod &lt;- lm(mpg ~ hp, data = mtcars)\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp       +1  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = 5))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp       +5   -0.341     0.0506 -6.74   &lt;0.001 35.9 -0.44 -0.242\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions for a change in the regressor between two arbitrary values:\n\navg_comparisons(mod, variables = list(hp = c(90, 110)))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp 110 - 90    -1.36      0.202 -6.74   &lt;0.001 35.9 -1.76 -0.968\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:\n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp  Q3 - Q1     -5.7      0.845 -6.74   &lt;0.001 35.9 -7.35  -4.04\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n#&gt; \n#&gt;  Term                Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   &lt;0.001 35.9 -6.04  -3.32\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"2sd\"))\n#&gt; \n#&gt;  Term            Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd) - (x - sd)    -9.36       1.39 -6.74   &lt;0.001 35.9 -12.1  -6.64\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n#&gt; \n#&gt;  Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp Max - Min    -19.3       2.86 -6.74   &lt;0.001 35.9 -24.9  -13.7\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/comparisons.html#interactions-and-cross-contrasts",
    "href": "vignettes/comparisons.html#interactions-and-cross-contrasts",
    "title": "Comparisons",
    "section": "",
    "text": "In some contexts we are interested in whether the “effect” of a variable changes, as a function of another variable. A very simple strategy to tackle this question is to estimate a model with a multiplicative interaction like this one:\n\nmod &lt;- lm(mpg ~ am * factor(cyl), data = mtcars)\n\nCalling avg_comparisons() with the by argument shows that the estimated comparisons differ based on cyl:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\")\n#&gt; \n#&gt;  Term          Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;    am mean(1) - mean(0)   4     5.18       2.05 2.521   0.0117 6.4  1.15   9.20\n#&gt;    am mean(1) - mean(0)   6     1.44       2.32 0.623   0.5336 0.9 -3.10   5.98\n#&gt;    am mean(1) - mean(0)   8     0.35       2.32 0.151   0.8799 0.2 -4.19   4.89\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nHowever, using the hypothesis argument for pairwise contrasts between the above comparisons reveals that the heterogeneity is not statistically significant:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  4 - 6     3.73       3.09 1.206    0.228 2.1 -2.33   9.80\n#&gt;  4 - 8     4.82       3.09 1.559    0.119 3.1 -1.24  10.89\n#&gt;  6 - 8     1.09       3.28 0.333    0.739 0.4 -5.33   7.51\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nIn other contexts, we are interested in a “cross-contrast” or “cross-comparisons”; we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the variables argument, and set the cross=TRUE:\n\navg_comparisons(mod, variables = c(\"cyl\", \"am\"), cross = TRUE)\n#&gt; \n#&gt;  Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 % C: am C: cyl\n#&gt;     -2.33       2.48 -0.942  0.34596 1.5  -7.19   2.52 1 - 0  6 - 4\n#&gt;     -7.50       2.77 -2.709  0.00674 7.2 -12.93  -2.07 1 - 0  8 - 4\n#&gt; \n#&gt; Columns: term, contrast_am, contrast_cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/comparisons.html#quantities-of-interest",
    "href": "vignettes/comparisons.html#quantities-of-interest",
    "title": "Comparisons",
    "section": "",
    "text": "This section compares 4 quantities:\n\nUnit-Level Contrasts\nAverage Contrast\nContrast at the Mean\nContrast Between Marginal Means\n\nThe ideas discussed in this section focus on contrasts, but they carry over directly to analogous types of marginal effects.\n\n\nIn models with interactions or non-linear components (e.g., link function), the value of a contrast or marginal effect can depend on the value of all the predictors in the model. As a result, contrasts and marginal effects are fundamentally unit-level quantities. The effect of a 1 unit increase in \\(X\\) can be different for Mary or John. Every row of a dataset has a different contrast and marginal effect.\nThe mtcars dataset has 32 rows, so the comparisons() function produces 32 contrast estimates:\n\nlibrary(marginaleffects)\nmod &lt;- glm(vs ~ factor(gear) + mpg, family = binomial, data = mtcars)\ncmp &lt;- comparisons(mod, variables = \"mpg\")\nnrow(cmp)\n#&gt; [1] 32\n\n\n\n\nBy default, the slopes() and comparisons() functions compute marginal effects and contrasts for every row of the original dataset. These unit-level estimates can be of great interest, as discussed in another vignette. Nevertheless, one may want to focus on one-number summaries: the avg_*() functions or the by argument compute the “Average Marginal Effect” or “Average Contrast,” by taking the mean of all the unit-level estimates.\n\navg_comparisons(mod, variables = \"mpg\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, variables = \"mpg\", by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nwhich are equivalent to:\n\nmean(cmp$estimate)\n#&gt; [1] 0.06091377\n\nWe could also show the full distribution of contrasts across our dataset with a histogram:\n\nlibrary(ggplot2)\n\ncmp &lt;- comparisons(mod, variables = \"gear\")\n\nggplot(cmp, aes(estimate)) +\n    geom_histogram(bins = 30) +\n    facet_wrap(~contrast, scale = \"free_x\") +\n    labs(x = \"Distribution of unit-level contrasts\")\n\n\n\n\nThis graph displays the effect of a change of 1 unit in the gear variable, for each individual in the observed data.\n\n\n\nAn alternative which used to be very common but has now fallen into a bit of disfavor is to compute “Contrasts at the mean.” The idea is to create a “synthetic” or “hypothetical” individual (row of the dataset) whose characteristics are completely average. Then, we compute and report the contrast for this specific hypothetical individual.\nThis can be achieved by setting newdata=\"mean\" or to newdata=datagrid(), both of which fix variables to their means or modes:\n\ncomparisons(mod, variables = \"mpg\", newdata = \"mean\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % gear  mpg\n#&gt;   mpg       +1    0.155     0.0539 2.88  0.00399 8.0 0.0495  0.261    3 20.1\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, gear, mpg \n#&gt; Type:  response\n\nContrasts at the mean can differ substantially from average contrasts.\nThe advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.\n\n\n\nYet another type of contrast is the “Contrast between marginal means.” This type of contrast is closely related to the “Contrast at the mean”, with a few wrinkles. It is the default approach used by the emmeans package for R.\nRoughly speaking, the procedure is as follows:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nThe contrast obtained through this approach has two critical characteristics:\n\nIt is the contrast for a synthetic individual with perfectly average qualities on every (numeric) predictor.\nIt is a weighted average of unit-level contrasts, where weights assume a perfectly balanced dataset across every categorical predictor.\n\nWith respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted as if the data were perfectly balanced?\nFor example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.\nIf the contrasts between marginal means is really the quantity of interest, it is easy to use the comparisons() to estimate contrasts between marginal means. The newdata determines the values of the predictors at which we want to compute contrasts. We can set newdata=\"marginalmeans\" to emulate the emmeans behavior. For example, here we compute contrasts in a model with an interaction:\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\nmod &lt;- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)\n\navg_comparisons(\n    mod,\n    newdata = \"marginalmeans\",\n    variables = c(\"species\", \"island\"))\n#&gt; \n#&gt;     Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  island  Dream - Biscoe       -0.434      0.451 -0.962    0.336   1.6 -1.318  0.450\n#&gt;  island  Torgersen - Biscoe    0.060      0.467  0.128    0.898   0.2 -0.856  0.976\n#&gt;  species Chinstrap - Adelie   10.563      0.418 25.272   &lt;0.001 465.7  9.744 11.382\n#&gt;  species Gentoo - Adelie       5.792      0.798  7.257   &lt;0.001  41.2  4.228  7.356\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWhich is equivalent to this in emmeans:\n\nemm &lt;- emmeans(\n    mod,\n    specs = c(\"species\", \"island\"))\ncontrast(emm, method = \"trt.vs.ctrl1\")\n#&gt;  contrast                            estimate    SE  df t.ratio p.value\n#&gt;  Chinstrap Biscoe - Adelie Biscoe      nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Biscoe - Adelie Biscoe          5.792 0.798 331   7.257  &lt;.0001\n#&gt;  Adelie Dream - Adelie Biscoe          -0.434 0.451 331  -0.962  0.7573\n#&gt;  Chinstrap Dream - Adelie Biscoe       nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Dream - Adelie Biscoe           5.358 1.035 331   5.177  &lt;.0001\n#&gt;  Adelie Torgersen - Adelie Biscoe       0.060 0.467 331   0.128  0.9987\n#&gt;  Chinstrap Torgersen - Adelie Biscoe   nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Torgersen - Adelie Biscoe       5.852 1.070 331   5.468  &lt;.0001\n#&gt; \n#&gt; Results are averaged over the levels of: sex \n#&gt; P value adjustment: dunnettx method for 5 tests\n\nThe emmeans section of the Alternative Software vignette shows further examples.\nThe excellent vignette of the emmeans package discuss the same issues in a slightly different (and more positive) way:\n\nThe point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.\n\n\nAll this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace."
  },
  {
    "objectID": "vignettes/comparisons.html#conditional-contrasts",
    "href": "vignettes/comparisons.html#conditional-contrasts",
    "title": "Comparisons",
    "section": "",
    "text": "Consider a model with an interaction term. What happens to the dependent variable when the hp variable increases by 10 units?\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt, data = mtcars)\n\nplot_comparisons(\n    mod,\n    variables = list(hp = 10),\n    condition = \"wt\")"
  },
  {
    "objectID": "vignettes/comparisons.html#transformations",
    "href": "vignettes/comparisons.html#transformations",
    "title": "Comparisons",
    "section": "",
    "text": "So far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:\n\ncomparison\ntransform\n\nConsider the case of a model with a single predictor \\(x\\). To compute average contrasts, we proceed as follows:\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x\\): \\(\\hat{y}_x\\)\nCompute adjusted predictions for each row of the dataset for the observed values \\(x + 1\\): \\(\\hat{y}_{x+1}\\)\ncomparison: Compute unit-level contrasts by taking the difference between (or some other function of) adjusted predictions: \\(\\hat{y}_{x+1} - \\hat{y}_x\\)\nCompute the average contrast by taking the mean of unit-level contrasts: \\(1/N \\sum_{i=1}^N \\hat{y}_{x+1} - \\hat{y}_x\\)\ntransform: Transform the average contrast or return them as-is.\n\nThe comparison argument of the comparisons() function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with hi value of \\(x\\), and predictions with a lo value of \\(x\\): function(hi, lo) hi-lo.\nThe transform argument of the comparisons() function applies a custom transformation to the unit-level contrasts.\nThe transform argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without transform."
  },
  {
    "objectID": "vignettes/comparisons.html#differences",
    "href": "vignettes/comparisons.html#differences",
    "title": "Comparisons",
    "section": "",
    "text": "The default contrast calculate by the comparisons() function is a (untransformed) difference between two adjusted predictions. For instance, to estimate the effect of a change of 1 unit, we do:\n\nlibrary(marginaleffects)\n\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\n\n## construct data\n\nmtcars_minus &lt;- mtcars_plus &lt;- mtcars\nmtcars_minus$mpg &lt;- mtcars_minus$mpg - 0.5\nmtcars_plus$mpg &lt;- mtcars_plus$mpg + 0.5\n\n## adjusted predictions\nyhat_minus &lt;- predict(mod, newdata = mtcars_minus, type = \"response\")\nyhat_plus &lt;- predict(mod, newdata = mtcars_plus, type = \"response\")\n\n## unit-level contrasts\ncon &lt;- yhat_plus - yhat_minus\n\n## average contrasts\nmean(con)\n#&gt; [1] 0.05540227\n\nWe can use the avg_comparisons() function , or the by argument to obtain the same results:\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/comparisons.html#difference-in-differences-in-differences",
    "href": "vignettes/comparisons.html#difference-in-differences-in-differences",
    "title": "Comparisons",
    "section": "",
    "text": "Going back to our Titanic example:\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\nIn this case, a contrast is a difference between predicted probabilities. We can compute that contrast for different types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Contrast PClass Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % SexCode  Age\n#&gt;  SexCode    1 - 0    1st    0.483     0.0631 7.65   &lt;0.001 45.5 0.359  0.606   0.381 30.4\n#&gt;  SexCode    1 - 0    3rd    0.335     0.0634 5.29   &lt;0.001 22.9 0.211  0.459   0.381 30.4\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, predicted_lo, predicted_hi, predicted, Survived, SexCode, Age \n#&gt; Type:  response\n\nOne we can notice above, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. Being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?\nTo answer this question, we can compute a difference-in-difference using the hypothesis argument (see the Hypothesis vignette for details). For example, using b1 and b2 to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:\n\ncomparisons(\n  titanic,\n  hypothesis = \"b1 - b2 = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  b1-b2=0    0.148     0.0894 1.65   0.0987 3.3 -0.0276  0.323\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNow, let’s say we consider more types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;     Term Contrast PClass   Age Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 % 97.5 % SexCode\n#&gt;  SexCode    1 - 0    1st  0.17   0.1081      0.122  0.883   0.3774   1.4 -0.1319  0.348   0.381\n#&gt;  SexCode    1 - 0    1st 71.00   0.8795      0.057 15.437   &lt;0.001 176.2  0.7679  0.991   0.381\n#&gt;  SexCode    1 - 0    3rd  0.17   0.0805      0.157  0.513   0.6081   0.7 -0.2272  0.388   0.381\n#&gt;  SexCode    1 - 0    3rd 71.00   0.4265      0.203  2.101   0.0356   4.8  0.0287  0.824   0.381\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, Age, predicted_lo, predicted_hi, predicted, Survived, SexCode \n#&gt; Type:  response\n\nWith these results, we could compute a triple difference:\n\ncomparisons(\n  titanic,\n  hypothesis = \"(b1 - b3) - (b2 - b4) = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;               Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b1-b3)-(b2-b4)=0   -0.425      0.359 -1.19    0.236 2.1 -1.13  0.278\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/comparisons.html#ratios",
    "href": "vignettes/comparisons.html#ratios",
    "title": "Comparisons",
    "section": "",
    "text": "Instead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, the adjrr function the Stata software package can compute “adjusted risk ratios”, which are ratios of adjusted predictions. To do this in R, we use the comparison argument:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result is the average adjusted risk ratio for an increment of 1, that is, the adjusted predictions when the mpg are incremented by 0.5, divided by the adjusted predictions when mpg is decremented by 0.5.\nThe comparison accepts different values for common types of contrasts: ‘difference’, ‘ratio’, ‘lnratio’, ‘ratioavg’, ‘lnratioavg’, ‘lnoravg’, ‘differenceavg’. These strings are shortcuts for functions that accept two vectors of adjusted predictions and returns a single vector of contrasts. For example, these two commands yield identical results:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, comparison = function(hi, lo) hi / lo)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:\n\navg_comparisons(mod, comparison = function(hi, lo) sqrt(hi) / log(lo + 10))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1    0.275     0.0252 10.9   &lt;0.001 89.4 0.225  0.324\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe same arguments work in the plotting function plot_comparisons() as well, which allows us to plot various custom contrasts. Here is a comparison of Adjusted Risk Ratio and Adjusted Risk Difference in a model of the probability of survival aboard the Titanic:\n\nlibrary(ggplot2)\nlibrary(patchwork)\ntitanic &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ntitanic &lt;- read.csv(titanic)\nmod_titanic &lt;- glm(\n    Survived ~ Sex * PClass + Age + I(Age^2),\n    family = binomial,\n    data = titanic)\n\navg_comparisons(mod_titanic)\n#&gt; \n#&gt;    Term      Contrast Estimate Std. Error     z Pr(&gt;|z|)     S    2.5 %  97.5 %\n#&gt;  Age    +1            -0.00639    0.00107  -6.0   &lt;0.001  28.9 -0.00848 -0.0043\n#&gt;  PClass 2nd - 1st     -0.20578    0.03954  -5.2   &lt;0.001  22.3 -0.28328 -0.1283\n#&gt;  PClass 3rd - 1st     -0.40428    0.03958 -10.2   &lt;0.001  78.9 -0.48187 -0.3267\n#&gt;  Sex    male - female -0.48468    0.03004 -16.1   &lt;0.001 192.2 -0.54355 -0.4258\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np1 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survival | Age + 1) / P(Survival | Age)\")\n\np2 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\") +\n    ylab(\"Adjusted Risk Difference\\nP(Survival | Age + 1) - P(Survival | Age)\")\n\np1 + p2\n\n\n\n\nBy default, the standard errors around contrasts are computed using the delta method on the scale determined by the type argument (e.g., “link” or “response”). Some analysts may prefer to proceed differently. For example, in Stata, the adjrr computes adjusted risk ratios (ARR) in two steps:\n\nCompute the natural log of the ratio between the mean of adjusted predictions with \\(x+1\\) and the mean of adjusted predictions with \\(x\\).\nExponentiate the estimate and confidence interval bounds.\n\nStep 1 is easy to achieve with the comparison argument described above. Step 2 can be achieved with the transform argument:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that we can use the lnratioavg shortcut instead of defining the function ourselves.\nThe order of operations in previous command was:\n\nCompute the custom unit-level log ratios\nExponentiate them\nTake the average using the avg_comparisons()\n\nThere is a very subtle difference between the procedure above and this code:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nSince the exp function is now passed to the transform argument of the comparisons() function, the exponentiation is now done only after unit-level contrasts have been averaged. This is what Stata appears to do under the hood, and the results are slightly different.\n\ncomparisons(\n    mod,\n    comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nNote that equivalent results can be obtained using shortcut strings in the comparison argument: “ratio”, “lnratio”, “lnratioavg”.\n\ncomparisons(\n    mod,\n    comparison = \"lnratioavg\",\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg mean(+1)     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nAll the same arguments apply to the plotting functions of the marginaleffects package as well. For example we can plot the Adjusted Risk Ratio in a model with a quadratic term:\n\nlibrary(ggplot2)\ndat_titanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\nmod2 &lt;- glm(Survived  ~ Age, data = dat_titanic, family = binomial)\nplot_comparisons(\n    mod2,\n    variables = list(\"Age\" = 10),\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survived = 1 | Age + 10) / P(Survived = 1 | Age)\")"
  },
  {
    "objectID": "vignettes/comparisons.html#forward-backward-centered-and-custom-differences",
    "href": "vignettes/comparisons.html#forward-backward-centered-and-custom-differences",
    "title": "Comparisons",
    "section": "",
    "text": "By default, the comparisons() function computes a “forward” difference. For example, if we ask comparisons() to estimate the effect of a 10-unit change in predictor x on outcome y, comparisons() will compare the predicted values with x and x+10.\n\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (mtcars$hp - min(mtcars$hp)) / (max(mtcars$hp) - min(mtcars$hp)) + 1\nmod &lt;- lm(mpg ~ log(new_hp), data = dat)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = 10))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp      +10     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:\n\nforward_diff &lt;- \\(x) data.frame(x, x + 10)\nbackward_diff &lt;- \\(x) data.frame(x - 10, x)\ncenter_diff &lt;- \\(x) data.frame(x - 5, x + 5)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = forward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = backward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -6.51      0.744 -8.74   &lt;0.001 58.6 -7.97  -5.05\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = center_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -4.06      0.464 -8.74   &lt;0.001 58.6 -4.97  -3.15\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that the last “centered” difference gives the same results as the default comparisons() call."
  },
  {
    "objectID": "vignettes/comparisons.html#lognormal-hurdle-model",
    "href": "vignettes/comparisons.html#lognormal-hurdle-model",
    "title": "Comparisons",
    "section": "",
    "text": "With hurdle models, we can fit two separate models simultaneously:\n\nA model that predicts if the outcome is zero or not zero\nIf the outcome is not zero, a model that predicts what the value of the outcome is\n\nWe can calculate predictions and marginal effects for each of these hurdle model processes, but doing so requires some variable transformation since the stages of these models use different link functions.\nThe hurdle_lognormal() family in brms uses logistic regression (with a logit link) for the hurdle part of the model and lognormal regression (where the outcome is logged before getting used in the model) for the non-hurdled part. Let’s look at an example of predicting GDP per capita (which is distributed exponentially) using life expectancy. We’ll add some artificial zeros so that we can work with a hurdle stage of the model.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(gapminder)\n\n## Build some 0s into the GDP column\nset.seed(1234)\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap0 = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero)\n\nmod &lt;- brm(\n  bf(gdpPercap0 ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = 4, cores = 4, seed = 1234)\n\nWe have two different sets of coefficients here for the two different processes. The hurdle part (hu) uses a logit link, and the non-hurdle part (mu) uses an identity link. However, that’s a slight misnomer—a true identity link would show the coefficients on a non-logged dollar value scale. Because we’re using a lognormal family, GDP per capita is pre-logged, so the “original” identity scale is actually logged dollars.\n\nsummary(mod)\n\n\n#&gt;  Family: hurdle_lognormal \n#&gt;   Links: mu = identity; sigma = identity; hu = logit \n#&gt; Formula: gdpPercap0 ~ lifeExp \n#&gt;          hu ~ lifeExp\n#&gt;    Data: gapminder (Number of observations: 1680) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept        3.47      0.09     3.29     3.65 1.00     4757     3378\n#&gt; hu_Intercept     3.16      0.40     2.37     3.96 1.00     2773     2679\n#&gt; lifeExp          0.08      0.00     0.08     0.08 1.00     5112     3202\n#&gt; hu_lifeExp      -0.10      0.01    -0.12    -0.08 1.00     2385     2652\n#&gt; ...\n\nWe can get predictions for the hu part of the model on the link (logit) scale:\n\npredictions(mod, dpar = \"hu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40   -0.817 -1.03 -0.604\n#&gt;       60   -2.805 -3.06 -2.555\n#&gt;       80   -4.790 -5.34 -4.275\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\n\n…or on the response (percentage point) scale:\n\npredictions(mod, dpar = \"hu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate   2.5 % 97.5 %\n#&gt;       40  0.30630 0.26231 0.3534\n#&gt;       60  0.05703 0.04466 0.0721\n#&gt;       80  0.00824 0.00478 0.0137\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can also get slopes for the hu part of the model on the link (logit) or response (percentage point) scales:\n\nslopes(mod, dpar = \"hu\", type = \"link\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp Estimate  2.5 %  97.5 %\n#&gt;  lifeExp      40  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      60  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      80  -0.0993 -0.116 -0.0837\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  link\n\nslopes(mod, dpar = \"hu\", type = \"response\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp  Estimate    2.5 %    97.5 %\n#&gt;  lifeExp      40 -0.021080 -0.02592 -0.016590\n#&gt;  lifeExp      60 -0.005322 -0.00615 -0.004562\n#&gt;  lifeExp      80 -0.000812 -0.00115 -0.000543\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWorking with the mu part of the model is trickier. Switching between type = \"link\" and type = \"response\" doesn’t change anything, since the outcome is pre-logged:\n\npredictions(mod, dpar = \"mu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\npredictions(mod, dpar = \"mu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nFor predictions, we need to exponentiate the results to scale them back up to dollar amounts. We can do this by post-processing the results (e.g. with dplyr::mutate(predicted = exp(predicted))), or we can use the transform argument in predictions() to pass the results to exp() after getting calculated:\n\npredictions(mod, dpar = \"mu\", \n            newdata = datagrid(lifeExp = seq(40, 80, 20)),\n            transform = exp)\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40      744   694    801\n#&gt;       60     3581  3449   3718\n#&gt;       80    17215 16110  18410\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can pass transform = exp to plot_predictions() too:\n\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"link\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       title = \"Hurdle part (hu)\",\n       subtitle = \"Logit-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"response\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       subtitle = \"Percentage point-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       title = \"Non-hurdle part (mu)\",\n       subtitle = \"Log-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  transform = exp,\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       subtitle = \"Dollar-scale predictions\")\n\n\n\n\nFor marginal effects, we need to transform the predictions before calculating the instantaneous slopes. We also can’t use the slopes() function directly—we need to use comparisons() and compute the numerical derivative ourselves (i.e. predict gdpPercap at lifeExp of 40 and 40.001 and calculate the slope between those predictions). We can use the comparison argument to pass the pair of predicted values to exp() before calculating the slopes:\n\n## step size of the numerical derivative\neps &lt;- 0.001\n\ncomparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  # rescale the elements of the slope\n  # (exp(40.001) - exp(40)) / exp(0.001)\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps\n)\n#&gt; \n#&gt;     Term Contrast lifeExp Estimate  2.5 % 97.5 %\n#&gt;  lifeExp   +0.001      40     58.4   55.8     61\n#&gt;  lifeExp   +0.001      60    280.9  266.6    296\n#&gt;  lifeExp   +0.001      80   1349.5 1222.6   1490\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWe can visually confirm that these are the instantaneous slopes at each of these levels of life expectancy:\n\npredictions_data &lt;- predictions(\n  mod,\n  newdata = datagrid(lifeExp = seq(30, 80, 1)),\n  dpar = \"mu\",\n  transform = exp) |&gt;\n  select(lifeExp, prediction = estimate)\n\nslopes_data &lt;- comparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps) |&gt;\n  select(lifeExp, estimate) |&gt;\n  left_join(predictions_data, by = \"lifeExp\") |&gt;\n  # Point-slope formula: (y - y1) = m(x - x1)\n  mutate(intercept = estimate * (-lifeExp) + prediction)\n\nggplot(predictions_data, aes(x = lifeExp, y = prediction)) +\n  geom_line(size = 1) + \n  geom_abline(data = slopes_data, aes(slope = estimate, intercept = intercept), \n              size = 0.5, color = \"red\") +\n  geom_point(data = slopes_data) +\n  geom_label(data = slopes_data, aes(label = paste0(\"Slope: \", round(estimate, 1))),\n             nudge_x = -1, hjust = 1) +\n  theme_minimal()\n\n\n\n\nWe now have this in the experiments section"
  },
  {
    "objectID": "vignettes/comparisons.html#visual-examples",
    "href": "vignettes/comparisons.html#visual-examples",
    "title": "Comparisons",
    "section": "",
    "text": "okabeito &lt;- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\n\nset.seed(1024)\nn &lt;- 200\nd &lt;- data.frame(\n  y = rnorm(n),\n  cond = as.factor(sample(0:1, n, TRUE)),\n  episode = as.factor(sample(0:4, n, TRUE)))\n\nmodel1 &lt;- lm(y ~ cond * episode, data = d)\n\np &lt;- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point()\n\n\n\n\n## do episodes 1 and 2 differ when `cond=0`\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = \"black\") +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0))    # grid\n#&gt; \n#&gt;     Term Contrast cond Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 % episode\n#&gt;  episode    2 - 1    0    0.241      0.396 0.609    0.542 0.9 -0.535   1.02       0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cond, predicted_lo, predicted_hi, predicted, y, episode \n#&gt; Type:  response\n\n## do cond=0 and cond=1 differ when episode = 1\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = \"cond\",              # comparison of interest\n  newdata = datagrid(episode = 1)) # grid\n#&gt; \n#&gt;  Term Contrast episode Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % cond\n#&gt;  cond    1 - 0       1    0.546      0.347 1.57    0.115 3.1 -0.134   1.23    0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, episode, predicted_lo, predicted_hi, predicted, y, cond \n#&gt; Type:  response\n\n## Is the difference between episode 1 and 2 larger in cond=0 or cond=1? \n## try this without the `hypothesis` argument to see what we are comparing more clearly\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  annotate(\"rect\", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = \"green\") +\n  annotate(\"rect\", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = \"orange\")  +\n  ggtitle(\"Is the green box taller than the orange box?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0:1),  # grid\n  hypothesis = \"b1 = b2\")          # hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.413      0.508 0.812    0.417 1.3 -0.583   1.41\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "index.html#interpreting-statistical-models-with-marginaleffects-for-r-and-python",
    "href": "index.html#interpreting-statistical-models-with-marginaleffects-for-r-and-python",
    "title": "The Marginal Effects Zoo",
    "section": "Interpreting statistical models with marginaleffects for R and Python",
    "text": "Interpreting statistical models with marginaleffects for R and Python\nParameter estimates are often hard to interpret substantively, especially when they are generated by complex models with non-linear components, interactions, or transformations. Instead of reporting unintuitive parameters, data analysts would rather focus on simple quantities of interest, which have straightforward scientific interpretations. Unfortunately, meaningful estimands—and their standard errors—are often tedious to compute, and the terminology used to describe them varies tremendously across fields.\nThese problems are compounded by the fact that modeling packages produce objects with very different structures and which hold different information. This means that end-users often have to write customized code to interpret the output of Linear, GLM, GAM, Bayesian, Mixed Effects, and other model types. This can lead to wasted effort, confusion, and mistakes, and it can hinder the implementation of best practices.\nThis online book introduces a conceptual framework and software tools to facilitate the interpretation of a vast range of statistical models in R and Python. The marginaleffects package supports over 100 classes of statistical models, including linear, GLM, GAM, mixed-effects, bayesian, categorical outcomes, and more. With a single unified interface, users can compute and plot many quantities of interest, including:\n\nPredictions (aka fitted values or adjusted predictions)\nComparisons such as risk differences, risk ratios, odds, etc.\nSlopes (aka marginal effects)\nMarginal means\nLinear and non-linear hypothesis tests\nEquivalence tests\nUncertainty estimates using the delta method, bootstrapping, simulation, or conformal inference.\n\nThe Marginal Effects Zoo is a free online book with over 30 chapters full of tutorials, case studies, and technical notes. It covers a wide range of topics, including how the marginaleffects package can facilitate the interpreation of analysis of:\n\nExperiments\nMachine learning models\nCausal inference with G-Computation\nBayesian modeling\nMultilevel regression with post-stratification (MRP)\nMissing data imputation\nMatching\nInverse probability weighting\nConformal prediction"
  },
  {
    "objectID": "index.html#do-you-like-this-project",
    "href": "index.html#do-you-like-this-project",
    "title": "The Marginal Effects Zoo",
    "section": "Do you like this project?",
    "text": "Do you like this project?\nIf you like this project, you can contribute in 3 main ways:\n\nCite the marginaleffects package in your work and tell your friends about it.\nSubmit bug reports, documentation improvements, or code contributions to the Github repositories of the R or Python versions of the package:\n\nhttps://github.com/vincentarelbundock/marginaleffects\nhttps://github.com/vincentarelbundock/pymarginaleffects\n\nMake a donation to The Native Women’s Shelter of Montreal and send me (Vincent) a quick note. You’ll make my day."
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "The Marginal Effects Zoo",
    "section": "Development",
    "text": "Development"
  },
  {
    "objectID": "man/inferences.html",
    "href": "man/inferences.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "(EXPERIMENTAL) Bootstrap, Conformal, and Simulation-Based Inference\n\n\nWarning: This function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other marginaleffects functions.\nApply this function to a marginaleffects object to change the inferential method used to compute uncertainty estimates.\n\n\n\ninferences(\n  x,\n  method,\n  R = 1000,\n  conf_type = \"perc\",\n  conformal_test = NULL,\n  conformal_calibration = NULL,\n  conformal_score = \"residual_abs\",\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nObject produced by one of the core marginaleffects functions.\n\n\n\n\nmethod\n\n\nString\n\n\n\"delta\": delta method standard errors\n\n\n\"boot\" package\n\n\n\"fwb\": fractional weighted bootstrap\n\n\n\"rsample\" package\n\n\n\"simulation\" from a multivariate normal distribution (Krinsky & Robb, 1986)\n\n\n\"mi\" multiple imputation for missing data\n\n\n\"conformal_split\": prediction intervals using split conformal prediction (see Angelopoulos & Bates, 2022)\n\n\n\"conformal_cv+\": prediction intervals using cross-validation+ conformal prediction (see Barber et al., 2020)\n\n\n\n\n\n\nR\n\n\nNumber of resamples, simulations, or cross-validation folds.\n\n\n\n\nconf_type\n\n\nString: type of bootstrap interval to construct.\n\n\nboot: \"perc\", \"norm\", \"basic\", or \"bca\"\n\n\nfwb: \"perc\", \"norm\", \"basic\", \"bc\", or \"bca\"\n\n\nrsample: \"perc\" or \"bca\"\n\n\nsimulation: argument ignored.\n\n\n\n\n\n\nconformal_test\n\n\nData frame of test data for conformal prediction.\n\n\n\n\nconformal_calibration\n\n\nData frame of calibration data for split conformal prediction (method=\"conformal_split).\n\n\n\n\nconformal_score\n\n\nString. Warning: The type argument in predictions() must generate predictions which are on the same scale as the outcome variable. Typically, this means that type must be \"response\" or \"probs\".\n\n\n\"residual_abs\" or \"residual_sq\" for regression tasks (numeric outcome)\n\n\n\"softmax\" for classification tasks (when predictions() returns a group columns, such as multinomial or ordinal logit models.\n\n\n\n\n\n\n…\n\n\n\n\nIf method=“boot”, additional arguments are passed to boot::boot().\n\n\nIf method=“fwb”, additional arguments are passed to fwb::fwb().\n\n\nIf method=“rsample”, additional arguments are passed to rsample::bootstraps().\n\n\nAdditional arguments are ignored for all other methods.\n\n\n\n\n\n\n\n\nWhen method=“simulation”, we conduct simulation-based inference following the method discussed in Krinsky & Robb (1986):\n\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model’s estimated coefficients and variance equal to the model’s variance-covariance matrix (classical, \"HC3\", or other).\n\n\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, slopes, or hypotheses.\n\n\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\n\nWhen method=“fwb”, drawn weights are supplied to the model fitting function’s weights argument; if the model doesn’t accept non-integer weights, this method should not be used. If weights were included in the original model fit, they are extracted by weights() and multiplied by the drawn weights. These weights are supplied to the wts argument of the estimation function (e.g., comparisons()).\n\n\n\nA marginaleffects object with simulation or bootstrap resamples and objects attached.\n\n\n\nKrinsky, I., and A. L. Robb. 1986. “On Approximating the Statistical Properties of Elasticities.” Review of Economics and Statistics 68 (4): 715–9.\nKing, Gary, Michael Tomz, and Jason Wittenberg. \"Making the most of statistical analyses: Improving interpretation and presentation.\" American journal of political science (2000): 347-361\nDowd, Bryan E., William H. Greene, and Edward C. Norton. \"Computation of standard errors.\" Health services research 49.2 (2014): 731-750.\nAngelopoulos, Anastasios N., and Stephen Bates. 2022. \"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.\" arXiv. https://doi.org/10.48550/arXiv.2107.07511.\nBarber, Rina Foygel, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. 2020. “Predictive Inference with the Jackknife+.” arXiv. http://arxiv.org/abs/1905.02928.\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nlibrary(magrittr)\nset.seed(1024)\nmod &lt;- lm(Sepal.Length ~ Sepal.Width * Species, data = iris)\n\n# bootstrap\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"boot\")\n\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"rsample\")\n\n# Fractional (bayesian) bootstrap\navg_slopes(mod, by = \"Species\") %&gt;%\n  inferences(method = \"fwb\") %&gt;%\n  posterior_draws(\"rvar\") %&gt;%\n  data.frame()\n\n# Simulation-based inference\nslopes(mod) %&gt;%\n  inferences(method = \"simulation\") %&gt;%\n  head()"
  },
  {
    "objectID": "man/inferences.html#inferences",
    "href": "man/inferences.html#inferences",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "(EXPERIMENTAL) Bootstrap, Conformal, and Simulation-Based Inference\n\n\nWarning: This function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other marginaleffects functions.\nApply this function to a marginaleffects object to change the inferential method used to compute uncertainty estimates.\n\n\n\ninferences(\n  x,\n  method,\n  R = 1000,\n  conf_type = \"perc\",\n  conformal_test = NULL,\n  conformal_calibration = NULL,\n  conformal_score = \"residual_abs\",\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nObject produced by one of the core marginaleffects functions.\n\n\n\n\nmethod\n\n\nString\n\n\n\"delta\": delta method standard errors\n\n\n\"boot\" package\n\n\n\"fwb\": fractional weighted bootstrap\n\n\n\"rsample\" package\n\n\n\"simulation\" from a multivariate normal distribution (Krinsky & Robb, 1986)\n\n\n\"mi\" multiple imputation for missing data\n\n\n\"conformal_split\": prediction intervals using split conformal prediction (see Angelopoulos & Bates, 2022)\n\n\n\"conformal_cv+\": prediction intervals using cross-validation+ conformal prediction (see Barber et al., 2020)\n\n\n\n\n\n\nR\n\n\nNumber of resamples, simulations, or cross-validation folds.\n\n\n\n\nconf_type\n\n\nString: type of bootstrap interval to construct.\n\n\nboot: \"perc\", \"norm\", \"basic\", or \"bca\"\n\n\nfwb: \"perc\", \"norm\", \"basic\", \"bc\", or \"bca\"\n\n\nrsample: \"perc\" or \"bca\"\n\n\nsimulation: argument ignored.\n\n\n\n\n\n\nconformal_test\n\n\nData frame of test data for conformal prediction.\n\n\n\n\nconformal_calibration\n\n\nData frame of calibration data for split conformal prediction (method=\"conformal_split).\n\n\n\n\nconformal_score\n\n\nString. Warning: The type argument in predictions() must generate predictions which are on the same scale as the outcome variable. Typically, this means that type must be \"response\" or \"probs\".\n\n\n\"residual_abs\" or \"residual_sq\" for regression tasks (numeric outcome)\n\n\n\"softmax\" for classification tasks (when predictions() returns a group columns, such as multinomial or ordinal logit models.\n\n\n\n\n\n\n…\n\n\n\n\nIf method=“boot”, additional arguments are passed to boot::boot().\n\n\nIf method=“fwb”, additional arguments are passed to fwb::fwb().\n\n\nIf method=“rsample”, additional arguments are passed to rsample::bootstraps().\n\n\nAdditional arguments are ignored for all other methods.\n\n\n\n\n\n\n\n\nWhen method=“simulation”, we conduct simulation-based inference following the method discussed in Krinsky & Robb (1986):\n\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model’s estimated coefficients and variance equal to the model’s variance-covariance matrix (classical, \"HC3\", or other).\n\n\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, slopes, or hypotheses.\n\n\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\n\nWhen method=“fwb”, drawn weights are supplied to the model fitting function’s weights argument; if the model doesn’t accept non-integer weights, this method should not be used. If weights were included in the original model fit, they are extracted by weights() and multiplied by the drawn weights. These weights are supplied to the wts argument of the estimation function (e.g., comparisons()).\n\n\n\nA marginaleffects object with simulation or bootstrap resamples and objects attached.\n\n\n\nKrinsky, I., and A. L. Robb. 1986. “On Approximating the Statistical Properties of Elasticities.” Review of Economics and Statistics 68 (4): 715–9.\nKing, Gary, Michael Tomz, and Jason Wittenberg. \"Making the most of statistical analyses: Improving interpretation and presentation.\" American journal of political science (2000): 347-361\nDowd, Bryan E., William H. Greene, and Edward C. Norton. \"Computation of standard errors.\" Health services research 49.2 (2014): 731-750.\nAngelopoulos, Anastasios N., and Stephen Bates. 2022. \"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.\" arXiv. https://doi.org/10.48550/arXiv.2107.07511.\nBarber, Rina Foygel, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. 2020. “Predictive Inference with the Jackknife+.” arXiv. http://arxiv.org/abs/1905.02928.\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nlibrary(magrittr)\nset.seed(1024)\nmod &lt;- lm(Sepal.Length ~ Sepal.Width * Species, data = iris)\n\n# bootstrap\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"boot\")\n\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"rsample\")\n\n# Fractional (bayesian) bootstrap\navg_slopes(mod, by = \"Species\") %&gt;%\n  inferences(method = \"fwb\") %&gt;%\n  posterior_draws(\"rvar\") %&gt;%\n  data.frame()\n\n# Simulation-based inference\nslopes(mod) %&gt;%\n  inferences(method = \"simulation\") %&gt;%\n  head()"
  },
  {
    "objectID": "man/print.marginaleffects.html",
    "href": "man/print.marginaleffects.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Print marginaleffects objects\n\n\nThis function controls the text which is printed to the console when one of the core marginalefffects functions is called and the object is returned: predictions(), comparisons(), slopes(), marginal_means(), hypotheses(), avg_predictions(), avg_comparisons(), avg_slopes().\nAll of those functions return standard data frames. Columns can be extracted by name, predictions(model)$estimate, and all the usual data manipulation functions work out-of-the-box: colnames(), head(), subset(), dplyr::filter(), dplyr::arrange(), etc.\nSome of the data columns are not printed by default. You can disable pretty printing and print the full results as a standard data frame using the style argument or by applying as.data.frame() on the object. See examples below.\n\n\n\n## S3 method for class 'marginaleffects'\nprint(\n  x,\n  digits = getOption(\"marginaleffects_print_digits\", default = 3),\n  p_eps = getOption(\"marginaleffects_print_p_eps\", default = 0.001),\n  topn = getOption(\"marginaleffects_print_topn\", default = 5),\n  nrows = getOption(\"marginaleffects_print_nrows\", default = 30),\n  ncols = getOption(\"marginaleffects_print_ncols\", default = 30),\n  style = getOption(\"marginaleffects_print_style\", default = \"summary\"),\n  type = getOption(\"marginaleffects_print_type\", default = TRUE),\n  column_names = getOption(\"marginaleffects_print_column_names\", default = TRUE),\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nAn object produced by one of the marginaleffects package functions.\n\n\n\n\ndigits\n\n\nThe number of digits to display.\n\n\n\n\np_eps\n\n\np values smaller than this number are printed in \"&lt;0.001\" style.\n\n\n\n\ntopn\n\n\nThe number of rows to be printed from the beginning and end of tables with more than nrows rows.\n\n\n\n\nnrows\n\n\nThe number of rows which will be printed before truncation.\n\n\n\n\nncols\n\n\nThe maximum number of column names to display at the bottom of the printed output.\n\n\n\n\nstyle\n\n\n\"summary\" or \"data.frame\"\n\n\n\n\ntype\n\n\nboolean: should the type be printed?\n\n\n\n\ncolumn_names\n\n\nboolean: should the column names be printed?\n\n\n\n\n…\n\n\nOther arguments are currently ignored.\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + am + factor(gear), data = mtcars)\np &lt;- predictions(mod, by = c(\"am\", \"gear\"))\np\n\n\n am gear Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0    3     16.1      0.759 21.2   &lt;0.001 329.6  14.6   17.6\n  0    4     21.0      1.470 14.3   &lt;0.001 152.1  18.2   23.9\n  1    4     26.3      1.039 25.3   &lt;0.001 466.1  24.2   28.3\n  1    5     21.4      1.315 16.3   &lt;0.001 195.2  18.8   24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nsubset(p, am == 1)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n     26.3       1.04 25.3   &lt;0.001 466.1   24.2    28.3\n     21.4       1.31 16.3   &lt;0.001 195.2   18.8    24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nprint(p, style = \"data.frame\")\n\n  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.046968e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697545  14.32212  1.591933e-46 152.1379 18.16933\n3  1    4 26.27500 1.0392747  25.28205 5.032443e-141 466.0606 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93067\n3  28.31194\n4  23.95655\n\ndata.frame(p)\n\n  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.046968e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697545  14.32212  1.591933e-46 152.1379 18.16933\n3  1    4 26.27500 1.0392747  25.28205 5.032443e-141 466.0606 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93067\n3  28.31194\n4  23.95655"
  },
  {
    "objectID": "man/print.marginaleffects.html#print.marginaleffects",
    "href": "man/print.marginaleffects.html#print.marginaleffects",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Print marginaleffects objects\n\n\nThis function controls the text which is printed to the console when one of the core marginalefffects functions is called and the object is returned: predictions(), comparisons(), slopes(), marginal_means(), hypotheses(), avg_predictions(), avg_comparisons(), avg_slopes().\nAll of those functions return standard data frames. Columns can be extracted by name, predictions(model)$estimate, and all the usual data manipulation functions work out-of-the-box: colnames(), head(), subset(), dplyr::filter(), dplyr::arrange(), etc.\nSome of the data columns are not printed by default. You can disable pretty printing and print the full results as a standard data frame using the style argument or by applying as.data.frame() on the object. See examples below.\n\n\n\n## S3 method for class 'marginaleffects'\nprint(\n  x,\n  digits = getOption(\"marginaleffects_print_digits\", default = 3),\n  p_eps = getOption(\"marginaleffects_print_p_eps\", default = 0.001),\n  topn = getOption(\"marginaleffects_print_topn\", default = 5),\n  nrows = getOption(\"marginaleffects_print_nrows\", default = 30),\n  ncols = getOption(\"marginaleffects_print_ncols\", default = 30),\n  style = getOption(\"marginaleffects_print_style\", default = \"summary\"),\n  type = getOption(\"marginaleffects_print_type\", default = TRUE),\n  column_names = getOption(\"marginaleffects_print_column_names\", default = TRUE),\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nAn object produced by one of the marginaleffects package functions.\n\n\n\n\ndigits\n\n\nThe number of digits to display.\n\n\n\n\np_eps\n\n\np values smaller than this number are printed in \"&lt;0.001\" style.\n\n\n\n\ntopn\n\n\nThe number of rows to be printed from the beginning and end of tables with more than nrows rows.\n\n\n\n\nnrows\n\n\nThe number of rows which will be printed before truncation.\n\n\n\n\nncols\n\n\nThe maximum number of column names to display at the bottom of the printed output.\n\n\n\n\nstyle\n\n\n\"summary\" or \"data.frame\"\n\n\n\n\ntype\n\n\nboolean: should the type be printed?\n\n\n\n\ncolumn_names\n\n\nboolean: should the column names be printed?\n\n\n\n\n…\n\n\nOther arguments are currently ignored.\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + am + factor(gear), data = mtcars)\np &lt;- predictions(mod, by = c(\"am\", \"gear\"))\np\n\n\n am gear Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0    3     16.1      0.759 21.2   &lt;0.001 329.6  14.6   17.6\n  0    4     21.0      1.470 14.3   &lt;0.001 152.1  18.2   23.9\n  1    4     26.3      1.039 25.3   &lt;0.001 466.1  24.2   28.3\n  1    5     21.4      1.315 16.3   &lt;0.001 195.2  18.8   24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nsubset(p, am == 1)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n     26.3       1.04 25.3   &lt;0.001 466.1   24.2    28.3\n     21.4       1.31 16.3   &lt;0.001 195.2   18.8    24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nprint(p, style = \"data.frame\")\n\n  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.046968e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697545  14.32212  1.591933e-46 152.1379 18.16933\n3  1    4 26.27500 1.0392747  25.28205 5.032443e-141 466.0606 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93067\n3  28.31194\n4  23.95655\n\ndata.frame(p)\n\n  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.046968e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697545  14.32212  1.591933e-46 152.1379 18.16933\n3  1    4 26.27500 1.0392747  25.28205 5.032443e-141 466.0606 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93067\n3  28.31194\n4  23.95655"
  },
  {
    "objectID": "man/predictions.html",
    "href": "man/predictions.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Predictions\n\n\nOutcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \"reference grid\").\n\n\npredictions(): unit-level (conditional) estimates.\n\n\navg_predictions(): average (marginal) estimates.\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the predictions vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/predictions.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\npredictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = FALSE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = TRUE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate predictions.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level predictions for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\nstring:\n\n\n\"mean\": Predictions at the Mean. Predictions when each predictor is held at its mean or mode.\n\n\n\"median\": Predictions at the Median. Predictions when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Predictions at Marginal Means. See Details section below.\n\n\n\"tukey\": Predictions at Tukey’s 5 numbers.\n\n\n\"grid\": Predictions on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nCounterfactual variables.\n\n\nOutput:\n\n\npredictions(): The entire dataset is replicated once for each unique combination of variables, and predictions are made.\n\n\navg_predictions(): The entire dataset is replicated, predictions are made, and they are marginalized by variables categories.\n\n\nWarning: This can be expensive in large datasets.\n\n\nWarning: Users who need \"conditional\" predictions should use the newdata argument instead of variables.\n\n\n\n\nInput:\n\n\nNULL: computes one prediction per row of newdata\n\n\nCharacter vector: the dataset is replicated once of every combination of unique values of the variables identified in variables.\n\n\nNamed list: names identify the subset of variables of interest and their values. For numeric variables, the variables argument supports functions and string shortcuts:\n\n\nA function which returns a numeric value\n\n\nNumeric vector: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\"threenum\": mean and 1 standard deviation on both sides\n\n\n\"fivenum\": Tukey’s five numbers\n\n\n\n\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nbyfun\n\n\nA function such as mean() or sum() used to aggregate estimates within the subgroups defined by the by argument. NULL uses the mean() function. Must accept a numeric vector and return a single numeric value. This is sometimes used to take the sum or mean of predicted probabilities across outcome or predictor levels. See examples section.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA data.frame with one row per observation and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nestimate: predicted outcome\n\n\nstd.error: standard errors computed using the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_predictions(): Average predictions\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n# Adjusted Prediction for every row of the original dataset\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\npred &lt;- predictions(mod)\nhead(pred)\n\n# Adjusted Predictions at User-Specified Values of the Regressors\npredictions(mod, newdata = datagrid(hp = c(100, 120), cyl = 4))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Average Adjusted Predictions (AAP)\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp * am * vs, mtcars)\n\navg_predictions(mod)\n\npredictions(mod, by = \"am\")\n\n# Conditional Adjusted Predictions\nplot_predictions(mod, condition = \"hp\")\n\n# Counterfactual predictions with the `variables` argument\n# the `mtcars` dataset has 32 rows\n\nmod &lt;- lm(mpg ~ hp + am, data = mtcars)\np &lt;- predictions(mod)\nhead(p)\nnrow(p)\n\n# average counterfactual predictions\navg_predictions(mod, variables = \"am\")\n\n# counterfactual predictions obtained by replicating the entire for different\n# values of the predictors\np &lt;- predictions(mod, variables = list(hp = c(90, 110)))\nnrow(p)\n\n\n# hypothesis test: is the prediction in the 1st row equal to the prediction in the 2nd row\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 = b2\")\n\n# same hypothesis test using row indices\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = lc)\n\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\npredictions(mod, by = c(\"am\", \"vs\"))\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n# first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n\n# average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n\n# sum of predicted probabilities for combined response levels\nmod &lt;- multinom(factor(cyl) ~ mpg + am, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    by = c(\"4,6\", \"4,6\", \"8\"),\n    group = as.character(c(4, 6, 8)))\npredictions(mod, newdata = \"mean\", byfun = sum, by = by)"
  },
  {
    "objectID": "man/predictions.html#predictions",
    "href": "man/predictions.html#predictions",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Predictions\n\n\nOutcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \"reference grid\").\n\n\npredictions(): unit-level (conditional) estimates.\n\n\navg_predictions(): average (marginal) estimates.\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the predictions vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/predictions.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\npredictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = FALSE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = TRUE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate predictions.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level predictions for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\nstring:\n\n\n\"mean\": Predictions at the Mean. Predictions when each predictor is held at its mean or mode.\n\n\n\"median\": Predictions at the Median. Predictions when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Predictions at Marginal Means. See Details section below.\n\n\n\"tukey\": Predictions at Tukey’s 5 numbers.\n\n\n\"grid\": Predictions on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nCounterfactual variables.\n\n\nOutput:\n\n\npredictions(): The entire dataset is replicated once for each unique combination of variables, and predictions are made.\n\n\navg_predictions(): The entire dataset is replicated, predictions are made, and they are marginalized by variables categories.\n\n\nWarning: This can be expensive in large datasets.\n\n\nWarning: Users who need \"conditional\" predictions should use the newdata argument instead of variables.\n\n\n\n\nInput:\n\n\nNULL: computes one prediction per row of newdata\n\n\nCharacter vector: the dataset is replicated once of every combination of unique values of the variables identified in variables.\n\n\nNamed list: names identify the subset of variables of interest and their values. For numeric variables, the variables argument supports functions and string shortcuts:\n\n\nA function which returns a numeric value\n\n\nNumeric vector: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\"threenum\": mean and 1 standard deviation on both sides\n\n\n\"fivenum\": Tukey’s five numbers\n\n\n\n\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nbyfun\n\n\nA function such as mean() or sum() used to aggregate estimates within the subgroups defined by the by argument. NULL uses the mean() function. Must accept a numeric vector and return a single numeric value. This is sometimes used to take the sum or mean of predicted probabilities across outcome or predictor levels. See examples section.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA data.frame with one row per observation and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nestimate: predicted outcome\n\n\nstd.error: standard errors computed using the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_predictions(): Average predictions\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n# Adjusted Prediction for every row of the original dataset\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\npred &lt;- predictions(mod)\nhead(pred)\n\n# Adjusted Predictions at User-Specified Values of the Regressors\npredictions(mod, newdata = datagrid(hp = c(100, 120), cyl = 4))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Average Adjusted Predictions (AAP)\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp * am * vs, mtcars)\n\navg_predictions(mod)\n\npredictions(mod, by = \"am\")\n\n# Conditional Adjusted Predictions\nplot_predictions(mod, condition = \"hp\")\n\n# Counterfactual predictions with the `variables` argument\n# the `mtcars` dataset has 32 rows\n\nmod &lt;- lm(mpg ~ hp + am, data = mtcars)\np &lt;- predictions(mod)\nhead(p)\nnrow(p)\n\n# average counterfactual predictions\navg_predictions(mod, variables = \"am\")\n\n# counterfactual predictions obtained by replicating the entire for different\n# values of the predictors\np &lt;- predictions(mod, variables = list(hp = c(90, 110)))\nnrow(p)\n\n\n# hypothesis test: is the prediction in the 1st row equal to the prediction in the 2nd row\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 = b2\")\n\n# same hypothesis test using row indices\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = lc)\n\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\npredictions(mod, by = c(\"am\", \"vs\"))\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n# first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n\n# average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n\n# sum of predicted probabilities for combined response levels\nmod &lt;- multinom(factor(cyl) ~ mpg + am, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    by = c(\"4,6\", \"4,6\", \"8\"),\n    group = as.character(c(4, 6, 8)))\npredictions(mod, newdata = \"mean\", byfun = sum, by = by)"
  },
  {
    "objectID": "man/slopes.html",
    "href": "man/slopes.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Slopes (aka Partial derivatives, Marginal Effects, or Trends)\n\n\nPartial derivative of the regression equation with respect to a regressor of interest.\n\n\nslopes(): unit-level (conditional) estimates.\n\n\navg_slopes(): average (marginal) estimates.\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the slopes vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/slopes.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\nslopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = FALSE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = TRUE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the slopes.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level slopes for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\nstring:\n\n\n\"mean\": Marginal Effects at the Mean. Slopes when each predictor is held at its mean or mode.\n\n\n\"median\": Marginal Effects at the Median. Slopes when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Marginal Effects at Marginal Means. See Details section below.\n\n\n\"tukey\": Marginal Effects at Tukey’s 5 numbers.\n\n\n\"grid\": Marginal Effects on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nNULL: compute slopes or comparisons for all the variables in the model object (can be slow).\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\"dydx\": dY/dX\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\"eydx\": dY/dX * Y\n\n\n\"dyex\": dY/dX / X\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA \"slope\" or \"marginal effect\" is the partial derivative of the regression equation with respect to a variable in the model. This function uses automatic differentiation to compute slopes for a vast array of models, including non-linear models with transformations (e.g., polynomials). Uncertainty estimates are computed using the delta method.\nNumerical derivatives for the slopes function are calculated using a simple epsilon difference approach: \\(\\partial Y / \\partial X = (f(X + \\varepsilon/2) - f(X-\\varepsilon/2)) / \\varepsilon\\), where f is the predict() method associated with the model class, and \\(\\varepsilon\\) is determined by the eps argument.\n\n\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nterm: the variable whose marginal effect is computed\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\nstd.error: standard errors computed by via the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_slopes(): Average slopes\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n\n\n\n# Unit-level (conditional) Marginal Effects\nmod &lt;- glm(am ~ hp * wt, data = mtcars, family = binomial)\nmfx &lt;- slopes(mod)\nhead(mfx)\n\n# Average Marginal Effect (AME)\navg_slopes(mod, by = TRUE)\n\n\n# Marginal Effect at the Mean (MEM)\nslopes(mod, newdata = datagrid())\n\n# Marginal Effect at User-Specified Values\n# Variables not explicitly included in `datagrid()` are held at their means\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n# Group-Average Marginal Effects (G-AME)\n# Calculate marginal effects for each observation, and then take the average\n# marginal effect within each subset of observations with different observed\n# values for the `cyl` variable:\nmod2 &lt;- lm(mpg ~ hp * cyl, data = mtcars)\navg_slopes(mod2, variables = \"hp\", by = \"cyl\")\n\n# Marginal Effects at User-Specified Values (counterfactual)\n# Variables not explicitly included in `datagrid()` are held at their\n# original values, and the whole dataset is duplicated once for each\n# combination of the values in `datagrid()`\nmfx &lt;- slopes(mod,\n              newdata = datagrid(hp = c(100, 110),\n              grid_type = \"counterfactual\"))\nhead(mfx)\n\n# Heteroskedasticity robust standard errors\nmfx &lt;- slopes(mod, vcov = sandwich::vcovHC(mod))\nhead(mfx)\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)"
  },
  {
    "objectID": "man/slopes.html#slopes",
    "href": "man/slopes.html#slopes",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Slopes (aka Partial derivatives, Marginal Effects, or Trends)\n\n\nPartial derivative of the regression equation with respect to a regressor of interest.\n\n\nslopes(): unit-level (conditional) estimates.\n\n\navg_slopes(): average (marginal) estimates.\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the slopes vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/slopes.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\nslopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = FALSE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = TRUE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the slopes.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level slopes for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\nstring:\n\n\n\"mean\": Marginal Effects at the Mean. Slopes when each predictor is held at its mean or mode.\n\n\n\"median\": Marginal Effects at the Median. Slopes when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Marginal Effects at Marginal Means. See Details section below.\n\n\n\"tukey\": Marginal Effects at Tukey’s 5 numbers.\n\n\n\"grid\": Marginal Effects on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nNULL: compute slopes or comparisons for all the variables in the model object (can be slow).\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\"dydx\": dY/dX\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\"eydx\": dY/dX * Y\n\n\n\"dyex\": dY/dX / X\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA \"slope\" or \"marginal effect\" is the partial derivative of the regression equation with respect to a variable in the model. This function uses automatic differentiation to compute slopes for a vast array of models, including non-linear models with transformations (e.g., polynomials). Uncertainty estimates are computed using the delta method.\nNumerical derivatives for the slopes function are calculated using a simple epsilon difference approach: \\(\\partial Y / \\partial X = (f(X + \\varepsilon/2) - f(X-\\varepsilon/2)) / \\varepsilon\\), where f is the predict() method associated with the model class, and \\(\\varepsilon\\) is determined by the eps argument.\n\n\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nterm: the variable whose marginal effect is computed\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\nstd.error: standard errors computed by via the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_slopes(): Average slopes\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n\n\n\n# Unit-level (conditional) Marginal Effects\nmod &lt;- glm(am ~ hp * wt, data = mtcars, family = binomial)\nmfx &lt;- slopes(mod)\nhead(mfx)\n\n# Average Marginal Effect (AME)\navg_slopes(mod, by = TRUE)\n\n\n# Marginal Effect at the Mean (MEM)\nslopes(mod, newdata = datagrid())\n\n# Marginal Effect at User-Specified Values\n# Variables not explicitly included in `datagrid()` are held at their means\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n# Group-Average Marginal Effects (G-AME)\n# Calculate marginal effects for each observation, and then take the average\n# marginal effect within each subset of observations with different observed\n# values for the `cyl` variable:\nmod2 &lt;- lm(mpg ~ hp * cyl, data = mtcars)\navg_slopes(mod2, variables = \"hp\", by = \"cyl\")\n\n# Marginal Effects at User-Specified Values (counterfactual)\n# Variables not explicitly included in `datagrid()` are held at their\n# original values, and the whole dataset is duplicated once for each\n# combination of the values in `datagrid()`\nmfx &lt;- slopes(mod,\n              newdata = datagrid(hp = c(100, 110),\n              grid_type = \"counterfactual\"))\nhead(mfx)\n\n# Heteroskedasticity robust standard errors\nmfx &lt;- slopes(mod, vcov = sandwich::vcovHC(mod))\nhead(mfx)\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)"
  },
  {
    "objectID": "man/plot_predictions.html",
    "href": "man/plot_predictions.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Predictions\n\n\nPlot predictions on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal predictions, that is, predictions made on the original data, but averaged by subgroups. This is analogous to using the by argument in the predictions() function.\nThe condition argument is used to plot conditional predictions, that is, predictions made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a predictions() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below.\nSee the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_predictions(\n  model,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  transform = NULL,\n  points = 0,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\ncondition\n\n\nConditional predictions\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum\n\n\n\n\n\n\nby\n\n\nMarginal predictions\n\n\nCharacter vector (max length 3): Names of the categorical predictors to marginalize across.\n\n\n1: x-axis. 2: color. 3: facets.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the predictions() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\npoints\n\n\nNumber between 0 and 1 which controls the transparency of raw data points. 0 (default) does not display any points.\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object or data frame (if draw=FALSE)\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\nplot_predictions(mod, condition = \"wt\")\n\n\n\nmod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\nplot_predictions(mod, condition = c(\"hp\", \"wt\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = \"threenum\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = range))"
  },
  {
    "objectID": "man/plot_predictions.html#plot_predictions",
    "href": "man/plot_predictions.html#plot_predictions",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Predictions\n\n\nPlot predictions on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal predictions, that is, predictions made on the original data, but averaged by subgroups. This is analogous to using the by argument in the predictions() function.\nThe condition argument is used to plot conditional predictions, that is, predictions made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a predictions() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below.\nSee the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_predictions(\n  model,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  transform = NULL,\n  points = 0,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\ncondition\n\n\nConditional predictions\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum\n\n\n\n\n\n\nby\n\n\nMarginal predictions\n\n\nCharacter vector (max length 3): Names of the categorical predictors to marginalize across.\n\n\n1: x-axis. 2: color. 3: facets.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the predictions() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\npoints\n\n\nNumber between 0 and 1 which controls the transparency of raw data points. 0 (default) does not display any points.\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object or data frame (if draw=FALSE)\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\nplot_predictions(mod, condition = \"wt\")\n\n\n\nmod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\nplot_predictions(mod, condition = c(\"hp\", \"wt\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = \"threenum\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = range))"
  },
  {
    "objectID": "man/datagrid.html",
    "href": "man/datagrid.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Data grids\n\n\nGenerate a data grid of user-specified values for use in the newdata argument of the predictions(), comparisons(), and slopes() functions. This is useful to define where in the predictor space we want to evaluate the quantities of interest. Ex: the predicted outcome or slope for a 37 year old college graduate.\n\n\ndatagrid() generates data frames with combinations of \"typical\" or user-supplied predictor values.\n\n\ndatagridcf() generates \"counter-factual\" data frames, by replicating the entire dataset once for every combination of predictor values supplied by the user.\n\n\n\n\n\ndatagrid(\n  ...,\n  model = NULL,\n  newdata = NULL,\n  by = NULL,\n  FUN_character = get_mode,\n  FUN_factor = get_mode,\n  FUN_logical = get_mode,\n  FUN_numeric = function(x) mean(x, na.rm = TRUE),\n  FUN_integer = function(x) round(mean(x, na.rm = TRUE)),\n  FUN_other = function(x) mean(x, na.rm = TRUE),\n  grid_type = \"typical\"\n)\n\ndatagridcf(..., model = NULL, newdata = NULL)\n\n\n\n\n\n\n\n…\n\n\nnamed arguments with vectors of values or functions for user-specified variables.\n\n\nFunctions are applied to the variable in the model dataset or newdata, and must return a vector of the appropriate type.\n\n\nCharacter vectors are automatically transformed to factors if necessary. +The output will include all combinations of these variables (see Examples below.)\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\ndata.frame (one and only one of the model and newdata arguments can be used.)\n\n\n\n\nby\n\n\ncharacter vector with grouping variables within which FUN_* functions are applied to create \"sub-grids\" with unspecified variables.\n\n\n\n\nFUN_character\n\n\nthe function to be applied to character variables.\n\n\n\n\nFUN_factor\n\n\nthe function to be applied to factor variables.\n\n\n\n\nFUN_logical\n\n\nthe function to be applied to logical variables.\n\n\n\n\nFUN_numeric\n\n\nthe function to be applied to numeric variables.\n\n\n\n\nFUN_integer\n\n\nthe function to be applied to integer variables.\n\n\n\n\nFUN_other\n\n\nthe function to be applied to other variable types.\n\n\n\n\ngrid_type\n\n\ncharacter\n\n\n\"typical\": variables whose values are not explicitly specified by the user in … are set to their mean or mode, or to the output of the functions supplied to FUN_type arguments.\n\n\n\"counterfactual\": the entire dataset is duplicated for each combination of the variable values specified in …. Variables not explicitly supplied to datagrid() are set to their observed values in the original dataset.\n\n\n\n\n\n\n\n\nIf datagrid is used in a predictions(), comparisons(), or slopes() call as the newdata argument, the model is automatically inserted in the model argument of datagrid() call, and users do not need to specify either the model or newdata arguments.\nIf users supply a model, the data used to fit that model is retrieved using the insight::get_data function.\n\n\n\nA data.frame in which each row corresponds to one combination of the named predictors supplied by the user via the … dots. Variables which are not explicitly defined are held at their mean or mode.\n\n\n\n\n\ndatagridcf(): Counterfactual data grid\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n# The output only has 2 rows, and all the variables except `hp` are at their\n# mean or mode.\ndatagrid(newdata = mtcars, hp = c(100, 110))\n\n       mpg    cyl     disp     drat      wt     qsec     vs      am   gear\n1 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n2 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n    carb  hp\n1 2.8125 100\n2 2.8125 110\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndatagrid(model = mod, hp = c(100, 110))\n\n       mpg  hp\n1 20.09062 100\n2 20.09062 110\n\n# Use in `marginaleffects` to compute \"Typical Marginal Effects\". When used\n# in `slopes()` or `predictions()` we do not need to specify the\n#`model` or `newdata` arguments.\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp 100  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp 110  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# datagrid accepts functions\ndatagrid(hp = range, cyl = unique, newdata = mtcars)\n\n       mpg     disp     drat      wt     qsec     vs      am   gear   carb  hp\n1 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n2 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n3 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n4 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n5 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n6 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n  cyl\n1   6\n2   4\n3   8\n4   6\n5   4\n6   8\n\ncomparisons(mod, newdata = datagrid(hp = fivenum))\n\n\n Term Contrast  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp       +1  52  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1  96  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 123  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 180  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 335  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# The full dataset is duplicated with each observation given counterfactual\n# values of 100 and 110 for the `hp` variable. The original `mtcars` includes\n# 32 rows, so the resulting dataset includes 64 rows.\ndg &lt;- datagrid(newdata = mtcars, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndg &lt;- datagrid(model = mod, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64"
  },
  {
    "objectID": "man/datagrid.html#datagrid",
    "href": "man/datagrid.html#datagrid",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Data grids\n\n\nGenerate a data grid of user-specified values for use in the newdata argument of the predictions(), comparisons(), and slopes() functions. This is useful to define where in the predictor space we want to evaluate the quantities of interest. Ex: the predicted outcome or slope for a 37 year old college graduate.\n\n\ndatagrid() generates data frames with combinations of \"typical\" or user-supplied predictor values.\n\n\ndatagridcf() generates \"counter-factual\" data frames, by replicating the entire dataset once for every combination of predictor values supplied by the user.\n\n\n\n\n\ndatagrid(\n  ...,\n  model = NULL,\n  newdata = NULL,\n  by = NULL,\n  FUN_character = get_mode,\n  FUN_factor = get_mode,\n  FUN_logical = get_mode,\n  FUN_numeric = function(x) mean(x, na.rm = TRUE),\n  FUN_integer = function(x) round(mean(x, na.rm = TRUE)),\n  FUN_other = function(x) mean(x, na.rm = TRUE),\n  grid_type = \"typical\"\n)\n\ndatagridcf(..., model = NULL, newdata = NULL)\n\n\n\n\n\n\n\n…\n\n\nnamed arguments with vectors of values or functions for user-specified variables.\n\n\nFunctions are applied to the variable in the model dataset or newdata, and must return a vector of the appropriate type.\n\n\nCharacter vectors are automatically transformed to factors if necessary. +The output will include all combinations of these variables (see Examples below.)\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\ndata.frame (one and only one of the model and newdata arguments can be used.)\n\n\n\n\nby\n\n\ncharacter vector with grouping variables within which FUN_* functions are applied to create \"sub-grids\" with unspecified variables.\n\n\n\n\nFUN_character\n\n\nthe function to be applied to character variables.\n\n\n\n\nFUN_factor\n\n\nthe function to be applied to factor variables.\n\n\n\n\nFUN_logical\n\n\nthe function to be applied to logical variables.\n\n\n\n\nFUN_numeric\n\n\nthe function to be applied to numeric variables.\n\n\n\n\nFUN_integer\n\n\nthe function to be applied to integer variables.\n\n\n\n\nFUN_other\n\n\nthe function to be applied to other variable types.\n\n\n\n\ngrid_type\n\n\ncharacter\n\n\n\"typical\": variables whose values are not explicitly specified by the user in … are set to their mean or mode, or to the output of the functions supplied to FUN_type arguments.\n\n\n\"counterfactual\": the entire dataset is duplicated for each combination of the variable values specified in …. Variables not explicitly supplied to datagrid() are set to their observed values in the original dataset.\n\n\n\n\n\n\n\n\nIf datagrid is used in a predictions(), comparisons(), or slopes() call as the newdata argument, the model is automatically inserted in the model argument of datagrid() call, and users do not need to specify either the model or newdata arguments.\nIf users supply a model, the data used to fit that model is retrieved using the insight::get_data function.\n\n\n\nA data.frame in which each row corresponds to one combination of the named predictors supplied by the user via the … dots. Variables which are not explicitly defined are held at their mean or mode.\n\n\n\n\n\ndatagridcf(): Counterfactual data grid\n\n\n\n\n\n\nlibrary(marginaleffects)\n\n# The output only has 2 rows, and all the variables except `hp` are at their\n# mean or mode.\ndatagrid(newdata = mtcars, hp = c(100, 110))\n\n       mpg    cyl     disp     drat      wt     qsec     vs      am   gear\n1 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n2 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n    carb  hp\n1 2.8125 100\n2 2.8125 110\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndatagrid(model = mod, hp = c(100, 110))\n\n       mpg  hp\n1 20.09062 100\n2 20.09062 110\n\n# Use in `marginaleffects` to compute \"Typical Marginal Effects\". When used\n# in `slopes()` or `predictions()` we do not need to specify the\n#`model` or `newdata` arguments.\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp 100  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp 110  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# datagrid accepts functions\ndatagrid(hp = range, cyl = unique, newdata = mtcars)\n\n       mpg     disp     drat      wt     qsec     vs      am   gear   carb  hp\n1 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n2 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n3 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n4 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n5 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n6 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n  cyl\n1   6\n2   4\n3   8\n4   6\n5   4\n6   8\n\ncomparisons(mod, newdata = datagrid(hp = fivenum))\n\n\n Term Contrast  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp       +1  52  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1  96  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 123  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 180  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 335  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# The full dataset is duplicated with each observation given counterfactual\n# values of 100 and 110 for the `hp` variable. The original `mtcars` includes\n# 32 rows, so the resulting dataset includes 64 rows.\ndg &lt;- datagrid(newdata = mtcars, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndg &lt;- datagrid(model = mod, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64"
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\nThe comparisons() now uses “forward contrasts” by default for numeric predictors, instead of “centered contrasts”. This can lead to small numerical differences in non-linear models.\nThe variables argument of the comparisons() function no longer accepts numeric vectors unless they are of length 2, specifying the low and high contrast values. This is to avoid ambiguity between the two vector version. Users should supply a data frame or a function instead. This is nearly as easy, and removes ambiguity.\n\nNew supported packages:\n\ndbarts: https://cran.r-project.org/package=dbarts\nmvgam: https://nicholasjclark.github.io/mvgam/ Not available on CRAN yet, but this package maintains its own marginaleffects support function.\nrms::Gls: https://cran.r-project.org/package=rms\n\nMisc:\n\ncomparisons(): The variables argument now accepts functions and data frames for factor, character, and logical variables.\nDeprecation warning for: plot_cap(), plot_cme(), and plot_cco(). These function names will be removed in version 1.0.0.\noptions(modelsummary_factory_default=...) is respected in Quarto and Rmarkdown documents.\n\nBugs:\n\nwts argument now respected in avg_slopes() for binary variables. Thanks to @trose64 for report #961\nCustom functions in the comparison argument of comparisons() did not supply the correct x vector length for bayesian models when the by argument is used. Thanks to @Sandhu-SS for report #931.\nAdd support for two facet variables (through facet_grid) when plotting using condition\ncomparisons(): When variables is a vector of length two and newdata has exactly two columns, there was ambiguity between custom vectors and length two vector of contrasts. But reported by C. Rainey on Twitter.\n\n\n\n\nMachine learning support:\n\ntidymodels package\nmlr3 package\n\nMisc:\n\nNew vignettes:\n\nInverse Probability Weighting\nMachine Learning\nMatching\n\nAdd support for hypotheses() to inferences(). Thanks to @Tristan-Siegfried for code contribution #908.\nSupport survival::survreg(). Thanks to Carlisle Rainey for Report #911.\ncolumn_names argument in print.marginaleffects() to suppress the printed column names at the bottom of the printout.\nThe function supplied to the comparison argument of the comparisons() function can now operate on x and on newdata directly (e.g., to check the number of observations).\nMore informative errors from predict().\n\nBugs:\n\nSome gamlss models generated an error related to the what argument. Thanks to @DHLocke for Issue #933\n\n\n\n\n\nhypotheses(): The FUN argument handles group columns gracefully.\nNative support for Amelia for multiple imputation.\n\nDocumentation:\n\nNew section on “Complex aggregations” in the Hypothesis testing vignette.\n\nBug fix:\n\nResults of the predictions() function could be inaccurate when (a) running version 0.15.0, (b) type is NULL or invlink(link), (c) model is glm(), and (d) the hypothesis argument is non-numeric. Thanks to @strengejacke for report #903\n\n\n\n\nNew:\n\nConformal prediction via inferences()\nhypothesis argument now accepts multiple string formulas.\nThe type argument now accepts an explicit invlink(link) value instead of silently back-transforming. Users are no longer pointed to type_dictionary. Instead, they should call their function with a bad type value, and they will obtain a list of valid types. The default type value is printed in the output. This is useful because the default type value is NULL, so the user often does not explicitly decide.\nAllow install with Rcpp 1.0.0 and greater.\n\nSupport new models:\n\nsurvey::svyolr()\n\nMisc:\n\ninferences(method=\"simulation\") uses the original point estimate rather than the mean of the simulation distribution. Issue #851.\nBetter documentation and error messages for newdata=NULL\nSome performance improvements for predictions() and marginalmeans() (#880, #882, @etiennebacher).\n\nBug fix:\n\nnewdata=\"median\" returned mean of binary variables. Thanks to @jkhanson1970 for report #896.\n\n\n\n\nBreaking changes:\n\nRow order of the output changes for some objects. Rows are not sorted alphabetically by term, by, and variables explicitly supplied to datagrid. This can affect hypothesis tests computed using the b1, b2, b3, and other indices.\nNew procedure numderiv argument use a different procedure to select the step size used in the finite difference numeric derivative used to compute standard errors: abs(x) * sqrt(.Machine$double.eps). The numerical results may not be exactly identical to previous versions of marginaleffects, but the step size should be adequate in a broader variety of cases. Note that users can use the numderiv argument for more control on numeric differentiation, as documented.\nbife models are no longer supported pending investigation in weird results in the tests. Looking for volunteers write more thorough tests.\n\nNew:\n\nSupport: logistf package.\nSupport: DCchoice package.\nSupport: stats::nls\nhypotheses() can now accept raw data frame, which gives a lot of flexibility for custom contrasts and functions. See the Hypothesis vignette for an example.\nnumderiv argument allows users to use finite difference (center or forward) or Richardson’s method to compute the numerical derivatives used in the calculation of standard errors.\n\nBug fixes:\n\ninferences() supports the cross argument for comparisons() objects. Thanks to Kirill Solovev for report #856.\nsplines::bs() in formulas could produce incorrect results due to weirdness in stats::model.matrix(). Thanks to @chiungming for report #831.\nmgcv with ocat are now supported. Thanks to Lorenzo Fabbri for Issue #844.\nquantreg problem with rowid merge did not affect estimates but did not return the full original data. Issue #829.\nget_modeldata() extracts weights variable when available.\npredictions() is no longer broken in some inferences() calls. Issue #853\nInaccurate averaging with comparison=differenceavg some models where all predictors are categorical. Thanks to Karl Ove Hufthammer for report #865.\n\nMisc:\n\nMajor refactor to simplify the code base and make maintenance easier.\n\n\n\n\nBreaking change:\n\nglmmTMB: Standard errors are no longer supported because they may have been erroneous. Follow Issue #810 on Github for developments: https://github.com/vincentarelbundock/marginaleffects/issues/810\n\nNew:\n\nhypothesis argument accepts wildcards: hypothesis = \"b*=b1\"\ns.value column in all output: Shannon transforms for p values. See Greenland (2019).\nmarginal_means supports mira (mice objects).\ncomparisons(): The variables arguments now accepts arbitrary numeric vectors of length equal to the number of rows in newdata. This allows users to specify fully custom treatment sizes. In the documentation examples, we show how to estimate the difference for a 1 standard deviation shift in a regressor, where the standard deviation is calculated on a group-wise basis.\ncomparisons(): the variables argument now accepts “revpairwise”, “revsequential”, “revreference” for factor and character variables.\ncomparisons(): the comparison argument now accept “lift” and “liftavg”.\n\nPerformance:\n\nComputing elasticities for linear models is now up to 30% faster (#787, @etiennebacher).\n\nBug fixes:\n\nBetter handling of environments when newdata is a function call. Thanks to @jcccf for report #814 and to @capnrefsmmat for the proposed fix using the rlang package.\nDegrees of freedom mismatch for joint hypothesis tests. Thanks to @snhansen for report #789.\n\n\n\n\nBreaking change:\n\nRow order of output has changed for many calls, especially those using the by argument. This may break hypothesis tests conducted by indexing b1, b2, etc. This was necessary to fix Issue #776. Thanks to @marcora for the report.\n\nNew:\n\nhypotheses(): Joint hypothesis tests (F and Chi-square) with the joint and joint_test arguments.\nvcov.hypotheses method.\nwts is now available in plot_predictions(), plot_comparisons(), and plot_slopes().\n\nBug:\n\nWrong order of rows in bayesian models with by argument. Thanks to @shirdekel for report #782.\n\n\n\n\n\nvcov() and coef() methods for marginaleffects objects.\nStrings in wts are accepted with the by argument.\npredictions() and avg_predictions() no longer use an automatic backtransformation for GLM models unless hypothesis is NULL.\nvcov() can be used to retrieve a full variance-covariance matrix from objects produced by comparisons(), slopes(), predictions(), or marginal_means() objects.\nWhen processing objects obtained using mice multiple imputation, the pooled model using mice::pool is attached to the model attribute of the output. This means that functions like modelsummary::modelsummary() will not erroneously report goodness-of-fit statistics from just a single model and will instead appropriately report the statistics for the pooled model. Thanks to @Tristan-Siegfried for PR #740.\nMore informative error messages on some prediction problems. Thanks to @andymilne for Report #751.\n\nPerformance:\n\ninferences() is now up to 17x faster and much more memory-efficient when method is \"boot\" or \"rsample\" (#770, #771, @etiennebacher).\n\nBugs:\n\nbrms models with nl=TRUE and a single predictor generated an error. Thanks to @Tristan-Siegried for Report #759.\navg_predictions(): Incorrect group-wise averaging when all predictors are categorical, the variables variable is used, and we are averaging with avg_ or the by argument. Thanks to BorgeJorge for report #766.\nBug when datagrid() when called inside a user-written function. Thanks to @NickCH-K for report #769 and to @capnrefsmmat for the diagnostics.\n\n\n\n\nBreaking change:\n\nRow orders are now more consistent, but may have changed from previous version. This could affect results from hypothesis with b1, b2, … indexing.\n\nSupport new models:\n\nnlme::lme()\nphylolm::phylolm()\nphylolm::phyloglm()\n\nNew:\n\nVignette on 2x2 experimental designs. Thanks to Demetri Pananos.\ncomparisons() accepts data frames with two numeric columns (“low” and “high”) to specify fully customizable contrasts.\ndatagrid() gets a new by argument to create apply grid-making functions within groups.\nplot_*() gain a newdata argument for use with by.\n\nBug:\n\ncomparisons(comparison = \"lnratioavg\") ignored wts argument. Thanks to Demetri Pananos for report #737.\nordinal::clm(): incorrect standard errors when location and scale parameters are the same. Thanks to MrJerryTAO for report #718.\nIncorrect label for “2sd” comparisons. Thanks to Andy Milne for report #720.\nInvalid factor levels in datagrid() means newdata argument gets ignored. Thanks to Josh Errickson for report #721.\nError in models with only categorical predictors and the by argument. Thanks to Sam Brilleman for report #723.\nElasticities are now supported for ordinal::clm() models. Thanks to MrJerryTAO for report #729.\nglmmTMB models with zero-inflated components are supported. Thanks to @Helsinki-Ronan and @strengejacke for report #734.\n\n\n\n\nBreaking changes:\n\ntype column is replaced by type attribute.\npredictions() only works with officially supported model types (same list as comparisons() and slopes()).\n\nRenamed arguments (backward compatibility is preserved):\n\ntransform_pre -&gt; comparison\ntransform_post -&gt; transform\n\nNew:\n\np_adjust argument: Adjust p-values for multiple comparisons.\nequivalence argument available everywhere.\n\nPerformance:\n\nMuch faster results in avg_*() functions for models with only categorical predictors and many rows of data, using deduplication and weights instead of unit-level estimates.\nFaster predictions in lm() and glm() models using RcppEigen.\nBayesian models with many rows. Thanks to Etienne Bacher. #694\nFaster predictions, especially with standard errors and large datasets.\n\nBugs:\n\nMultiple imputation with mira objects was not pooling all datasets. Thanks to @Generalized for report #711.\nSupport for more models with offsets. Thanks to @mariofiorini for report #705.\nError on predictions() with by and wts. Thanks to Noah Greifer for report #695.\nafex: some models generated errors. Thanks to Daniel Lüdecke for report #696.\ngroup column name is always forbidden. Thanks to Daniel Lüdecke for report #697.\nBlank graphs in plot_comparisons() with a list in variables.\ntype=\"link\" produced an error with some categorical brms models. Thanks to @shirdekel for report #703.\nError on predictions(variables = ...) for glmmTMB models. Thanks to Daniel Lüdecke for report #707.\nby with user-specified function in comparison and factor predictor did not aggregate correctly. Thanks to @joaotedde for report #715.\nordinal::clm: Support cum.prob and linear.predictor prediction types. Thanks to @MrJerryTAO for report #717.\n\n\n\n\nPerformance:\n\n2-4x faster execution for many calls. Thanks to Etienne Bacher.\n\nNew models supported:\n\nMCMCglmm::MCMCglmm\nRchoice::hetprob\nRchoice::ivpml\nMultiple imputation using mice and any package which can return a list of imputed data frames (e.g., Amelia, missRanger, etc.)\n\nPlot improvements:\n\nNew by argument to display marginal estimates by subgroup.\nNew rug argument to display tick marks in the margins.\nNew points argument in plot_predictions() to display a scatter plot.\nNew gray argument to plot in grayscale using line types and shapes instead of color.\nThe effect argument is renamed to variables in plot_slopes() and plot_comparisons(). This improves consistency with the analogous slopes() and comparisons() functions.\nThe plotting vignette was re-written.\n\nOther:\n\nSupport multiple imputation with mice mira objects. The multiple imputation vignette was rewritten.\nThe variables_grid argument in marginal_means() is renamed newdata. Backward compatibility is maintained.\navg_*() returns an informative error when vcov is “satterthwaite” or “kenward-roger”\n“satterthwaite” and “kenward-roger” are now supported when newdata is not NULL\nInformative error when hypothesis includes a b# larger than the available number of estimates.\navg_predictions(model, variables = \"x\") computes average counterfactual predictions by subgroups of x\ndatagrid() and plot_*() functions are faster in datasets with many extraneous columns.\nIn predictions(type = NULL) with glm() and Gam() we first make predictions on the link scale and then backtransform them. Setting type=\"response\" explicitly makes predictions directly on the response scale without backtransformation.\nStandard errors now supported for more glmmTMB models.\nUse the numDeriv package for numeric differentiation in the calculation of delta method standard error. A global option can now be passed to numDeriv::jacobian:\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = list(method = \"Richardson\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = NULL)\n\nPrint:\n\nPrint fewer significant digits.\nprint.marginaleffects now prints all columns supplied to newdata\nLess redundant labels when using hypothesis\n\nMany improvements to documentation.\n\nBugfixes:\n\nStandard errors could be inaccurate in models with non-linear components (and interactions) when some of the coefficients were very small. This was related to the step size used for numerical differentiation for the delta method. Issue #684.\navg_predictions(by =) did not work when the dataset included a column named term. Issue #683.\nbrms models with multivariate outcome collapsed categories in comparisons(). Issue #639.\nhypotheses() now works on lists and in calls to lapply(), purrr::map(), etc. Issue #660.\n\n\n\n\nBreaking changes:\n\nAll functions return an estimate column instead of the function-specific predicted, comparisons, dydx, etc. This change only affects unit-level estimates, and not average estimates, which already used the estimate column name.\nThe transform_avg argument in tidy() deprecated. Use transform_post instead.\nplot_*(draw=FALSE) now return the actual variable names supplied to the condition argument, rather than the opaque “condition1”, “condition2”, etc.\n\nNew models supported:\n\nblme package.\n\nNew features:\n\nNew functions: avg_predictions(), avg_comparisons(), avg_slopes()\nEquivalence, non-inferiority, and non-superiority tests with the hypotheses() function and equivalence argument.\nNew experimental inferences() function: simulation-based inferences and bootstrap using the boot, rsample, and fwb package.\nNew df argument to set degrees of freedom manually for p and CI.\nPretty print() for all objects.\nby argument\n\nTRUE returns average (marginal) predictions, comparisons, or slopes.\nSupports bayesian models.\n\nhypothesis argument\n\nNumeric value sets the null used in calculating Z and p.\nExample: comparisons(mod, transform_pre = \"ratio\", hypothesis = 1)\n\nAll arguments from the main functions are now available through tidy(), and summary(): conf_level, transform_post, etc.\nBayesian posterior distribution summaries (median, mean, HDI, quantiles) can be customized using global options. See ?comparisons\n\nRenamed functions (backward-compatibility is maintained by keeping the old function names as aliases):\n\nmarginaleffects() -&gt; slopes()\nposteriordraws() -&gt; posterior_draws()\nmarginalmeans() -&gt; marginal_means()\nplot_cap() -&gt; plot_predictions()\nplot_cme() -&gt; plot_slopes()\nplot_cco() -&gt; plot_comparisons()\n\nBug fixes:\n\nIncorrect results: In 0.8.1, plot_*() the threenum and minmax labels did not correspond to the correct numeric values.\nFix corner case for slopes when the dataset includes infinite values.\nmlogit error with factors.\nThe vcov argument now accepts functions for most models.\n\nOther:\n\nRemoved major performance bottleneck for slopes()\n\n\n\n\n\ndeltamethod() can run hypothesis tests on objects produced by the comparisons(), marginaleffects(), predictions(), and marginalmeans() functions. This feature relies on match.call(), which means it may not always work when used programmatically, inside functions and nested environments. It is generally safer and more efficient to use the hypothesis argument.\nplot_cme() and plot_cco() accept lists with user-specified values for the regressors, and can display nice labels for shortcut string-functions like “threenum” or “quartile”.\nposterior_draws: new shape argument to return MCMC draws in various formats, including the new rvar structure from the posterior package.\ntransform_avg function gets printed in summary() output.\ntransform_post and transform_avg support string shortcuts: “exp” and “ln”\nAdded support for mlm models from lm(). Thanks to Noah Greifer.\n\nBug fixes:\n\nhypothesis argument with bayesian models and tidy() used to raise an error.\nMissing values for some regressors in the comparisons() output for brms models.\n\n\n\n\nBreaking change:\n\nThe interaction argument is deprecated and replaced by the cross argument. This is to reduce ambiguity with respect to the interaction argument in emmeans, which does something completely different, akin to the difference-in-differences illustrated in the Interactions vignette.\n\n71 classes of models supported, including the new:\n\nrms::ols\nrms::lrm\nrms::orm\n\nNew features:\n\nPlots: plot_cme(), plot_cap(), and plot_cco() are now much more flexible in specifying the comparisons to display. The condition argument accepts lists, functions, and shortcuts for common reference values, such as “minmax”, “threenum”, etc.\nvariables argument of the comparisons() function is more flexible:\n\nAccepts functions to specify custom differences in numeric variables (e.g., forward and backward differencing).\nCan specify pairs of factors to compare in the variables argument of the comparisons function.\n\nvariables argument of the predictions() function is more flexible:\n\nAccepts shortcut strings, functions, and vectors of arbitrary length.\n\nIntegrate out random effects in bayesian brms models (see Bayesian analysis vignette)\n\nNew vignettes:\n\nExperiments\nExtending marginal effects\nIntegrating out random effects in bayesian models\n\nBug fixes and minor improvements:\n\nThe default value of conf_level in summary() and tidy() is now NULL, which inherits the conf_level value in the original comparisons/marginaleffects/predictions calls.\nFix typo in function names for missing “lnratioavgwts”\nInteractions with fixest::i() are parsed properly as categorical variables\nFor betareg objects, inference can now be done on all coefficients using deltamethod(). previously only the location coefficients were available.\nFor objects from crch package, a number of bugs have been fixed; standard errors should now be correct for deltamethod(), marginaleffects(), etc.\nFixed a bug in the tidy() function for glmmTMB models without random effects, which caused all t statistics to be identical.\n\n\n\n\n\nNew supported model class: gamlss. Thanks to Marcio Augusto Diniz.\nmarginalmeans() accepts a wts argument with values: “equal”, “proportional”, “cells”.\nby argument\n\naccepts data frames for complex groupings.\nin marginalmeans only accepts data frames.\naccepts “group” to group by response level.\nworks with bayesian models.\n\nbyfun argument for the predictions() function to aggregate using different functions.\nhypothesis argument\n\nThe matrix column names are used as labels for hypothesis tests.\nBetter labels with “sequential”, “reference”, “pairwise”.\nnew shortcuts “revpairwise”, “revsequential”, “revreference”\n\nwts argument is respected in by argument and with *avg shortcuts in the transform_pre argument.\ntidy.predictions() and tidy.marginalmeans() get a new transform_avg argument.\nNew vignettes:\n\nUnit-level contrasts in logistic regressions. Thanks to @arthur-albuquerque.\nPython Numpy models in marginaleffects. Thanks to @timpipeseek.\nBootstrap example in standard errors vignette.\n\n\n\n\n\nBreaking changes:\n\nby is deprecated in summary() and tidy(). Use the same by argument in the main functions instead: comparisons(), marginaleffects(), predictions()\nCharacter vectors are no longer supported in the variables argument of the predictions() function. Use newdata=\"fivenum\" or “grid”, “mean”, or “median” instead.\n\nCritical bug fix:\n\nContrasts with interactions were incorrect in version 0.6.0. The error should have been obvious to most analysts in most cases (weird-looking alignment). Thanks to @vmikk.\n\nNew supported packages and models:\n\nsurvival::clogit\nbiglm: The main quantities can be computed, but not the delta method standard errors. See https://github.com/vincentarelbundock/marginaleffects/issues/387\n\nNew vignette:\n\nElasticity\nFrequently Asked Questions\n\nNew features:\n\nElasticity and semi-elasticity using the new slope argument in marginaleffects(): eyex, dyex, eydx\ndatagrid() accepts functions: datagrid(newdata = mtcars, hp = range, mpg = fivenum, wt = sd)\nNew datagridcf() function to create counterfactual datasets. This is a shortcut to the datagrid() function with default to grid_type = \"counterfactual\"\nNew by arguments in predictions(), comparisons(), marginaleffects()\nNew newdata shortcuts: “tukey”, “grid”\nNew string shortcuts for transform_pre in comparisons()\nmarginalmeans() now back transforms confidence intervals when possible.\nvcov argument string shortcuts are now case-insensitive\nThe default contrast in comparisons() for binary predictors is now a difference between 1 and 0, rather than +1 relative to baseline.\ndocumentation improvements\n\n\n\n\nNew supported packages and models:\n\ntidymodels objects of class tidy_model are supported if the fit engine is supported by marginaleffects.\n\nNew function:\n\ndeltamethod(): Hypothesis tests on functions of parameters\nplot_cco(): Plot conditional contrasts\n\nNew arguments:\n\nhypothesis for hypothesis tests and custom contrasts\ntransform_post in predictions()\nwts argument in predictions() only affects average predictions in tidy() or summary().\n\nNew or improved vignettes:\n\nHypothesis Tests and Custom Contrasts using the Delta Method: https://marginaleffects.com/articles/hypothesis.html\nMultiple Imputation: https://marginaleffects.com/articles/multiple_imputation.html\nCausal Inference with the g-Formula: https://marginaleffects.com/articles/gcomputation.html (Thanks to Rohan Kapre for the idea)\n\nDeprecated or renamed arguments:\n\ncontrast_factor and contrast_numeric arguments are deprecated in comparisons(). Use a named list in the variables argument instead. Backward compatibility is maintained.\nThe transform_post argument in tidy() and summary() is renamed to transform_avg to disambiguate against the argument of the same name in comparisons(). Backward compatibility is preserved.\n\nMisc:\n\ntidy.predictions() computes standard errors using the delta method for average predictions\nSupport gam models with matrix columns.\neps in marginaleffects() is now “adaptive” by default: it equals 0.0001 multiplied the range of the predictor variable\ncomparisons() now supports “log of marginal odds ratio” in the transform_pre argument. Thanks to Noah Greifer.\nNew transform_pre shortcuts: dydx, expdydx\ntidy.predictions() computes standard errors and confidence intervals for linear models or GLM on the link scale.\n\n\n\n\nBreaking changes:\n\ntype no longer accepts a character vector. Must be a single string.\nconf.int argument deprecated. Use vcov = FALSE instead.\n\nNew supported packages and models:\n\nmlogit\nmhurdle\ntobit1\nglmmTMB\n\nNew features:\n\ninteraction argument in comparisons() to compute interactions between contrasts (cross-contrasts).\nby argument in tidy() and summary() computes group-average marginal effects and comparisons.\ntransform_pre argument can define custom contrasts between adjusted predictions (e.g., log adjusted risk ratios). Available in comparisons().\ntransform_post argument allows back transformation before returning the final results. Available in comparisons(), marginalmeans(), summary(), tidy().\nThe variables argument of the comparisons() function accepts a named list to specify variable-specific contrast types.\nRobust standard errors with the vcov argument. This requires version 0.17.1 of the insight package.\n\nsandwich package shortcuts: vcov = \"HC3\", \"HC2\", \"NeweyWest\", and more.\nMixed effects models: vcov = \"satterthwaite\" or \"kenward-roger\"\nOne-sided formula to clusters: vcov = ~cluster_variable\nVariance-covariance matrix\nFunction which returns a named squared matrix\n\nmarginalmeans() allows interactions\nBayesian Model Averaging for brms models using type = \"average\". See vignette on the marginaleffects website.\neps argument for step size of numerical derivative\nmarginaleffects and comparisons now report confidence intervals by default.\nNew dependency on the data.table package yields substantial performance improvements.\nMore informative error messages and warnings\nBug fixes and performance improvements\n\nNew pages on the marginaleffects website: https://marginaleffects.com/\n\nAlternative software packages\nRobust standard errors (and more)\nPerformance tips\nTables and plots\nMultinomial Logit and Discrete Choice Models\nGeneralized Additive Models\nMixed effects models (Bayesian and Frequentist)\nTransformations and Custom Contrasts: Adjusted Risk Ratio Example\n\nArgument name changes (backward compatibility is preserved:\n\nEverywhere:\n\nconf.level -&gt; conf_level\n\ndatagrid():\n\nFUN.factor -&gt; FUN_factor (same for related arguments)\ngrid.type -&gt; grid_type\n\n\n\n\n\nNew supported packages and models:\n\nstats::loess\nsampleSelection::selection\nsampleSelection::heckit\n\nMisc:\n\nmgcv::bam models allow exclude argument.\nGam models allow include_smooth argument.\nNew tests\nBug fixes\n\n\n\n\nNew function:\n\ncomparisons() computes contrasts\n\nMisc:\n\nSpeed optimizations\npredictions() and plot_cap() include confidence intervals for linear models\nMore robust handling of in-formula functions: factor(), strata(), mo()\nDo not overwrite user’s ggplot2::theme_set() call\n\n\n\n\n\nBug fixes\n\n\n\n\nNew supported models:\n\nmclogit::mclogit\nrobust::lmRob\nrobustlmm::rlmer\nfixest confidence intervals in predictions\n\nMisc:\n\nSupport modelbased::visualisation_matrix in newdata without having to specify x explicitly.\ntidy.predictions() and summary.predictions() methods.\nDocumentation improvements.\nCRAN test fixes\n\n\n\n\nSupport for new models and packages:\n\nbrglm2::bracl\nmclogit::mblogit\nscam::scam\nlmerTest::lmer\n\nMisc:\n\nDrop numDeriv dependency, but make it available via a global option: options(“marginaleffects_numDeriv” = list(method = “Richardson”, method.args = list(eps = 1e-5, d = 0.0001)))\nBugfixes\nDocumentation improvements\nCRAN tests\n\n\n\n\ndocumentation bugfix\n\n\n\nBreaking changes:\n\npredictions returns predictions for every observation in the original dataset instead of newdata=datagrid().\nmarginalmeans objects have new column names, as do the corresponding tidy and summary outputs.\n\nNew supported packages and models:\n\nbrms::brm\nrstanarm::stanglm\nbrglm2::brmultinom\nMASS::glmmPQL\naod::betabin\n\nMisc:\n\ndatagrid function supersedes typical and counterfactual with the grid.type argument. The typical and counterfactual functions will remain available and exported, but their use is not encouraged.\nposterior_draws function can be applied to a predictions or a marginaleffects object to extract draws from the posterior distribution.\nmarginalmeans standard errors are now computed using the delta method.\npredictions standard errors are now computed using the delta method when they are not available from insight::get_predicted.\nNew vignette on Bayesian models with brms\nNew vignette on Mixed effects models with lme4\nIf the data.table package is installed, marginaleffects will automatically use it to speed things up.\nContrast definition reported in a separate column of marginaleffects output.\nSafer handling of the type argument.\nComprehensive list of supported and tests models on the website.\nMany bug fixes\nMany new tests, including several against emmeans\n\n\n\n\nBreaking change:\n\ndata argument becomes newdata in all functions.\n\nNew supported packages and models:\n\nlme4:glmer.nb\nmgcv::gam\nordinal::clm\nmgcv\n\nmarginalmeans:\n\nNew variables_grid argument\n\npredictions:\n\nSupport mgcv\n\nplot_cap\n\nNew type argument\n\nMisc:\n\nNew validity checks and tests\n\n\n\n\nFirst release. Bravo!\nThanks to Marco Avina Mendoza, Resul Umit, and all those who offered comments and suggestions."
  },
  {
    "objectID": "NEWS.html#dev",
    "href": "NEWS.html#dev",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\nThe comparisons() now uses “forward contrasts” by default for numeric predictors, instead of “centered contrasts”. This can lead to small numerical differences in non-linear models.\nThe variables argument of the comparisons() function no longer accepts numeric vectors unless they are of length 2, specifying the low and high contrast values. This is to avoid ambiguity between the two vector version. Users should supply a data frame or a function instead. This is nearly as easy, and removes ambiguity.\n\nNew supported packages:\n\ndbarts: https://cran.r-project.org/package=dbarts\nmvgam: https://nicholasjclark.github.io/mvgam/ Not available on CRAN yet, but this package maintains its own marginaleffects support function.\nrms::Gls: https://cran.r-project.org/package=rms\n\nMisc:\n\ncomparisons(): The variables argument now accepts functions and data frames for factor, character, and logical variables.\nDeprecation warning for: plot_cap(), plot_cme(), and plot_cco(). These function names will be removed in version 1.0.0.\noptions(modelsummary_factory_default=...) is respected in Quarto and Rmarkdown documents.\n\nBugs:\n\nwts argument now respected in avg_slopes() for binary variables. Thanks to @trose64 for report #961\nCustom functions in the comparison argument of comparisons() did not supply the correct x vector length for bayesian models when the by argument is used. Thanks to @Sandhu-SS for report #931.\nAdd support for two facet variables (through facet_grid) when plotting using condition\ncomparisons(): When variables is a vector of length two and newdata has exactly two columns, there was ambiguity between custom vectors and length two vector of contrasts. But reported by C. Rainey on Twitter."
  },
  {
    "objectID": "NEWS.html#section",
    "href": "NEWS.html#section",
    "title": "News",
    "section": "",
    "text": "Machine learning support:\n\ntidymodels package\nmlr3 package\n\nMisc:\n\nNew vignettes:\n\nInverse Probability Weighting\nMachine Learning\nMatching\n\nAdd support for hypotheses() to inferences(). Thanks to @Tristan-Siegfried for code contribution #908.\nSupport survival::survreg(). Thanks to Carlisle Rainey for Report #911.\ncolumn_names argument in print.marginaleffects() to suppress the printed column names at the bottom of the printout.\nThe function supplied to the comparison argument of the comparisons() function can now operate on x and on newdata directly (e.g., to check the number of observations).\nMore informative errors from predict().\n\nBugs:\n\nSome gamlss models generated an error related to the what argument. Thanks to @DHLocke for Issue #933"
  },
  {
    "objectID": "NEWS.html#section-1",
    "href": "NEWS.html#section-1",
    "title": "News",
    "section": "",
    "text": "hypotheses(): The FUN argument handles group columns gracefully.\nNative support for Amelia for multiple imputation.\n\nDocumentation:\n\nNew section on “Complex aggregations” in the Hypothesis testing vignette.\n\nBug fix:\n\nResults of the predictions() function could be inaccurate when (a) running version 0.15.0, (b) type is NULL or invlink(link), (c) model is glm(), and (d) the hypothesis argument is non-numeric. Thanks to @strengejacke for report #903"
  },
  {
    "objectID": "NEWS.html#section-2",
    "href": "NEWS.html#section-2",
    "title": "News",
    "section": "",
    "text": "New:\n\nConformal prediction via inferences()\nhypothesis argument now accepts multiple string formulas.\nThe type argument now accepts an explicit invlink(link) value instead of silently back-transforming. Users are no longer pointed to type_dictionary. Instead, they should call their function with a bad type value, and they will obtain a list of valid types. The default type value is printed in the output. This is useful because the default type value is NULL, so the user often does not explicitly decide.\nAllow install with Rcpp 1.0.0 and greater.\n\nSupport new models:\n\nsurvey::svyolr()\n\nMisc:\n\ninferences(method=\"simulation\") uses the original point estimate rather than the mean of the simulation distribution. Issue #851.\nBetter documentation and error messages for newdata=NULL\nSome performance improvements for predictions() and marginalmeans() (#880, #882, @etiennebacher).\n\nBug fix:\n\nnewdata=\"median\" returned mean of binary variables. Thanks to @jkhanson1970 for report #896."
  },
  {
    "objectID": "NEWS.html#section-3",
    "href": "NEWS.html#section-3",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\nRow order of the output changes for some objects. Rows are not sorted alphabetically by term, by, and variables explicitly supplied to datagrid. This can affect hypothesis tests computed using the b1, b2, b3, and other indices.\nNew procedure numderiv argument use a different procedure to select the step size used in the finite difference numeric derivative used to compute standard errors: abs(x) * sqrt(.Machine$double.eps). The numerical results may not be exactly identical to previous versions of marginaleffects, but the step size should be adequate in a broader variety of cases. Note that users can use the numderiv argument for more control on numeric differentiation, as documented.\nbife models are no longer supported pending investigation in weird results in the tests. Looking for volunteers write more thorough tests.\n\nNew:\n\nSupport: logistf package.\nSupport: DCchoice package.\nSupport: stats::nls\nhypotheses() can now accept raw data frame, which gives a lot of flexibility for custom contrasts and functions. See the Hypothesis vignette for an example.\nnumderiv argument allows users to use finite difference (center or forward) or Richardson’s method to compute the numerical derivatives used in the calculation of standard errors.\n\nBug fixes:\n\ninferences() supports the cross argument for comparisons() objects. Thanks to Kirill Solovev for report #856.\nsplines::bs() in formulas could produce incorrect results due to weirdness in stats::model.matrix(). Thanks to @chiungming for report #831.\nmgcv with ocat are now supported. Thanks to Lorenzo Fabbri for Issue #844.\nquantreg problem with rowid merge did not affect estimates but did not return the full original data. Issue #829.\nget_modeldata() extracts weights variable when available.\npredictions() is no longer broken in some inferences() calls. Issue #853\nInaccurate averaging with comparison=differenceavg some models where all predictors are categorical. Thanks to Karl Ove Hufthammer for report #865.\n\nMisc:\n\nMajor refactor to simplify the code base and make maintenance easier."
  },
  {
    "objectID": "NEWS.html#section-4",
    "href": "NEWS.html#section-4",
    "title": "News",
    "section": "",
    "text": "Breaking change:\n\nglmmTMB: Standard errors are no longer supported because they may have been erroneous. Follow Issue #810 on Github for developments: https://github.com/vincentarelbundock/marginaleffects/issues/810\n\nNew:\n\nhypothesis argument accepts wildcards: hypothesis = \"b*=b1\"\ns.value column in all output: Shannon transforms for p values. See Greenland (2019).\nmarginal_means supports mira (mice objects).\ncomparisons(): The variables arguments now accepts arbitrary numeric vectors of length equal to the number of rows in newdata. This allows users to specify fully custom treatment sizes. In the documentation examples, we show how to estimate the difference for a 1 standard deviation shift in a regressor, where the standard deviation is calculated on a group-wise basis.\ncomparisons(): the variables argument now accepts “revpairwise”, “revsequential”, “revreference” for factor and character variables.\ncomparisons(): the comparison argument now accept “lift” and “liftavg”.\n\nPerformance:\n\nComputing elasticities for linear models is now up to 30% faster (#787, @etiennebacher).\n\nBug fixes:\n\nBetter handling of environments when newdata is a function call. Thanks to @jcccf for report #814 and to @capnrefsmmat for the proposed fix using the rlang package.\nDegrees of freedom mismatch for joint hypothesis tests. Thanks to @snhansen for report #789."
  },
  {
    "objectID": "NEWS.html#section-5",
    "href": "NEWS.html#section-5",
    "title": "News",
    "section": "",
    "text": "Breaking change:\n\nRow order of output has changed for many calls, especially those using the by argument. This may break hypothesis tests conducted by indexing b1, b2, etc. This was necessary to fix Issue #776. Thanks to @marcora for the report.\n\nNew:\n\nhypotheses(): Joint hypothesis tests (F and Chi-square) with the joint and joint_test arguments.\nvcov.hypotheses method.\nwts is now available in plot_predictions(), plot_comparisons(), and plot_slopes().\n\nBug:\n\nWrong order of rows in bayesian models with by argument. Thanks to @shirdekel for report #782."
  },
  {
    "objectID": "NEWS.html#section-6",
    "href": "NEWS.html#section-6",
    "title": "News",
    "section": "",
    "text": "vcov() and coef() methods for marginaleffects objects.\nStrings in wts are accepted with the by argument.\npredictions() and avg_predictions() no longer use an automatic backtransformation for GLM models unless hypothesis is NULL.\nvcov() can be used to retrieve a full variance-covariance matrix from objects produced by comparisons(), slopes(), predictions(), or marginal_means() objects.\nWhen processing objects obtained using mice multiple imputation, the pooled model using mice::pool is attached to the model attribute of the output. This means that functions like modelsummary::modelsummary() will not erroneously report goodness-of-fit statistics from just a single model and will instead appropriately report the statistics for the pooled model. Thanks to @Tristan-Siegfried for PR #740.\nMore informative error messages on some prediction problems. Thanks to @andymilne for Report #751.\n\nPerformance:\n\ninferences() is now up to 17x faster and much more memory-efficient when method is \"boot\" or \"rsample\" (#770, #771, @etiennebacher).\n\nBugs:\n\nbrms models with nl=TRUE and a single predictor generated an error. Thanks to @Tristan-Siegried for Report #759.\navg_predictions(): Incorrect group-wise averaging when all predictors are categorical, the variables variable is used, and we are averaging with avg_ or the by argument. Thanks to BorgeJorge for report #766.\nBug when datagrid() when called inside a user-written function. Thanks to @NickCH-K for report #769 and to @capnrefsmmat for the diagnostics."
  },
  {
    "objectID": "NEWS.html#section-7",
    "href": "NEWS.html#section-7",
    "title": "News",
    "section": "",
    "text": "Breaking change:\n\nRow orders are now more consistent, but may have changed from previous version. This could affect results from hypothesis with b1, b2, … indexing.\n\nSupport new models:\n\nnlme::lme()\nphylolm::phylolm()\nphylolm::phyloglm()\n\nNew:\n\nVignette on 2x2 experimental designs. Thanks to Demetri Pananos.\ncomparisons() accepts data frames with two numeric columns (“low” and “high”) to specify fully customizable contrasts.\ndatagrid() gets a new by argument to create apply grid-making functions within groups.\nplot_*() gain a newdata argument for use with by.\n\nBug:\n\ncomparisons(comparison = \"lnratioavg\") ignored wts argument. Thanks to Demetri Pananos for report #737.\nordinal::clm(): incorrect standard errors when location and scale parameters are the same. Thanks to MrJerryTAO for report #718.\nIncorrect label for “2sd” comparisons. Thanks to Andy Milne for report #720.\nInvalid factor levels in datagrid() means newdata argument gets ignored. Thanks to Josh Errickson for report #721.\nError in models with only categorical predictors and the by argument. Thanks to Sam Brilleman for report #723.\nElasticities are now supported for ordinal::clm() models. Thanks to MrJerryTAO for report #729.\nglmmTMB models with zero-inflated components are supported. Thanks to @Helsinki-Ronan and @strengejacke for report #734."
  },
  {
    "objectID": "NEWS.html#section-8",
    "href": "NEWS.html#section-8",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\ntype column is replaced by type attribute.\npredictions() only works with officially supported model types (same list as comparisons() and slopes()).\n\nRenamed arguments (backward compatibility is preserved):\n\ntransform_pre -&gt; comparison\ntransform_post -&gt; transform\n\nNew:\n\np_adjust argument: Adjust p-values for multiple comparisons.\nequivalence argument available everywhere.\n\nPerformance:\n\nMuch faster results in avg_*() functions for models with only categorical predictors and many rows of data, using deduplication and weights instead of unit-level estimates.\nFaster predictions in lm() and glm() models using RcppEigen.\nBayesian models with many rows. Thanks to Etienne Bacher. #694\nFaster predictions, especially with standard errors and large datasets.\n\nBugs:\n\nMultiple imputation with mira objects was not pooling all datasets. Thanks to @Generalized for report #711.\nSupport for more models with offsets. Thanks to @mariofiorini for report #705.\nError on predictions() with by and wts. Thanks to Noah Greifer for report #695.\nafex: some models generated errors. Thanks to Daniel Lüdecke for report #696.\ngroup column name is always forbidden. Thanks to Daniel Lüdecke for report #697.\nBlank graphs in plot_comparisons() with a list in variables.\ntype=\"link\" produced an error with some categorical brms models. Thanks to @shirdekel for report #703.\nError on predictions(variables = ...) for glmmTMB models. Thanks to Daniel Lüdecke for report #707.\nby with user-specified function in comparison and factor predictor did not aggregate correctly. Thanks to @joaotedde for report #715.\nordinal::clm: Support cum.prob and linear.predictor prediction types. Thanks to @MrJerryTAO for report #717."
  },
  {
    "objectID": "NEWS.html#section-9",
    "href": "NEWS.html#section-9",
    "title": "News",
    "section": "",
    "text": "Performance:\n\n2-4x faster execution for many calls. Thanks to Etienne Bacher.\n\nNew models supported:\n\nMCMCglmm::MCMCglmm\nRchoice::hetprob\nRchoice::ivpml\nMultiple imputation using mice and any package which can return a list of imputed data frames (e.g., Amelia, missRanger, etc.)\n\nPlot improvements:\n\nNew by argument to display marginal estimates by subgroup.\nNew rug argument to display tick marks in the margins.\nNew points argument in plot_predictions() to display a scatter plot.\nNew gray argument to plot in grayscale using line types and shapes instead of color.\nThe effect argument is renamed to variables in plot_slopes() and plot_comparisons(). This improves consistency with the analogous slopes() and comparisons() functions.\nThe plotting vignette was re-written.\n\nOther:\n\nSupport multiple imputation with mice mira objects. The multiple imputation vignette was rewritten.\nThe variables_grid argument in marginal_means() is renamed newdata. Backward compatibility is maintained.\navg_*() returns an informative error when vcov is “satterthwaite” or “kenward-roger”\n“satterthwaite” and “kenward-roger” are now supported when newdata is not NULL\nInformative error when hypothesis includes a b# larger than the available number of estimates.\navg_predictions(model, variables = \"x\") computes average counterfactual predictions by subgroups of x\ndatagrid() and plot_*() functions are faster in datasets with many extraneous columns.\nIn predictions(type = NULL) with glm() and Gam() we first make predictions on the link scale and then backtransform them. Setting type=\"response\" explicitly makes predictions directly on the response scale without backtransformation.\nStandard errors now supported for more glmmTMB models.\nUse the numDeriv package for numeric differentiation in the calculation of delta method standard error. A global option can now be passed to numDeriv::jacobian:\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = list(method = \"Richardson\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = NULL)\n\nPrint:\n\nPrint fewer significant digits.\nprint.marginaleffects now prints all columns supplied to newdata\nLess redundant labels when using hypothesis\n\nMany improvements to documentation.\n\nBugfixes:\n\nStandard errors could be inaccurate in models with non-linear components (and interactions) when some of the coefficients were very small. This was related to the step size used for numerical differentiation for the delta method. Issue #684.\navg_predictions(by =) did not work when the dataset included a column named term. Issue #683.\nbrms models with multivariate outcome collapsed categories in comparisons(). Issue #639.\nhypotheses() now works on lists and in calls to lapply(), purrr::map(), etc. Issue #660."
  },
  {
    "objectID": "NEWS.html#section-10",
    "href": "NEWS.html#section-10",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\nAll functions return an estimate column instead of the function-specific predicted, comparisons, dydx, etc. This change only affects unit-level estimates, and not average estimates, which already used the estimate column name.\nThe transform_avg argument in tidy() deprecated. Use transform_post instead.\nplot_*(draw=FALSE) now return the actual variable names supplied to the condition argument, rather than the opaque “condition1”, “condition2”, etc.\n\nNew models supported:\n\nblme package.\n\nNew features:\n\nNew functions: avg_predictions(), avg_comparisons(), avg_slopes()\nEquivalence, non-inferiority, and non-superiority tests with the hypotheses() function and equivalence argument.\nNew experimental inferences() function: simulation-based inferences and bootstrap using the boot, rsample, and fwb package.\nNew df argument to set degrees of freedom manually for p and CI.\nPretty print() for all objects.\nby argument\n\nTRUE returns average (marginal) predictions, comparisons, or slopes.\nSupports bayesian models.\n\nhypothesis argument\n\nNumeric value sets the null used in calculating Z and p.\nExample: comparisons(mod, transform_pre = \"ratio\", hypothesis = 1)\n\nAll arguments from the main functions are now available through tidy(), and summary(): conf_level, transform_post, etc.\nBayesian posterior distribution summaries (median, mean, HDI, quantiles) can be customized using global options. See ?comparisons\n\nRenamed functions (backward-compatibility is maintained by keeping the old function names as aliases):\n\nmarginaleffects() -&gt; slopes()\nposteriordraws() -&gt; posterior_draws()\nmarginalmeans() -&gt; marginal_means()\nplot_cap() -&gt; plot_predictions()\nplot_cme() -&gt; plot_slopes()\nplot_cco() -&gt; plot_comparisons()\n\nBug fixes:\n\nIncorrect results: In 0.8.1, plot_*() the threenum and minmax labels did not correspond to the correct numeric values.\nFix corner case for slopes when the dataset includes infinite values.\nmlogit error with factors.\nThe vcov argument now accepts functions for most models.\n\nOther:\n\nRemoved major performance bottleneck for slopes()"
  },
  {
    "objectID": "NEWS.html#section-11",
    "href": "NEWS.html#section-11",
    "title": "News",
    "section": "",
    "text": "deltamethod() can run hypothesis tests on objects produced by the comparisons(), marginaleffects(), predictions(), and marginalmeans() functions. This feature relies on match.call(), which means it may not always work when used programmatically, inside functions and nested environments. It is generally safer and more efficient to use the hypothesis argument.\nplot_cme() and plot_cco() accept lists with user-specified values for the regressors, and can display nice labels for shortcut string-functions like “threenum” or “quartile”.\nposterior_draws: new shape argument to return MCMC draws in various formats, including the new rvar structure from the posterior package.\ntransform_avg function gets printed in summary() output.\ntransform_post and transform_avg support string shortcuts: “exp” and “ln”\nAdded support for mlm models from lm(). Thanks to Noah Greifer.\n\nBug fixes:\n\nhypothesis argument with bayesian models and tidy() used to raise an error.\nMissing values for some regressors in the comparisons() output for brms models."
  },
  {
    "objectID": "NEWS.html#section-12",
    "href": "NEWS.html#section-12",
    "title": "News",
    "section": "",
    "text": "Breaking change:\n\nThe interaction argument is deprecated and replaced by the cross argument. This is to reduce ambiguity with respect to the interaction argument in emmeans, which does something completely different, akin to the difference-in-differences illustrated in the Interactions vignette.\n\n71 classes of models supported, including the new:\n\nrms::ols\nrms::lrm\nrms::orm\n\nNew features:\n\nPlots: plot_cme(), plot_cap(), and plot_cco() are now much more flexible in specifying the comparisons to display. The condition argument accepts lists, functions, and shortcuts for common reference values, such as “minmax”, “threenum”, etc.\nvariables argument of the comparisons() function is more flexible:\n\nAccepts functions to specify custom differences in numeric variables (e.g., forward and backward differencing).\nCan specify pairs of factors to compare in the variables argument of the comparisons function.\n\nvariables argument of the predictions() function is more flexible:\n\nAccepts shortcut strings, functions, and vectors of arbitrary length.\n\nIntegrate out random effects in bayesian brms models (see Bayesian analysis vignette)\n\nNew vignettes:\n\nExperiments\nExtending marginal effects\nIntegrating out random effects in bayesian models\n\nBug fixes and minor improvements:\n\nThe default value of conf_level in summary() and tidy() is now NULL, which inherits the conf_level value in the original comparisons/marginaleffects/predictions calls.\nFix typo in function names for missing “lnratioavgwts”\nInteractions with fixest::i() are parsed properly as categorical variables\nFor betareg objects, inference can now be done on all coefficients using deltamethod(). previously only the location coefficients were available.\nFor objects from crch package, a number of bugs have been fixed; standard errors should now be correct for deltamethod(), marginaleffects(), etc.\nFixed a bug in the tidy() function for glmmTMB models without random effects, which caused all t statistics to be identical."
  },
  {
    "objectID": "NEWS.html#section-13",
    "href": "NEWS.html#section-13",
    "title": "News",
    "section": "",
    "text": "New supported model class: gamlss. Thanks to Marcio Augusto Diniz.\nmarginalmeans() accepts a wts argument with values: “equal”, “proportional”, “cells”.\nby argument\n\naccepts data frames for complex groupings.\nin marginalmeans only accepts data frames.\naccepts “group” to group by response level.\nworks with bayesian models.\n\nbyfun argument for the predictions() function to aggregate using different functions.\nhypothesis argument\n\nThe matrix column names are used as labels for hypothesis tests.\nBetter labels with “sequential”, “reference”, “pairwise”.\nnew shortcuts “revpairwise”, “revsequential”, “revreference”\n\nwts argument is respected in by argument and with *avg shortcuts in the transform_pre argument.\ntidy.predictions() and tidy.marginalmeans() get a new transform_avg argument.\nNew vignettes:\n\nUnit-level contrasts in logistic regressions. Thanks to @arthur-albuquerque.\nPython Numpy models in marginaleffects. Thanks to @timpipeseek.\nBootstrap example in standard errors vignette."
  },
  {
    "objectID": "NEWS.html#section-14",
    "href": "NEWS.html#section-14",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\nby is deprecated in summary() and tidy(). Use the same by argument in the main functions instead: comparisons(), marginaleffects(), predictions()\nCharacter vectors are no longer supported in the variables argument of the predictions() function. Use newdata=\"fivenum\" or “grid”, “mean”, or “median” instead.\n\nCritical bug fix:\n\nContrasts with interactions were incorrect in version 0.6.0. The error should have been obvious to most analysts in most cases (weird-looking alignment). Thanks to @vmikk.\n\nNew supported packages and models:\n\nsurvival::clogit\nbiglm: The main quantities can be computed, but not the delta method standard errors. See https://github.com/vincentarelbundock/marginaleffects/issues/387\n\nNew vignette:\n\nElasticity\nFrequently Asked Questions\n\nNew features:\n\nElasticity and semi-elasticity using the new slope argument in marginaleffects(): eyex, dyex, eydx\ndatagrid() accepts functions: datagrid(newdata = mtcars, hp = range, mpg = fivenum, wt = sd)\nNew datagridcf() function to create counterfactual datasets. This is a shortcut to the datagrid() function with default to grid_type = \"counterfactual\"\nNew by arguments in predictions(), comparisons(), marginaleffects()\nNew newdata shortcuts: “tukey”, “grid”\nNew string shortcuts for transform_pre in comparisons()\nmarginalmeans() now back transforms confidence intervals when possible.\nvcov argument string shortcuts are now case-insensitive\nThe default contrast in comparisons() for binary predictors is now a difference between 1 and 0, rather than +1 relative to baseline.\ndocumentation improvements"
  },
  {
    "objectID": "NEWS.html#section-15",
    "href": "NEWS.html#section-15",
    "title": "News",
    "section": "",
    "text": "New supported packages and models:\n\ntidymodels objects of class tidy_model are supported if the fit engine is supported by marginaleffects.\n\nNew function:\n\ndeltamethod(): Hypothesis tests on functions of parameters\nplot_cco(): Plot conditional contrasts\n\nNew arguments:\n\nhypothesis for hypothesis tests and custom contrasts\ntransform_post in predictions()\nwts argument in predictions() only affects average predictions in tidy() or summary().\n\nNew or improved vignettes:\n\nHypothesis Tests and Custom Contrasts using the Delta Method: https://marginaleffects.com/articles/hypothesis.html\nMultiple Imputation: https://marginaleffects.com/articles/multiple_imputation.html\nCausal Inference with the g-Formula: https://marginaleffects.com/articles/gcomputation.html (Thanks to Rohan Kapre for the idea)\n\nDeprecated or renamed arguments:\n\ncontrast_factor and contrast_numeric arguments are deprecated in comparisons(). Use a named list in the variables argument instead. Backward compatibility is maintained.\nThe transform_post argument in tidy() and summary() is renamed to transform_avg to disambiguate against the argument of the same name in comparisons(). Backward compatibility is preserved.\n\nMisc:\n\ntidy.predictions() computes standard errors using the delta method for average predictions\nSupport gam models with matrix columns.\neps in marginaleffects() is now “adaptive” by default: it equals 0.0001 multiplied the range of the predictor variable\ncomparisons() now supports “log of marginal odds ratio” in the transform_pre argument. Thanks to Noah Greifer.\nNew transform_pre shortcuts: dydx, expdydx\ntidy.predictions() computes standard errors and confidence intervals for linear models or GLM on the link scale."
  },
  {
    "objectID": "NEWS.html#section-16",
    "href": "NEWS.html#section-16",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\ntype no longer accepts a character vector. Must be a single string.\nconf.int argument deprecated. Use vcov = FALSE instead.\n\nNew supported packages and models:\n\nmlogit\nmhurdle\ntobit1\nglmmTMB\n\nNew features:\n\ninteraction argument in comparisons() to compute interactions between contrasts (cross-contrasts).\nby argument in tidy() and summary() computes group-average marginal effects and comparisons.\ntransform_pre argument can define custom contrasts between adjusted predictions (e.g., log adjusted risk ratios). Available in comparisons().\ntransform_post argument allows back transformation before returning the final results. Available in comparisons(), marginalmeans(), summary(), tidy().\nThe variables argument of the comparisons() function accepts a named list to specify variable-specific contrast types.\nRobust standard errors with the vcov argument. This requires version 0.17.1 of the insight package.\n\nsandwich package shortcuts: vcov = \"HC3\", \"HC2\", \"NeweyWest\", and more.\nMixed effects models: vcov = \"satterthwaite\" or \"kenward-roger\"\nOne-sided formula to clusters: vcov = ~cluster_variable\nVariance-covariance matrix\nFunction which returns a named squared matrix\n\nmarginalmeans() allows interactions\nBayesian Model Averaging for brms models using type = \"average\". See vignette on the marginaleffects website.\neps argument for step size of numerical derivative\nmarginaleffects and comparisons now report confidence intervals by default.\nNew dependency on the data.table package yields substantial performance improvements.\nMore informative error messages and warnings\nBug fixes and performance improvements\n\nNew pages on the marginaleffects website: https://marginaleffects.com/\n\nAlternative software packages\nRobust standard errors (and more)\nPerformance tips\nTables and plots\nMultinomial Logit and Discrete Choice Models\nGeneralized Additive Models\nMixed effects models (Bayesian and Frequentist)\nTransformations and Custom Contrasts: Adjusted Risk Ratio Example\n\nArgument name changes (backward compatibility is preserved:\n\nEverywhere:\n\nconf.level -&gt; conf_level\n\ndatagrid():\n\nFUN.factor -&gt; FUN_factor (same for related arguments)\ngrid.type -&gt; grid_type"
  },
  {
    "objectID": "NEWS.html#section-17",
    "href": "NEWS.html#section-17",
    "title": "News",
    "section": "",
    "text": "New supported packages and models:\n\nstats::loess\nsampleSelection::selection\nsampleSelection::heckit\n\nMisc:\n\nmgcv::bam models allow exclude argument.\nGam models allow include_smooth argument.\nNew tests\nBug fixes"
  },
  {
    "objectID": "NEWS.html#section-18",
    "href": "NEWS.html#section-18",
    "title": "News",
    "section": "",
    "text": "New function:\n\ncomparisons() computes contrasts\n\nMisc:\n\nSpeed optimizations\npredictions() and plot_cap() include confidence intervals for linear models\nMore robust handling of in-formula functions: factor(), strata(), mo()\nDo not overwrite user’s ggplot2::theme_set() call"
  },
  {
    "objectID": "NEWS.html#section-19",
    "href": "NEWS.html#section-19",
    "title": "News",
    "section": "",
    "text": "Bug fixes"
  },
  {
    "objectID": "NEWS.html#section-20",
    "href": "NEWS.html#section-20",
    "title": "News",
    "section": "",
    "text": "New supported models:\n\nmclogit::mclogit\nrobust::lmRob\nrobustlmm::rlmer\nfixest confidence intervals in predictions\n\nMisc:\n\nSupport modelbased::visualisation_matrix in newdata without having to specify x explicitly.\ntidy.predictions() and summary.predictions() methods.\nDocumentation improvements.\nCRAN test fixes"
  },
  {
    "objectID": "NEWS.html#section-21",
    "href": "NEWS.html#section-21",
    "title": "News",
    "section": "",
    "text": "Support for new models and packages:\n\nbrglm2::bracl\nmclogit::mblogit\nscam::scam\nlmerTest::lmer\n\nMisc:\n\nDrop numDeriv dependency, but make it available via a global option: options(“marginaleffects_numDeriv” = list(method = “Richardson”, method.args = list(eps = 1e-5, d = 0.0001)))\nBugfixes\nDocumentation improvements\nCRAN tests"
  },
  {
    "objectID": "NEWS.html#section-22",
    "href": "NEWS.html#section-22",
    "title": "News",
    "section": "",
    "text": "documentation bugfix"
  },
  {
    "objectID": "NEWS.html#section-23",
    "href": "NEWS.html#section-23",
    "title": "News",
    "section": "",
    "text": "Breaking changes:\n\npredictions returns predictions for every observation in the original dataset instead of newdata=datagrid().\nmarginalmeans objects have new column names, as do the corresponding tidy and summary outputs.\n\nNew supported packages and models:\n\nbrms::brm\nrstanarm::stanglm\nbrglm2::brmultinom\nMASS::glmmPQL\naod::betabin\n\nMisc:\n\ndatagrid function supersedes typical and counterfactual with the grid.type argument. The typical and counterfactual functions will remain available and exported, but their use is not encouraged.\nposterior_draws function can be applied to a predictions or a marginaleffects object to extract draws from the posterior distribution.\nmarginalmeans standard errors are now computed using the delta method.\npredictions standard errors are now computed using the delta method when they are not available from insight::get_predicted.\nNew vignette on Bayesian models with brms\nNew vignette on Mixed effects models with lme4\nIf the data.table package is installed, marginaleffects will automatically use it to speed things up.\nContrast definition reported in a separate column of marginaleffects output.\nSafer handling of the type argument.\nComprehensive list of supported and tests models on the website.\nMany bug fixes\nMany new tests, including several against emmeans"
  },
  {
    "objectID": "NEWS.html#section-24",
    "href": "NEWS.html#section-24",
    "title": "News",
    "section": "",
    "text": "Breaking change:\n\ndata argument becomes newdata in all functions.\n\nNew supported packages and models:\n\nlme4:glmer.nb\nmgcv::gam\nordinal::clm\nmgcv\n\nmarginalmeans:\n\nNew variables_grid argument\n\npredictions:\n\nSupport mgcv\n\nplot_cap\n\nNew type argument\n\nMisc:\n\nNew validity checks and tests"
  },
  {
    "objectID": "NEWS.html#section-25",
    "href": "NEWS.html#section-25",
    "title": "News",
    "section": "",
    "text": "First release. Bravo!\nThanks to Marco Avina Mendoza, Resul Umit, and all those who offered comments and suggestions."
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite package ‘marginaleffects’ in publications use:\n\n  Arel-Bundock V (2023). _marginaleffects: Predictions, Comparisons,\n  Slopes, Marginal Means, and Hypothesis Tests_. R package version\n  0.16.0.9016, &lt;https://marginaleffects.com/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests},\n    author = {Vincent Arel-Bundock},\n    year = {2023},\n    note = {R package version 0.16.0.9016},\n    url = {https://marginaleffects.com/},\n  }"
  },
  {
    "objectID": "man/posterior_draws.html",
    "href": "man/posterior_draws.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Extract Posterior Draws or Bootstrap Resamples from marginaleffects Objects\n\n\nExtract Posterior Draws or Bootstrap Resamples from marginaleffects Objects\n\n\n\nposterior_draws(x, shape = \"long\")\n\n\n\n\n\n\n\nx\n\n\nAn object produced by a marginaleffects package function, such as predictions(), avg_slopes(), hypotheses(), etc.\n\n\n\n\nshape\n\n\nstring indicating the shape of the output format:\n\n\n\"long\": long format data frame\n\n\n\"DxP\": Matrix with draws as rows and parameters as columns\n\n\n\"PxD\": Matrix with draws as rows and parameters as columns\n\n\n\"rvar\": Random variable datatype (see posterior package documentation).\n\n\n\n\n\n\n\n\nA data.frame with drawid and draw columns."
  },
  {
    "objectID": "man/posterior_draws.html#posterior_draws",
    "href": "man/posterior_draws.html#posterior_draws",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Extract Posterior Draws or Bootstrap Resamples from marginaleffects Objects\n\n\nExtract Posterior Draws or Bootstrap Resamples from marginaleffects Objects\n\n\n\nposterior_draws(x, shape = \"long\")\n\n\n\n\n\n\n\nx\n\n\nAn object produced by a marginaleffects package function, such as predictions(), avg_slopes(), hypotheses(), etc.\n\n\n\n\nshape\n\n\nstring indicating the shape of the output format:\n\n\n\"long\": long format data frame\n\n\n\"DxP\": Matrix with draws as rows and parameters as columns\n\n\n\"PxD\": Matrix with draws as rows and parameters as columns\n\n\n\"rvar\": Random variable datatype (see posterior package documentation).\n\n\n\n\n\n\n\n\nA data.frame with drawid and draw columns."
  },
  {
    "objectID": "man/plot_slopes.html",
    "href": "man/plot_slopes.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Slopes\n\n\nPlot slopes on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal slopes, that is, slopes made on the original data, but averaged by subgroups. This is analogous to using the by argument in the slopes() function.\nThe condition argument is used to plot conditional slopes, that is, slopes made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a slopes() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_slopes(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  slope = \"dydx\",\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose marginal effect (slope) we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the slopes() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\"dydx\": dY/dX\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\"eydx\": dY/dX * Y\n\n\n\"dyex\": dY/dX / X\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "man/plot_slopes.html#plot_slopes",
    "href": "man/plot_slopes.html#plot_slopes",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Slopes\n\n\nPlot slopes on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal slopes, that is, slopes made on the original data, but averaged by subgroups. This is analogous to using the by argument in the slopes() function.\nThe condition argument is used to plot conditional slopes, that is, slopes made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a slopes() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_slopes(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  slope = \"dydx\",\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose marginal effect (slope) we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the slopes() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\"dydx\": dY/dX\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\"eydx\": dY/dX * Y\n\n\n\"dyex\": dY/dX / X\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "man/plot_comparisons.html",
    "href": "man/plot_comparisons.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Comparisons\n\n\nPlot comparisons on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal comparisons, that is, comparisons made on the original data, but averaged by subgroups. This is analogous to using the by argument in the comparisons() function.\nThe condition argument is used to plot conditional comparisons, that is, comparisons made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a comparisons() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_comparisons(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  comparison = \"difference\",\n  transform = NULL,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose contrast we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the comparisons() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\nstring: shortcuts to common contrast functions.\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_comparisons(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "man/plot_comparisons.html#plot_comparisons",
    "href": "man/plot_comparisons.html#plot_comparisons",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Plot Conditional or Marginal Comparisons\n\n\nPlot comparisons on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\nThe by argument is used to plot marginal comparisons, that is, comparisons made on the original data, but averaged by subgroups. This is analogous to using the by argument in the comparisons() function.\nThe condition argument is used to plot conditional comparisons, that is, comparisons made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a comparisons() call.\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\nhttps://marginaleffects.com\n\n\n\n\n\nplot_comparisons(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  comparison = \"difference\",\n  transform = NULL,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose contrast we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\nCharacter vector (max length 4): Names of the predictors to display.\n\n\nNamed list (max length 4): List names correspond to predictors. List elements can be:\n\n\nNumeric vector\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the comparisons() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\nstring: shortcuts to common contrast functions.\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA ggplot2 object\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_comparisons(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "man/marginal_means.html",
    "href": "man/marginal_means.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Marginal Means\n\n\nMarginal means are adjusted predictions, averaged across a grid of categorical predictors, holding other numeric predictors at their means. To learn more, read the marginal means vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\nhttps://marginaleffects.com/articles/marginalmeans.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\nmarginal_means(\n  model,\n  variables = NULL,\n  newdata = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  transform = NULL,\n  cross = FALSE,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  wts = \"equal\",\n  by = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nCharacter vector of variable names: compute marginal means for each category of the listed variables.\n\n\nNULL: calculate marginal means for all logical, character, or factor variables in the dataset used to fit model. Hint: Set cross=TRUE to compute marginal means for combinations of focal variables.\n\n\n\n\n\n\nnewdata\n\n\nGrid of predictor values over which we marginalize.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL create a grid with all combinations of all categorical predictors in the model. Warning: can be expensive.\n\n\nCharacter vector: subset of categorical variables to use when building the balanced grid of predictors. Other variables are held to their mean or mode.\n\n\nData frame: A data frame which includes all the predictors in the original model. The full dataset is replicated once for every combination of the focal variables in the variables argument, using the datagridcf() function.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute marginal effects or contrasts. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\ncross\n\n\nTRUE or FALSE\n\n\nFALSE (default): Marginal means are computed for each predictor individually.\n\n\nTRUE: Marginal means are computed for each combination of predictors specified in the variables argument.\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nwts\n\n\ncharacter value. Weights to use in the averaging.\n\n\n\"equal\": each combination of variables in newdata gets equal weight.\n\n\n\"cells\": each combination of values for the variables in the newdata gets a weight proportional to its frequency in the original data.\n\n\n\"proportional\": each combination of values for the variables in newdata – except for those in the variables argument – gets a weight proportional to its frequency in the original data.\n\n\n\n\n\n\nby\n\n\nCollapse marginal means into categories. Data frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nThis function begins by calling the predictions function to obtain a grid of predictors, and adjusted predictions for each cell. The grid includes all combinations of the categorical variables listed in the variables and newdata arguments, or all combinations of the categorical variables used to fit the model if newdata is NULL. In the prediction grid, numeric variables are held at their means.\nAfter constructing the grid and filling the grid with adjusted predictions, marginal_means computes marginal means for the variables listed in the variables argument, by average across all categories in the grid.\nmarginal_means can only compute standard errors for linear models, or for predictions on the link scale, that is, with the type argument set to \"link\".\nThe marginaleffects website compares the output of this function to the popular emmeans package, which provides similar but more advanced functionality: https://marginaleffects.com/\n\n\n\nData frame of marginal means with one row per variable-value combination.\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# simple marginal means for each level of `cyl`\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ carb + cyl + am, dat)\n\nmarginal_means(\n  mod,\n  variables = \"cyl\")\n\n\n Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  cyl     4 23.1       1.66 13.9   &lt;0.001 144.3  19.9   26.4\n  cyl     6 20.4       1.34 15.2   &lt;0.001 171.9  17.8   23.0\n  cyl     8 16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapse levels of cyl by averaging\nby &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  by = c(\"4 & 6\", \"4 & 6\", \"8\"))\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by)\n\n\n    By Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 4 & 6 21.7       1.13 19.2   &lt;0.001 270.8  19.5   24.0\n 8     16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# pairwise differences between collapsed levels\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by,\n  hypothesis = \"pairwise\")\n\n\n      Term Mean Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n 4 & 6 - 8 5.54       1.51 3.66   &lt;0.001 12.0  2.57    8.5\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# cross\nmarginal_means(mod,\n  variables = c(\"cyl\", \"carb\"),\n  cross = TRUE)\n\n\n Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 25.8       1.26 20.43   &lt;0.001 305.7 23.34   28.3\n 25.6       1.17 21.93   &lt;0.001 351.8 23.30   27.9\n 25.3       2.37 10.71   &lt;0.001  86.6 20.70   30.0\n 21.9       1.90 11.51   &lt;0.001  99.4 18.15   25.6\n 20.3       3.77  5.39   &lt;0.001  23.7 12.91   27.7\n 19.8       3.81  5.18   &lt;0.001  22.1 12.29   27.2\n 23.1       1.77 13.08   &lt;0.001 127.4 19.63   26.5\n 22.9       1.87 12.24   &lt;0.001 112.0 19.20   26.5\n 22.6       2.37  9.56   &lt;0.001  69.5 17.98   27.2\n 19.1       1.34 14.31   &lt;0.001 151.8 16.53   21.8\n 17.6       3.00  5.85   &lt;0.001  27.6 11.68   23.5\n 17.0       3.48  4.89   &lt;0.001  19.9 10.21   23.9\n 18.9       1.94  9.74   &lt;0.001  72.1 15.11   22.7\n 18.7       1.57 11.90   &lt;0.001 106.0 15.61   21.8\n 18.4       1.83 10.07   &lt;0.001  76.8 14.85   22.0\n 15.0       1.20 12.53   &lt;0.001 117.2 12.63   17.3\n 13.4       3.36  3.99   &lt;0.001  13.9  6.81   20.0\n 12.9       3.00  4.28   &lt;0.001  15.7  6.98   18.8\n\nResults averaged over levels of: am \nColumns: cyl, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapsed cross\nby &lt;- expand.grid(\n  cyl = unique(mtcars$cyl),\n  carb = unique(mtcars$carb))\nby$by &lt;- ifelse(\n  by$cyl == 4,\n  paste(\"Control:\", by$carb),\n  paste(\"Treatment:\", by$carb))\n\n\n# Convert numeric variables to categorical before fitting the model\ndat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$carb &lt;- as.factor(dat$carb)\nmod &lt;- lm(mpg ~ hp + am + carb, data = dat)\n\n# Compute and summarize marginal means\nmarginal_means(mod)\n\n\n Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n am   FALSE 17.9      1.244 14.37   &lt;0.001 153.0  15.4   20.3\n am   TRUE  23.1      0.974 23.72   &lt;0.001 410.9  21.2   25.0\n carb 1     22.0      1.345 16.35   &lt;0.001 197.2  19.4   24.6\n carb 2     21.5      1.025 20.95   &lt;0.001 321.5  19.5   23.5\n carb 3     20.6      1.780 11.55   &lt;0.001 100.1  17.1   24.0\n carb 4     18.8      1.042 18.06   &lt;0.001 239.9  16.8   20.9\n carb 6     18.5      3.019  6.12   &lt;0.001  30.0  12.6   24.4\n carb 8     21.6      4.055  5.33   &lt;0.001  23.3  13.7   29.6\n\nResults averaged over levels of: hp, am, carb \nColumns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Contrast between marginal means (carb2 - carb1), or \"is the 1st marginal means equal to the 2nd?\"\n# see the vignette on \"Hypothesis Tests and Custom Contrasts\" on the `marginaleffects` website.\nlc &lt;- c(-1, 1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = \"b2 = b1\")\n\n\n  Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b2=b1 -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n   Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n custom -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Multiple custom contrasts\nlc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    -1, 1, 0, 0, 0, 0\n    ),\n  ncol = 2,\n  dimnames = list(NULL, c(\"A\", \"B\")))\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n Term   Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    A  1.199       6.15  0.195    0.845 0.2 -10.85  13.25\n    B -0.514       1.48 -0.348    0.728 0.5  -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response"
  },
  {
    "objectID": "man/marginal_means.html#marginal_means",
    "href": "man/marginal_means.html#marginal_means",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Marginal Means\n\n\nMarginal means are adjusted predictions, averaged across a grid of categorical predictors, holding other numeric predictors at their means. To learn more, read the marginal means vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\nhttps://marginaleffects.com/articles/marginalmeans.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\nmarginal_means(\n  model,\n  variables = NULL,\n  newdata = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  transform = NULL,\n  cross = FALSE,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  wts = \"equal\",\n  by = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nCharacter vector of variable names: compute marginal means for each category of the listed variables.\n\n\nNULL: calculate marginal means for all logical, character, or factor variables in the dataset used to fit model. Hint: Set cross=TRUE to compute marginal means for combinations of focal variables.\n\n\n\n\n\n\nnewdata\n\n\nGrid of predictor values over which we marginalize.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL create a grid with all combinations of all categorical predictors in the model. Warning: can be expensive.\n\n\nCharacter vector: subset of categorical variables to use when building the balanced grid of predictors. Other variables are held to their mean or mode.\n\n\nData frame: A data frame which includes all the predictors in the original model. The full dataset is replicated once for every combination of the focal variables in the variables argument, using the datagridcf() function.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute marginal effects or contrasts. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\ncross\n\n\nTRUE or FALSE\n\n\nFALSE (default): Marginal means are computed for each predictor individually.\n\n\nTRUE: Marginal means are computed for each combination of predictors specified in the variables argument.\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nwts\n\n\ncharacter value. Weights to use in the averaging.\n\n\n\"equal\": each combination of variables in newdata gets equal weight.\n\n\n\"cells\": each combination of values for the variables in the newdata gets a weight proportional to its frequency in the original data.\n\n\n\"proportional\": each combination of values for the variables in newdata – except for those in the variables argument – gets a weight proportional to its frequency in the original data.\n\n\n\n\n\n\nby\n\n\nCollapse marginal means into categories. Data frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nThis function begins by calling the predictions function to obtain a grid of predictors, and adjusted predictions for each cell. The grid includes all combinations of the categorical variables listed in the variables and newdata arguments, or all combinations of the categorical variables used to fit the model if newdata is NULL. In the prediction grid, numeric variables are held at their means.\nAfter constructing the grid and filling the grid with adjusted predictions, marginal_means computes marginal means for the variables listed in the variables argument, by average across all categories in the grid.\nmarginal_means can only compute standard errors for linear models, or for predictions on the link scale, that is, with the type argument set to \"link\".\nThe marginaleffects website compares the output of this function to the popular emmeans package, which provides similar but more advanced functionality: https://marginaleffects.com/\n\n\n\nData frame of marginal means with one row per variable-value combination.\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# simple marginal means for each level of `cyl`\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ carb + cyl + am, dat)\n\nmarginal_means(\n  mod,\n  variables = \"cyl\")\n\n\n Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  cyl     4 23.1       1.66 13.9   &lt;0.001 144.3  19.9   26.4\n  cyl     6 20.4       1.34 15.2   &lt;0.001 171.9  17.8   23.0\n  cyl     8 16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapse levels of cyl by averaging\nby &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  by = c(\"4 & 6\", \"4 & 6\", \"8\"))\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by)\n\n\n    By Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 4 & 6 21.7       1.13 19.2   &lt;0.001 270.8  19.5   24.0\n 8     16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# pairwise differences between collapsed levels\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by,\n  hypothesis = \"pairwise\")\n\n\n      Term Mean Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n 4 & 6 - 8 5.54       1.51 3.66   &lt;0.001 12.0  2.57    8.5\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# cross\nmarginal_means(mod,\n  variables = c(\"cyl\", \"carb\"),\n  cross = TRUE)\n\n\n Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 25.8       1.26 20.43   &lt;0.001 305.7 23.34   28.3\n 25.6       1.17 21.93   &lt;0.001 351.8 23.30   27.9\n 25.3       2.37 10.71   &lt;0.001  86.6 20.70   30.0\n 21.9       1.90 11.51   &lt;0.001  99.4 18.15   25.6\n 20.3       3.77  5.39   &lt;0.001  23.7 12.91   27.7\n 19.8       3.81  5.18   &lt;0.001  22.1 12.29   27.2\n 23.1       1.77 13.08   &lt;0.001 127.4 19.63   26.5\n 22.9       1.87 12.24   &lt;0.001 112.0 19.20   26.5\n 22.6       2.37  9.56   &lt;0.001  69.5 17.98   27.2\n 19.1       1.34 14.31   &lt;0.001 151.8 16.53   21.8\n 17.6       3.00  5.85   &lt;0.001  27.6 11.68   23.5\n 17.0       3.48  4.89   &lt;0.001  19.9 10.21   23.9\n 18.9       1.94  9.74   &lt;0.001  72.1 15.11   22.7\n 18.7       1.57 11.90   &lt;0.001 106.0 15.61   21.8\n 18.4       1.83 10.07   &lt;0.001  76.8 14.85   22.0\n 15.0       1.20 12.53   &lt;0.001 117.2 12.63   17.3\n 13.4       3.36  3.99   &lt;0.001  13.9  6.81   20.0\n 12.9       3.00  4.28   &lt;0.001  15.7  6.98   18.8\n\nResults averaged over levels of: am \nColumns: cyl, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapsed cross\nby &lt;- expand.grid(\n  cyl = unique(mtcars$cyl),\n  carb = unique(mtcars$carb))\nby$by &lt;- ifelse(\n  by$cyl == 4,\n  paste(\"Control:\", by$carb),\n  paste(\"Treatment:\", by$carb))\n\n\n# Convert numeric variables to categorical before fitting the model\ndat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$carb &lt;- as.factor(dat$carb)\nmod &lt;- lm(mpg ~ hp + am + carb, data = dat)\n\n# Compute and summarize marginal means\nmarginal_means(mod)\n\n\n Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n am   FALSE 17.9      1.244 14.37   &lt;0.001 153.0  15.4   20.3\n am   TRUE  23.1      0.974 23.72   &lt;0.001 410.9  21.2   25.0\n carb 1     22.0      1.345 16.35   &lt;0.001 197.2  19.4   24.6\n carb 2     21.5      1.025 20.95   &lt;0.001 321.5  19.5   23.5\n carb 3     20.6      1.780 11.55   &lt;0.001 100.1  17.1   24.0\n carb 4     18.8      1.042 18.06   &lt;0.001 239.9  16.8   20.9\n carb 6     18.5      3.019  6.12   &lt;0.001  30.0  12.6   24.4\n carb 8     21.6      4.055  5.33   &lt;0.001  23.3  13.7   29.6\n\nResults averaged over levels of: hp, am, carb \nColumns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Contrast between marginal means (carb2 - carb1), or \"is the 1st marginal means equal to the 2nd?\"\n# see the vignette on \"Hypothesis Tests and Custom Contrasts\" on the `marginaleffects` website.\nlc &lt;- c(-1, 1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = \"b2 = b1\")\n\n\n  Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b2=b1 -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n   Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n custom -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Multiple custom contrasts\nlc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    -1, 1, 0, 0, 0, 0\n    ),\n  ncol = 2,\n  dimnames = list(NULL, c(\"A\", \"B\")))\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n Term   Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    A  1.199       6.15  0.195    0.845 0.2 -10.85  13.25\n    B -0.514       1.48 -0.348    0.728 0.5  -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response"
  },
  {
    "objectID": "man/hypotheses.html",
    "href": "man/hypotheses.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "(Non-)Linear Tests for Null Hypotheses, Joint Hypotheses, Equivalence, Non Superiority, and Non Inferiority\n\n\nUncertainty estimates are calculated as first-order approximate standard errors for linear or non-linear functions of a vector of random variables with known or estimated covariance matrix. In that sense, hypotheses emulates the behavior of the excellent and well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models; requires fewer dependencies; expands the range of tests to equivalence and superiority/inferiority; and offers convenience features like robust standard errors.\nTo learn more, read the hypothesis tests vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\nhttps://marginaleffects.com/articles/hypothesis.html\n\n\nhttps://marginaleffects.com/\n\n\nWarning #1: Tests are conducted directly on the scale defined by the type argument. For some models, it can make sense to conduct hypothesis or equivalence tests on the “link” scale instead of the “response” scale which is often the default.\nWarning #2: For hypothesis tests on objects produced by the marginaleffects package, it is safer to use the hypothesis argument of the original function. Using hypotheses() may not work in certain environments, in lists, or when working programmatically with *apply style functions.\nWarning #3: The tests assume that the hypothesis expression is (approximately) normally distributed, which for non-linear functions of the parameters may not be realistic. More reliable confidence intervals can be obtained using the inferences() function with method = “boot”.\n\n\n\nhypotheses(\n  model,\n  hypothesis = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  df = Inf,\n  equivalence = NULL,\n  joint = FALSE,\n  joint_test = \"f\",\n  FUN = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object or object generated by the comparisons(), slopes(), predictions(), or marginal_means() functions.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\njoint\n\n\nJoint test of statistical significance. The null hypothesis value can be set using the hypothesis argument.\n\n\nFALSE: Hypotheses are not tested jointly.\n\n\nTRUE: All parameters are tested jointly.\n\n\nString: A regular expression to match parameters to be tested jointly. grep(joint, perl = TRUE)\n\n\nCharacter vector of parameter names to be tested. Characters refer to the names of the vector returned by coef(object).\n\n\nInteger vector of indices. Which parameters positions to test jointly.\n\n\n\n\n\n\njoint_test\n\n\nA character string specifying the type of test, either \"f\" or \"chisq\". The null hypothesis is set by the hypothesis argument, with default null equal to 0 for all parameters.\n\n\n\n\nFUN\n\n\nNULL or function.\n\n\nNULL (default): hypothesis test on a model’s coefficients, or on the quantities estimated by one of the marginaleffects package functions.\n\n\nFunction which accepts a model object and returns a numeric vector or a data.frame with two columns called term and estimate. This argument can be useful when users want to conduct a hypothesis test on an arbitrary function of quantities held in a model object. See examples below.\n\n\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nThe test statistic for the joint Wald test is calculated as (R * theta_hat - r)’ * inv(R * V_hat * R’) * (R * theta_hat - r) / Q, where theta_hat is the vector of estimated parameters, V_hat is the estimated covariance matrix, R is a Q x P matrix for testing Q hypotheses on P parameters, r is a Q x 1 vector for the null hypothesis, and Q is the number of rows in R. If the test is a Chi-squared test, the test statistic is not normalized.\nThe p-value is then calculated based on either the F-distribution (for F-test) or the Chi-squared distribution (for Chi-squared test). For the F-test, the degrees of freedom are Q and (n - P), where n is the sample size and P is the number of parameters. For the Chi-squared test, the degrees of freedom are Q.\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\n# When `FUN` and `hypotheses` are `NULL`, `hypotheses()` returns a data.frame of parameters\nhypotheses(mod)\n\n\n         Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Test of equality between coefficients\nhypotheses(mod, hypothesis = \"hp = wt\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Non-linear function\nhypotheses(mod, hypothesis = \"exp(hp + wt) = 0.1\")\n\n\n               Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Robust standard errors\nhypotheses(mod, hypothesis = \"hp = wt\", vcov = \"HC3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# b1, b2, ... shortcuts can be used to identify the position of the\n# parameters of interest in the output of FUN\nhypotheses(mod, hypothesis = \"b2 = b3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# wildcard\nhypotheses(mod, hypothesis = \"b* / b2 = 1\")\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n b1 / b2 = 1    -1551      764.0 -2.03   0.0423 4.6 -3048.9    -54\n b2 / b2 = 1        0         NA    NA       NA  NA      NA     NA\n b3 / b2 = 1      137       78.1  1.75   0.0804 3.6   -16.6    290\n b4 / b2 = 1      144      111.0  1.30   0.1938 2.4   -73.3    362\n b5 / b2 = 1      137      151.9  0.90   0.3679 1.4  -161.0    435\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# term names with special characters have to be enclosed in backticks\nhypotheses(mod, hypothesis = \"`factor(cyl)6` = `factor(cyl)8`\")\n\n\n                            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %\n `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41\n 97.5 %\n   3.07\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmod2 &lt;- lm(mpg ~ hp * drat, data = mtcars)\nhypotheses(mod2, hypothesis = \"`hp:drat` = drat\")\n\n\n             Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n `hp:drat` = drat    -6.08       2.89 -2.1   0.0357 4.8 -11.8 -0.405\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# predictions(), comparisons(), and slopes()\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\ncmp &lt;- comparisons(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 %  97.5 %\n b1=b2    -0.28      0.104 -2.7  0.00684 7.2 -0.483 -0.0771\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmfx &lt;- slopes(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b2 = 0.2\")\n\n\n   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n b2=0.2   0.0938      0.109 0.857    0.391 1.4 -0.121  0.308\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\npre &lt;- predictions(mod, newdata = datagrid(hp = 110, mpg = c(30, 35)))\nhypotheses(pre, hypothesis = \"b1 = b2\")\n\n\n  Term  Estimate Std. Error      z Pr(&gt;|z|)   S     2.5 %   97.5 %\n b1=b2 -3.57e-05   0.000172 -0.207    0.836 0.3 -0.000373 0.000302\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# The `FUN` argument can be used to compute standard errors for fitted values\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) predict(x, type = \"link\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n    1   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    2   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    3    0.233      0.781  0.299    0.765 0.4 -1.30  1.764\n    4   -0.595      0.647 -0.919    0.358 1.5 -1.86  0.674\n    5   -0.418      0.647 -0.645    0.519 0.9 -1.69  0.851\n    6   -5.026      2.195 -2.290    0.022 5.5 -9.33 -0.725\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nf &lt;- function(x) predict(x, type = \"response\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    1  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    2  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    3  0.55803     0.1926 2.898  0.00376 8.1  0.1806 0.9355\n    4  0.35560     0.1483 2.398  0.01648 5.9  0.0650 0.6462\n    5  0.39710     0.1550 2.562  0.01041 6.6  0.0933 0.7009\n    6  0.00652     0.0142 0.459  0.64653 0.6 -0.0213 0.0344\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Complex aggregation\n# Step 1: Collapse predicted probabilities by outcome level, for each individual\n# Step 2: Take the mean of the collapsed probabilities by group and `cyl`\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\n\naggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 & 4\", \"5\")) |&gt;\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n\n\n Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n 3 & 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n 3 & 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n 3 & 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n 5        6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n 5        4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n 5        8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n\nColumns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Equivalence, non-inferiority, and non-superiority tests\nmod &lt;- lm(mpg ~ hp + factor(gear), data = mtcars)\np &lt;- predictions(mod, newdata = \"median\")\nhypotheses(p, equivalence = c(17, 18))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf)\n     19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404\n p (Equiv)  hp gear\n     0.951 123    3\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\nmfx &lt;- avg_slopes(mod, variables = \"hp\")\nhypotheses(mfx, equivalence = c(-.1, .1))\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup)\n   hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001\n p (NonInf) p (Equiv)\n    0.00135   0.00135\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\ncmp &lt;- avg_comparisons(mod, variables = \"gear\", hypothesis = \"pairwise\")\nhypotheses(cmp, equivalence = c(0, 10))\n\n\n              Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n (4 - 3) - (5 - 3)    -3.94       2.05 -1.92   0.0543 4.2 -7.95 0.0727\n p (NonSup) p (NonInf) p (Equiv)\n     &lt;0.001      0.973     0.973\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\n# joint hypotheses: character vector\nmodel &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n\n\n\nJoint hypothesis test:\nas.factor(cyl)6:hp = 0\nas.factor(cyl)8:hp = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 2.11    0.142    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n as.factor(cyl)6:hp = 0\n as.factor(cyl)8:hp = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 5.7  0.00197    4   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.12  0.00665    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 1\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.84  0.00411    2   26\n\nColumns: statistic, p.value, df1, df2 \n\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 2\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 7.47  0.00273    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n cyl 6 - 4 = 0\n cyl 8 - 4 = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 1.6    0.221    2   26\n\nColumns: statistic, p.value, df1, df2"
  },
  {
    "objectID": "man/hypotheses.html#hypotheses",
    "href": "man/hypotheses.html#hypotheses",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "(Non-)Linear Tests for Null Hypotheses, Joint Hypotheses, Equivalence, Non Superiority, and Non Inferiority\n\n\nUncertainty estimates are calculated as first-order approximate standard errors for linear or non-linear functions of a vector of random variables with known or estimated covariance matrix. In that sense, hypotheses emulates the behavior of the excellent and well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models; requires fewer dependencies; expands the range of tests to equivalence and superiority/inferiority; and offers convenience features like robust standard errors.\nTo learn more, read the hypothesis tests vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\nhttps://marginaleffects.com/articles/hypothesis.html\n\n\nhttps://marginaleffects.com/\n\n\nWarning #1: Tests are conducted directly on the scale defined by the type argument. For some models, it can make sense to conduct hypothesis or equivalence tests on the “link” scale instead of the “response” scale which is often the default.\nWarning #2: For hypothesis tests on objects produced by the marginaleffects package, it is safer to use the hypothesis argument of the original function. Using hypotheses() may not work in certain environments, in lists, or when working programmatically with *apply style functions.\nWarning #3: The tests assume that the hypothesis expression is (approximately) normally distributed, which for non-linear functions of the parameters may not be realistic. More reliable confidence intervals can be obtained using the inferences() function with method = “boot”.\n\n\n\nhypotheses(\n  model,\n  hypothesis = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  df = Inf,\n  equivalence = NULL,\n  joint = FALSE,\n  joint_test = \"f\",\n  FUN = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object or object generated by the comparisons(), slopes(), predictions(), or marginal_means() functions.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\njoint\n\n\nJoint test of statistical significance. The null hypothesis value can be set using the hypothesis argument.\n\n\nFALSE: Hypotheses are not tested jointly.\n\n\nTRUE: All parameters are tested jointly.\n\n\nString: A regular expression to match parameters to be tested jointly. grep(joint, perl = TRUE)\n\n\nCharacter vector of parameter names to be tested. Characters refer to the names of the vector returned by coef(object).\n\n\nInteger vector of indices. Which parameters positions to test jointly.\n\n\n\n\n\n\njoint_test\n\n\nA character string specifying the type of test, either \"f\" or \"chisq\". The null hypothesis is set by the hypothesis argument, with default null equal to 0 for all parameters.\n\n\n\n\nFUN\n\n\nNULL or function.\n\n\nNULL (default): hypothesis test on a model’s coefficients, or on the quantities estimated by one of the marginaleffects package functions.\n\n\nFunction which accepts a model object and returns a numeric vector or a data.frame with two columns called term and estimate. This argument can be useful when users want to conduct a hypothesis test on an arbitrary function of quantities held in a model object. See examples below.\n\n\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nThe test statistic for the joint Wald test is calculated as (R * theta_hat - r)’ * inv(R * V_hat * R’) * (R * theta_hat - r) / Q, where theta_hat is the vector of estimated parameters, V_hat is the estimated covariance matrix, R is a Q x P matrix for testing Q hypotheses on P parameters, r is a Q x 1 vector for the null hypothesis, and Q is the number of rows in R. If the test is a Chi-squared test, the test statistic is not normalized.\nThe p-value is then calculated based on either the F-distribution (for F-test) or the Chi-squared distribution (for Chi-squared test). For the F-test, the degrees of freedom are Q and (n - P), where n is the sample size and P is the number of parameters. For the Chi-squared test, the degrees of freedom are Q.\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\n# When `FUN` and `hypotheses` are `NULL`, `hypotheses()` returns a data.frame of parameters\nhypotheses(mod)\n\n\n         Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Test of equality between coefficients\nhypotheses(mod, hypothesis = \"hp = wt\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Non-linear function\nhypotheses(mod, hypothesis = \"exp(hp + wt) = 0.1\")\n\n\n               Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Robust standard errors\nhypotheses(mod, hypothesis = \"hp = wt\", vcov = \"HC3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# b1, b2, ... shortcuts can be used to identify the position of the\n# parameters of interest in the output of FUN\nhypotheses(mod, hypothesis = \"b2 = b3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# wildcard\nhypotheses(mod, hypothesis = \"b* / b2 = 1\")\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n b1 / b2 = 1    -1551      764.0 -2.03   0.0423 4.6 -3048.9    -54\n b2 / b2 = 1        0         NA    NA       NA  NA      NA     NA\n b3 / b2 = 1      137       78.1  1.75   0.0804 3.6   -16.6    290\n b4 / b2 = 1      144      111.0  1.30   0.1938 2.4   -73.3    362\n b5 / b2 = 1      137      151.9  0.90   0.3679 1.4  -161.0    435\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# term names with special characters have to be enclosed in backticks\nhypotheses(mod, hypothesis = \"`factor(cyl)6` = `factor(cyl)8`\")\n\n\n                            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %\n `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41\n 97.5 %\n   3.07\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmod2 &lt;- lm(mpg ~ hp * drat, data = mtcars)\nhypotheses(mod2, hypothesis = \"`hp:drat` = drat\")\n\n\n             Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n `hp:drat` = drat    -6.08       2.89 -2.1   0.0357 4.8 -11.8 -0.405\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# predictions(), comparisons(), and slopes()\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\ncmp &lt;- comparisons(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 %  97.5 %\n b1=b2    -0.28      0.104 -2.7  0.00684 7.2 -0.483 -0.0771\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmfx &lt;- slopes(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b2 = 0.2\")\n\n\n   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n b2=0.2   0.0938      0.109 0.857    0.391 1.4 -0.121  0.308\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\npre &lt;- predictions(mod, newdata = datagrid(hp = 110, mpg = c(30, 35)))\nhypotheses(pre, hypothesis = \"b1 = b2\")\n\n\n  Term  Estimate Std. Error      z Pr(&gt;|z|)   S     2.5 %   97.5 %\n b1=b2 -3.57e-05   0.000172 -0.207    0.836 0.3 -0.000373 0.000302\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# The `FUN` argument can be used to compute standard errors for fitted values\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) predict(x, type = \"link\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n    1   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    2   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    3    0.233      0.781  0.299    0.765 0.4 -1.30  1.764\n    4   -0.595      0.647 -0.919    0.358 1.5 -1.86  0.674\n    5   -0.418      0.647 -0.645    0.519 0.9 -1.69  0.851\n    6   -5.026      2.195 -2.290    0.022 5.5 -9.33 -0.725\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nf &lt;- function(x) predict(x, type = \"response\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    1  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    2  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    3  0.55803     0.1926 2.898  0.00376 8.1  0.1806 0.9355\n    4  0.35560     0.1483 2.398  0.01648 5.9  0.0650 0.6462\n    5  0.39710     0.1550 2.562  0.01041 6.6  0.0933 0.7009\n    6  0.00652     0.0142 0.459  0.64653 0.6 -0.0213 0.0344\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Complex aggregation\n# Step 1: Collapse predicted probabilities by outcome level, for each individual\n# Step 2: Take the mean of the collapsed probabilities by group and `cyl`\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\n\naggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 & 4\", \"5\")) |&gt;\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n\n\n Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n 3 & 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n 3 & 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n 3 & 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n 5        6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n 5        4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n 5        8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n\nColumns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Equivalence, non-inferiority, and non-superiority tests\nmod &lt;- lm(mpg ~ hp + factor(gear), data = mtcars)\np &lt;- predictions(mod, newdata = \"median\")\nhypotheses(p, equivalence = c(17, 18))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf)\n     19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404\n p (Equiv)  hp gear\n     0.951 123    3\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\nmfx &lt;- avg_slopes(mod, variables = \"hp\")\nhypotheses(mfx, equivalence = c(-.1, .1))\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup)\n   hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001\n p (NonInf) p (Equiv)\n    0.00135   0.00135\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\ncmp &lt;- avg_comparisons(mod, variables = \"gear\", hypothesis = \"pairwise\")\nhypotheses(cmp, equivalence = c(0, 10))\n\n\n              Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n (4 - 3) - (5 - 3)    -3.94       2.05 -1.92   0.0543 4.2 -7.95 0.0727\n p (NonSup) p (NonInf) p (Equiv)\n     &lt;0.001      0.973     0.973\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\n# joint hypotheses: character vector\nmodel &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n\n\n\nJoint hypothesis test:\nas.factor(cyl)6:hp = 0\nas.factor(cyl)8:hp = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 2.11    0.142    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n as.factor(cyl)6:hp = 0\n as.factor(cyl)8:hp = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 5.7  0.00197    4   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.12  0.00665    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 1\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.84  0.00411    2   26\n\nColumns: statistic, p.value, df1, df2 \n\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 2\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 7.47  0.00273    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n cyl 6 - 4 = 0\n cyl 8 - 4 = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 1.6    0.221    2   26\n\nColumns: statistic, p.value, df1, df2"
  },
  {
    "objectID": "man/comparisons.html",
    "href": "man/comparisons.html",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Comparisons Between Predictions Made With Different Regressor Values\n\n\nPredict the outcome variable at different regressor values (e.g., college graduates vs. others), and compare those predictions by computing a difference, ratio, or some other function. comparisons() can return many quantities of interest, such as contrasts, differences, risk ratios, changes in log odds, lift, slopes, elasticities, etc.\n\n\ncomparisons(): unit-level (conditional) estimates.\n\n\navg_comparisons(): average (marginal) estimates.\n\n\nvariables identifies the focal regressors whose \"effect\" we are interested in. comparison determines how predictions with different regressor values are compared (difference, ratio, odds, etc.). The newdata argument and the datagrid() function control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the comparisons vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/comparisons.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\ncomparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  comparison = \"difference\",\n  type = NULL,\n  vcov = TRUE,\n  by = FALSE,\n  conf_level = 0.95,\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  vcov = TRUE,\n  by = TRUE,\n  conf_level = 0.95,\n  comparison = \"difference\",\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the comparisons.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level contrasts for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\ndata frame: Unit-level contrasts for each row of the newdata data frame.\n\n\nstring:\n\n\n\"mean\": Contrasts at the Mean. Contrasts when each predictor is held at its mean or mode.\n\n\n\"median\": Contrasts at the Median. Contrasts when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Contrasts at Marginal Means.\n\n\n\"tukey\": Contrasts at Tukey’s 5 numbers.\n\n\n\"grid\": Contrasts on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nnewdata = datagrid(mpg = fivenum): mpg variable held at Tukey’s five numbers (using the fivenum function), and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nNULL: compute comparisons for all the variables in the model object (can be slow).\n\n\nCharacter vector: subset of variables (usually faster).\n\n\nNamed list: names identify the subset of variables of interest, and values define the type of contrast to compute. Acceptable values depend on the variable type:\n\n\nFactor or character variables:\n\n\n\"reference\": Each factor level is compared to the factor reference (base) level\n\n\n\"all\": All combinations of observed levels\n\n\n\"sequential\": Each factor level is compared to the previous factor level\n\n\n\"pairwise\": Each factor level is compared to all other levels\n\n\n\"minmax\": The highest and lowest levels of a factor.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses.\n\n\nVector of length 2 with the two values to compare.\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\n\nLogical variables:\n\n\nNULL: contrast between TRUE and FALSE\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\n\nNumeric variables:\n\n\nNumeric of length 1: Forward contrast for a gap of x, computed between the observed value and the observed value plus x. Users can set a global option to get a \"center\" or \"backward\" contrast instead: options(marginaleffects_contrast_direction=“center”)\n\n\nNumeric vector of length 2: Contrast between the largest and the smallest elements of the x vector.\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\nExamples:\n\n\nvariables = list(gear = “pairwise”, hp = 10)\n\n\nvariables = list(gear = “sequential”, hp = c(100, 120))\n\n\nvariables = list(hp = \\(x) data.frame(low = x - 5, high = x + 10))\n\n\nSee the Examples section below for more.\n\n\n\n\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\nstring: shortcuts to common contrast functions.\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\ncross\n\n\n\n\nFALSE: Contrasts represent the change in adjusted predictions when one predictor changes and all other variables are held constant.\n\n\nTRUE: Contrasts represent the changes in adjusted predictions when all the predictors specified in the variables argument are manipulated simultaneously (a \"cross-contrast\").\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nterm: the variable whose marginal effect is computed\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\nstd.error: standard errors computed by via the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_comparisons(): Average comparisons\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nThe following transformations can be applied by supplying one of the shortcut strings to the comparison argument. hi is a vector of adjusted predictions for the \"high\" side of the contrast. lo is a vector of adjusted predictions for the \"low\" side of the contrast. y is a vector of adjusted predictions for the original data. x is the predictor in the original data. eps is the step size to use to compute derivatives and elasticities.\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\ndifference\n\n\n(hi, lo) hi - lo\n\n\n\n\ndifferenceavg\n\n\n(hi, lo) mean(hi - lo)\n\n\n\n\ndydx\n\n\n(hi, lo, eps) (hi - lo)/eps\n\n\n\n\neyex\n\n\n(hi, lo, eps, y, x) (hi - lo)/eps * (x/y)\n\n\n\n\neydx\n\n\n(hi, lo, eps, y, x) ((hi - lo)/eps)/y\n\n\n\n\ndyex\n\n\n(hi, lo, eps, x) ((hi - lo)/eps) * x\n\n\n\n\ndydxavg\n\n\n(hi, lo, eps) mean((hi - lo)/eps)\n\n\n\n\neyexavg\n\n\n(hi, lo, eps, y, x) mean((hi - lo)/eps * (x/y))\n\n\n\n\neydxavg\n\n\n(hi, lo, eps, y, x) mean(((hi - lo)/eps)/y)\n\n\n\n\ndyexavg\n\n\n(hi, lo, eps, x) mean(((hi - lo)/eps) * x)\n\n\n\n\nratio\n\n\n(hi, lo) hi/lo\n\n\n\n\nratioavg\n\n\n(hi, lo) mean(hi)/mean(lo)\n\n\n\n\nlnratio\n\n\n(hi, lo) log(hi/lo)\n\n\n\n\nlnratioavg\n\n\n(hi, lo) log(mean(hi)/mean(lo))\n\n\n\n\nlnor\n\n\n(hi, lo) log((hi/(1 - hi))/(lo/(1 - lo)))\n\n\n\n\nlnoravg\n\n\n(hi, lo) log((mean(hi)/(1 - mean(hi)))/(mean(lo)/(1 - mean(lo))))\n\n\n\n\nlift\n\n\n(hi, lo) (hi - lo)/lo\n\n\n\n\nliftavg\n\n\n(hi, lo) (mean(hi - lo))/mean(lo)\n\n\n\n\nexpdydx\n\n\n(hi, lo, eps) ((exp(hi) - exp(lo))/exp(eps))/eps\n\n\n\n\nexpdydxavg\n\n\n(hi, lo, eps) mean(((exp(hi) - exp(lo))/exp(eps))/eps)\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# Linear model\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\navg_comparisons(mod, variables = list(cyl = \"reference\"))\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n\n# GLM with different scale types\nmod &lt;- glm(am ~ factor(gear), data = mtcars)\navg_comparisons(mod, type = \"response\")\navg_comparisons(mod, type = \"link\")\n\n# Contrasts at the mean\ncomparisons(mod, newdata = \"mean\")\n\n# Contrasts between marginal means\ncomparisons(mod, newdata = \"marginalmeans\")\n\n# Contrasts at user-specified values\ncomparisons(mod, newdata = datagrid(am = 0, gear = tmp$gear))\ncomparisons(mod, newdata = datagrid(am = unique, gear = max))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\ncomparisons(m, variables = \"hp\", newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Numeric contrasts\nmod &lt;- lm(mpg ~ hp, data = mtcars)\navg_comparisons(mod, variables = list(hp = 1))\navg_comparisons(mod, variables = list(hp = 5))\navg_comparisons(mod, variables = list(hp = c(90, 100)))\navg_comparisons(mod, variables = list(hp = \"iqr\"))\navg_comparisons(mod, variables = list(hp = \"sd\"))\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n\n# using a function to specify a custom difference in one regressor\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (dat$hp - min(dat$hp)) / (max(dat$hp) - min(dat$hp)) + 1\nmodlog &lt;- lm(mpg ~ log(new_hp) + factor(cyl), data = dat)\nfdiff &lt;- \\(x) data.frame(x, x + 10)\navg_comparisons(modlog, variables = list(new_hp = fdiff))\n\n# Adjusted Risk Ratio: see the contrasts vignette\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\navg_comparisons(mod, comparison = \"lnratioavg\", transform = exp)\n\n# Adjusted Risk Ratio: Manual specification of the `comparison`\navg_comparisons(\n     mod,\n     comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n     transform = exp)\n# cross contrasts\nmod &lt;- lm(mpg ~ factor(cyl) * factor(gear) + hp, data = mtcars)\navg_comparisons(mod, variables = c(\"cyl\", \"gear\"), cross = TRUE)\n\n# variable-specific contrasts\navg_comparisons(mod, variables = list(gear = \"sequential\", hp = 10))\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n# Effect of a 1 group-wise standard deviation change\n# First we calculate the SD in each group of `cyl`\n# Second, we use that SD as the treatment size in the `variables` argument\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\ntmp &lt;- mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    mutate(hp_sd = sd(hp))\navg_comparisons(mod, variables = list(hp = tmp$hp_sd), by = \"cyl\")\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\ncomparisons(mod, by = TRUE)\n\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\navg_comparisons(mod, variables = \"hp\", by = c(\"vs\", \"am\"))\n\nlibrary(nnet)\nmod &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\ncomparisons(mod, type = \"probs\", by = by)"
  },
  {
    "objectID": "man/comparisons.html#comparisons",
    "href": "man/comparisons.html#comparisons",
    "title": "The Marginal Effects Zoo",
    "section": "",
    "text": "Comparisons Between Predictions Made With Different Regressor Values\n\n\nPredict the outcome variable at different regressor values (e.g., college graduates vs. others), and compare those predictions by computing a difference, ratio, or some other function. comparisons() can return many quantities of interest, such as contrasts, differences, risk ratios, changes in log odds, lift, slopes, elasticities, etc.\n\n\ncomparisons(): unit-level (conditional) estimates.\n\n\navg_comparisons(): average (marginal) estimates.\n\n\nvariables identifies the focal regressors whose \"effect\" we are interested in. comparison determines how predictions with different regressor values are compared (difference, ratio, odds, etc.). The newdata argument and the datagrid() function control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\nSee the comparisons vignette and package website for worked examples and case studies:\n\n\nhttps://marginaleffects.com/articles/comparisons.html\n\n\nhttps://marginaleffects.com/\n\n\n\n\n\ncomparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  comparison = \"difference\",\n  type = NULL,\n  vcov = TRUE,\n  by = FALSE,\n  conf_level = 0.95,\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  vcov = TRUE,\n  by = TRUE,\n  conf_level = 0.95,\n  comparison = \"difference\",\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the comparisons.\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\nNULL (default): Unit-level contrasts for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\ndata frame: Unit-level contrasts for each row of the newdata data frame.\n\n\nstring:\n\n\n\"mean\": Contrasts at the Mean. Contrasts when each predictor is held at its mean or mode.\n\n\n\"median\": Contrasts at the Median. Contrasts when each predictor is held at its median or mode.\n\n\n\"marginalmeans\": Contrasts at Marginal Means.\n\n\n\"tukey\": Contrasts at Tukey’s 5 numbers.\n\n\n\"grid\": Contrasts on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\nnewdata = datagrid(mpg = fivenum): mpg variable held at Tukey’s five numbers (using the fivenum function), and other regressors fixed at their means or modes.\n\n\nSee the Examples section and the datagrid documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\nNULL: compute comparisons for all the variables in the model object (can be slow).\n\n\nCharacter vector: subset of variables (usually faster).\n\n\nNamed list: names identify the subset of variables of interest, and values define the type of contrast to compute. Acceptable values depend on the variable type:\n\n\nFactor or character variables:\n\n\n\"reference\": Each factor level is compared to the factor reference (base) level\n\n\n\"all\": All combinations of observed levels\n\n\n\"sequential\": Each factor level is compared to the previous factor level\n\n\n\"pairwise\": Each factor level is compared to all other levels\n\n\n\"minmax\": The highest and lowest levels of a factor.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses.\n\n\nVector of length 2 with the two values to compare.\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\n\nLogical variables:\n\n\nNULL: contrast between TRUE and FALSE\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\n\nNumeric variables:\n\n\nNumeric of length 1: Forward contrast for a gap of x, computed between the observed value and the observed value plus x. Users can set a global option to get a \"center\" or \"backward\" contrast instead: options(marginaleffects_contrast_direction=“center”)\n\n\nNumeric vector of length 2: Contrast between the largest and the smallest elements of the x vector.\n\n\nData frame with the same number of rows as newdata, with two columns of \"lo\" and \"hi\" values to compare.\n\n\nFunction that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\nExamples:\n\n\nvariables = list(gear = “pairwise”, hp = 10)\n\n\nvariables = list(gear = “sequential”, hp = c(100, 120))\n\n\nvariables = list(hp = \\(x) data.frame(low = x - 5, high = x + 10))\n\n\nSee the Examples section below for more.\n\n\n\n\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\nstring: shortcuts to common contrast functions.\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\nSquare covariance matrix\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\nFALSE: return the original unit-level estimates.\n\n\nTRUE: aggregate estimates for each term.\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\nSee examples below.\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function’s documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\ncross\n\n\n\n\nFALSE: Contrasts represent the change in adjusted predictions when one predictor changes and all other variables are held constant.\n\n\nTRUE: Contrasts represent the changes in adjusted predictions when all the predictors specified in the variables argument are manipulated simultaneously (a \"cross-contrast\").\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in avg_*() or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\nNumeric:\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The b* wildcard can be used to test hypotheses on all estimates. Examples:\n\n\nhp = drat\n\n\nhp + drat = 12\n\n\nb1 + b2 + b3 = 0\n\n\nb* / b1 = 1\n\n\n\n\nString:\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\nnumderiv\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\"richardson\": Richardson extrapolation method\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(“fdcenter”, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n…\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.\n\n\n\n\n\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\nrowid: row number of the newdata data frame\n\n\ntype: prediction type, as defined by the type argument\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\nterm: the variable whose marginal effect is computed\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\nstd.error: standard errors computed by via the delta method.\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument.\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst’s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al. (2020).\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\nSee ?print.marginaleffects for printing options.\n\n\n\n\n\navg_comparisons(): Average comparisons\n\n\n\n\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\nhttps://marginaleffects.com/articles/uncertainty.html\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\nhttps://marginaleffects.com/articles/bootstrap.html\n\n\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws\n\n\n\n\n\n\n\n\n\n\n\n\nThe following transformations can be applied by supplying one of the shortcut strings to the comparison argument. hi is a vector of adjusted predictions for the \"high\" side of the contrast. lo is a vector of adjusted predictions for the \"low\" side of the contrast. y is a vector of adjusted predictions for the original data. x is the predictor in the original data. eps is the step size to use to compute derivatives and elasticities.\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\ndifference\n\n\n(hi, lo) hi - lo\n\n\n\n\ndifferenceavg\n\n\n(hi, lo) mean(hi - lo)\n\n\n\n\ndydx\n\n\n(hi, lo, eps) (hi - lo)/eps\n\n\n\n\neyex\n\n\n(hi, lo, eps, y, x) (hi - lo)/eps * (x/y)\n\n\n\n\neydx\n\n\n(hi, lo, eps, y, x) ((hi - lo)/eps)/y\n\n\n\n\ndyex\n\n\n(hi, lo, eps, x) ((hi - lo)/eps) * x\n\n\n\n\ndydxavg\n\n\n(hi, lo, eps) mean((hi - lo)/eps)\n\n\n\n\neyexavg\n\n\n(hi, lo, eps, y, x) mean((hi - lo)/eps * (x/y))\n\n\n\n\neydxavg\n\n\n(hi, lo, eps, y, x) mean(((hi - lo)/eps)/y)\n\n\n\n\ndyexavg\n\n\n(hi, lo, eps, x) mean(((hi - lo)/eps) * x)\n\n\n\n\nratio\n\n\n(hi, lo) hi/lo\n\n\n\n\nratioavg\n\n\n(hi, lo) mean(hi)/mean(lo)\n\n\n\n\nlnratio\n\n\n(hi, lo) log(hi/lo)\n\n\n\n\nlnratioavg\n\n\n(hi, lo) log(mean(hi)/mean(lo))\n\n\n\n\nlnor\n\n\n(hi, lo) log((hi/(1 - hi))/(lo/(1 - lo)))\n\n\n\n\nlnoravg\n\n\n(hi, lo) log((mean(hi)/(1 - mean(hi)))/(mean(lo)/(1 - mean(lo))))\n\n\n\n\nlift\n\n\n(hi, lo) (hi - lo)/lo\n\n\n\n\nliftavg\n\n\n(hi, lo) (mean(hi - lo))/mean(lo)\n\n\n\n\nexpdydx\n\n\n(hi, lo, eps) ((exp(hi) - exp(lo))/exp(eps))/eps\n\n\n\n\nexpdydxavg\n\n\n(hi, lo, eps) mean(((exp(hi) - exp(lo))/exp(eps))/eps)\n\n\n\n\n\n\n\n\n\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\noptions(“marginaleffects_posterior_interval” = “eti”)\noptions(“marginaleffects_posterior_interval” = “hdi”)\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\noptions(“marginaleffects_posterior_center” = “mean”)\noptions(“marginaleffects_posterior_center” = “median”)\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default).\n\n\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\nNon-inferiority:\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\np: Upper-tail probability\n\n\nNon-superiority:\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\np: Lower-tail probability\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.\n\n\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=“invlink(link)” will not always be equivalent to the average of estimates with type=“response”.\nSome of the most common type values are:\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob\n\n\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106–114.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191–93. https://doi.org/10.1093/aje/kwaa136\n\n\n\n\n\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# Linear model\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\navg_comparisons(mod, variables = list(cyl = \"reference\"))\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n\n# GLM with different scale types\nmod &lt;- glm(am ~ factor(gear), data = mtcars)\navg_comparisons(mod, type = \"response\")\navg_comparisons(mod, type = \"link\")\n\n# Contrasts at the mean\ncomparisons(mod, newdata = \"mean\")\n\n# Contrasts between marginal means\ncomparisons(mod, newdata = \"marginalmeans\")\n\n# Contrasts at user-specified values\ncomparisons(mod, newdata = datagrid(am = 0, gear = tmp$gear))\ncomparisons(mod, newdata = datagrid(am = unique, gear = max))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\ncomparisons(m, variables = \"hp\", newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Numeric contrasts\nmod &lt;- lm(mpg ~ hp, data = mtcars)\navg_comparisons(mod, variables = list(hp = 1))\navg_comparisons(mod, variables = list(hp = 5))\navg_comparisons(mod, variables = list(hp = c(90, 100)))\navg_comparisons(mod, variables = list(hp = \"iqr\"))\navg_comparisons(mod, variables = list(hp = \"sd\"))\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n\n# using a function to specify a custom difference in one regressor\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (dat$hp - min(dat$hp)) / (max(dat$hp) - min(dat$hp)) + 1\nmodlog &lt;- lm(mpg ~ log(new_hp) + factor(cyl), data = dat)\nfdiff &lt;- \\(x) data.frame(x, x + 10)\navg_comparisons(modlog, variables = list(new_hp = fdiff))\n\n# Adjusted Risk Ratio: see the contrasts vignette\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\navg_comparisons(mod, comparison = \"lnratioavg\", transform = exp)\n\n# Adjusted Risk Ratio: Manual specification of the `comparison`\navg_comparisons(\n     mod,\n     comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n     transform = exp)\n# cross contrasts\nmod &lt;- lm(mpg ~ factor(cyl) * factor(gear) + hp, data = mtcars)\navg_comparisons(mod, variables = c(\"cyl\", \"gear\"), cross = TRUE)\n\n# variable-specific contrasts\navg_comparisons(mod, variables = list(gear = \"sequential\", hp = 10))\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n# Effect of a 1 group-wise standard deviation change\n# First we calculate the SD in each group of `cyl`\n# Second, we use that SD as the treatment size in the `variables` argument\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\ntmp &lt;- mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    mutate(hp_sd = sd(hp))\navg_comparisons(mod, variables = list(hp = tmp$hp_sd), by = \"cyl\")\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\ncomparisons(mod, by = TRUE)\n\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\navg_comparisons(mod, variables = \"hp\", by = c(\"vs\", \"am\"))\n\nlibrary(nnet)\nmod &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\ncomparisons(mod, type = \"probs\", by = by)"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "License",
    "section": "",
    "text": "Copyright (C) 2023 Vincent Arel-Bundock vincent.arel-bundock@umontreal.ca\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\n\n\nVersion 3, 29 June 2007\nCopyright © 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\n\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee."
  },
  {
    "objectID": "LICENSE.html#gnu-general-public-license",
    "href": "LICENSE.html#gnu-general-public-license",
    "title": "License",
    "section": "",
    "text": "Version 3, 29 June 2007\nCopyright © 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed."
  },
  {
    "objectID": "LICENSE.html#preamble",
    "href": "LICENSE.html#preamble",
    "title": "License",
    "section": "",
    "text": "The GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "LICENSE.html#terms-and-conditions",
    "href": "LICENSE.html#terms-and-conditions",
    "title": "License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee."
  },
  {
    "objectID": "vignettes/predictions.html",
    "href": "vignettes/predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "In the context of this package, an “Adjusted Prediction” is defined as:\n\nThe outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. “reference grid”).\n\nHere, the word “Adjusted” simply means “model-derived” or “model-based.”\n\n\nUsing the type argument of the predictions() function we can specify the “scale” on which to make predictions. This refers to either the scale used to estimate the model (i.e., link scale) or to a more interpretable scale (e.g., response scale). For example, when fitting a linear regression model using the lm() function, the link scale and the response scale are identical. An “Adjusted Prediction” computed on either scale will be expressed as the mean value of the response variable at the given values of the predictor variables.\nOn the other hand, when fitting a binary logistic regression model using the glm() function (which uses a binomial family and a logit link ), the link scale and the response scale will be different: an “Adjusted Prediction” computed on the link scale will be expressed as a log odds of a “successful” response at the given values of the predictor variables, whereas an “Adjusted Prediction” computed on the response scale will be expressed as a probability that the response variable equals 1.\nThe default value of the type argument for most models is “response”, which means that the predictions() function will compute predicted probabilities (binomial family), Poisson means (poisson family), etc.\n\n\n\nTo compute adjusted predictions we must first specify the values of the predictors to consider: a “reference grid.” For example, if our model is a linear model fitted with the lm() function which relates the response variable Happiness with the predictor variables Age, Gender and Income, the reference grid could be a data.frame with values for Age, Gender and Income: Age = 40, Gender = Male, Income = 60000.\nThe “reference grid” may or may not correspond to actual observations in the dataset used to fit the model; the example values given above could match the mean values of each variable, or they could represent a specific observed (or hypothetical) individual. The reference grid can include many different rows if we want to make predictions for different combinations of predictors. By default, the predictions() function uses the full original dataset as a reference grid, which means it will compute adjusted predictions for each of the individuals observed in the dataset that was used to fit the model.\n\n\n\nBy default, predictions calculates the regression-adjusted predicted values for every observation in the original dataset:\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\npred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      26.4      0.962 27.5   &lt;0.001 549.0  24.5   28.3\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      15.9      0.992 16.0   &lt;0.001 190.0  14.0   17.9\n#&gt;      20.2      1.219 16.5   &lt;0.001 201.8  17.8   22.5\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nIn many cases, this is too limiting, and researchers will want to specify a grid of “typical” values over which to compute adjusted predictions.\n\n\n\nThere are two main ways to select the reference grid over which we want to compute adjusted predictions. The first is using the variables argument. The second is with the newdata argument and the datagrid() function\n\n\nThe variables argument is a handy way to create and make predictions on counterfactual datasets. For example, here the dataset that we used to fit the model has 32 rows. The counterfactual dataset with two distinct values of hp has 64 rows: each of the original rows appears twice, that is, once with each of the values that we specified in the variables argument:\n\np &lt;- predictions(mod, variables = list(hp = c(100, 120)))\nhead(p)\n#&gt; \n#&gt;  cyl  hp Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    4 100     26.2      0.986 26.63   &lt;0.001 516.6  24.3   28.2\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    8 100     17.7      1.881  9.42   &lt;0.001  67.6  14.0   21.4\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt; \n#&gt; Columns: rowid, rowidcf, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp \n#&gt; Type:  response\nnrow(p)\n#&gt; [1] 64\n\n\n\n\nA second strategy to construct grids of predictors for adjusted predictions is to combine the newdata argument and the datagrid function. Recall that this function creates a “typical” dataset with all variables at their means or modes, except those we explicitly define:\n\ndatagrid(cyl = c(4, 6, 8), model = mod)\n#&gt;        mpg       hp cyl\n#&gt; 1 20.09062 146.6875   4\n#&gt; 2 20.09062 146.6875   6\n#&gt; 3 20.09062 146.6875   8\n\nWe can also use this datagrid function in a predictions call (omitting the model argument):\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp cyl\n#&gt;      16.6       1.28 13   &lt;0.001 125.6  14.1   19.1 147   8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp\n#&gt;    4     25.1       1.37 18.4   &lt;0.001 247.5  22.4   27.8 147\n#&gt;    6     19.2       1.25 15.4   &lt;0.001 174.5  16.7   21.6 147\n#&gt;    8     16.6       1.28 13.0   &lt;0.001 125.6  14.1   19.1 147\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nUsers can change the summary function used to summarize each type of variables using the FUN_numeric, FUN_factor, and related arguments. For example:\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp drat cyl am\n#&gt;      22.0       1.29 17.0   &lt;0.001 214.0  19.4   24.5 123  3.7   6  1\n#&gt;      18.2       1.27 14.3   &lt;0.001 151.9  15.7   20.7 123  3.7   6  0\n#&gt;      25.5       1.32 19.3   &lt;0.001 274.0  23.0   28.1 123  3.7   4  1\n#&gt;      21.8       1.54 14.1   &lt;0.001 148.3  18.8   24.8 123  3.7   4  0\n#&gt;      22.6       2.14 10.6   &lt;0.001  84.2  18.4   26.8 123  3.7   8  1\n#&gt;      18.9       1.73 10.9   &lt;0.001  89.0  15.5   22.3 123  3.7   8  0\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \n#&gt; Type:  response\n\nThe data.frame produced by predictions is “tidy”, which makes it easy to manipulate with other R packages and functions:\n\nlibrary(kableExtra)\nlibrary(tidyverse)\n\npredictions(\n    mod,\n    newdata = datagrid(cyl = mtcars$cyl, hp = c(100, 110))) |&gt;\n    select(hp, cyl, estimate) |&gt;\n    pivot_wider(values_from = estimate, names_from = cyl) |&gt;\n    kbl(caption = \"A table of Adjusted Predictions\") |&gt;\n    kable_styling() |&gt;\n    add_header_above(header = c(\" \" = 1, \"cyl\" = 3))\n\n\nA table of Adjusted Predictions\n\n\n\n\n\n\n\n\n\n\ncyl\n\n\n\nhp\n4\n6\n8\n\n\n\n\n100\n26.24623\n20.27858\n17.72538\n\n\n110\n26.00585\n20.03819\n17.48500\n\n\n\n\n\n\n\n\n\n\nAn alternative approach to construct grids of predictors is to use grid_type = \"counterfactual\" argument value. This will duplicate the whole dataset, with the different values specified by the user.\nFor example, the mtcars dataset has 32 rows. This command produces a new dataset with 64 rows, with each row of the original dataset duplicated with the two values of the am variable supplied (0 and 1):\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\nnd &lt;- datagrid(model = mod, am = 0:1, grid_type = \"counterfactual\")\n\ndim(nd)\n#&gt; [1] 64  4\n\nThen, we can use this dataset and the predictions function to create interesting visualizations:\n\npred &lt;- predictions(mod, newdata = datagrid(am = 0:1, grid_type = \"counterfactual\")) |&gt;\n    select(am, estimate, rowidcf) |&gt;\n    pivot_wider(id_cols = rowidcf, \n                names_from = am,\n                values_from = estimate)\n\nggplot(pred, aes(x = `0`, y = `1`)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1) +\n    labs(x = \"Predicted Pr(vs=1), when am = 0\",\n         y = \"Predicted Pr(vs=1), when am = 1\")\n\n\n\n\nIn this graph, each dot represents the predicted probability that vs=1 for one observation of the dataset, in the counterfactual worlds where am is either 0 or 1.\n\n\n\n\nSome analysts may want to calculate an “Adjusted Prediction at the Mean,” that is, the predicted outcome when all the regressors are held at their mean (or mode). To achieve this, we use the datagrid function. By default, this function produces a grid of data with regressors at their means or modes, so all we need to do to get the APM is:\n\npredictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n\nThis is equivalent to calling:\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n\n\n\n\nAn “Average Adjusted Prediction” is the outcome of a two step process:\n\nCreate a new dataset with each of the original regressor values, but fixing some regressors to values of interest.\nTake the average of the predicted values in this new dataset.\n\nWe can obtain AAPs by applying the avg_*() functions or by argument:\n\nmodlin &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\navg_predictions(modlin)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.1      0.556 36.1   &lt;0.001 946.7    19   21.2\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis is equivalent to:\n\npred &lt;- predictions(modlin)\nmean(pred$estimate)\n#&gt; [1] 20.09062\n\nNote that in GLM models with a non-linear link function, the default type is invlink(link). This means that predictions are first made on the link scale, averaged, and then back transformed. Thus, the average prediction may not be exactly identical to the average of predictions:\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\navg_predictions(mod)$estimate\n#&gt; [1] 0.06308965\n\n## Step 1: predict on the link scale\np &lt;- predictions(mod, type = \"link\")$estimate\n## Step 2: average\np &lt;- mean(p)\n## Step 3: backtransform\nmod$family$linkinv(p)\n#&gt; [1] 0.06308965\n\nUsers who want the average of individual-level predictions on the response scale can specify the type argument explicitly:\n\navg_predictions(mod, type = \"response\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;     0.437     0.0429 10.2   &lt;0.001 78.8 0.353  0.522\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\nWe can compute average adjusted predictions for different subsets of the data with the by argument.\n\npredictions(mod, by = \"am\")\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;   0   0.0591   0.1163 3.1 0.00198  0.665\n#&gt;   1   0.0694   0.0755 3.7 0.00424  0.566\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\nIn the next example, we create a “counterfactual” data grid where each observation of the dataset is repeated twice, with different values of the am variable, and all other variables held at the observed values. We also show the equivalent results using dplyr:\n\npredictions(\n    mod,\n    type = \"response\",\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0    0.526     0.0330 15.93   &lt;0.001 187.3 0.461  0.591\n#&gt;   1    0.330     0.0646  5.11   &lt;0.001  21.6 0.204  0.457\n#&gt; \n#&gt; Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    type = \"response\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mean(estimate))\n#&gt; # A tibble: 2 × 2\n#&gt;      am   AAP\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     0 0.526\n#&gt; 2     1 0.330\n\nNote that the two results are exactly identical when we specify type=\"response\" explicitly. However, they will differ slightly when we leave type unspecified, because marginaleffects will then automatically make predictions and average on the link scale, before backtransforming (\"invlink(link)\"):\n\npredictions(\n    mod,\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;   0  0.24043   0.3922 1.4 2.22e-02  0.815\n#&gt;   1  0.00696   0.0359 4.8 6.81e-05  0.419\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\npredictions(\n    mod,\n    type = \"link\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mod$family$linkinv(mean(estimate)))\n#&gt; # A tibble: 2 × 2\n#&gt;      am     AAP\n#&gt;   &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1     0 0.240  \n#&gt; 2     1 0.00696\n\n\n\nOne place where this is particularly useful is in multinomial models with different response levels. For example, here we compute the average predicted outcome for each outcome level in a multinomial logit model. Note that response levels are identified by the “group” column.\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n## first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n#&gt; \n#&gt;  Group Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n#&gt;      3 4.04e-01   1.97e-01   2.0567   0.0397 4.7  1.90e-02 7.90e-01\n#&gt;      3 1.00e+00   1.25e-03 802.4784   &lt;0.001 Inf  9.98e-01 1.00e+00\n#&gt;      3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \n#&gt; Type:  probs\n\n## average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;      3    0.469     0.0404 11.60   &lt;0.001 100.9 0.3895  0.548\n#&gt;      4    0.375     0.0614  6.11   &lt;0.001  29.9 0.2546  0.495\n#&gt;      5    0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can use custom aggregations by supplying a data frame to the by argument. All columns of this data frame must be present in the output of predictions(), and the data frame must also include a by column of labels. In this example, we “collapse” response groups:\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n#&gt; \n#&gt;   By Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3,4    0.422     0.0231 18.25   &lt;0.001 244.7 0.3766  0.467\n#&gt;  5      0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nThis can be very useful in combination with the hypothesis argument. For example, here we compute the difference between average adjusted predictions for the 3 and 4 response levels, compared to the 5 response level:\n\npredictions(nom, type = \"probs\", by = by, hypothesis = \"sequential\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  5 - 3,4   -0.266     0.0694 -3.83   &lt;0.001 12.9 -0.402  -0.13\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can also use more complicated aggregations. Here, we compute the predicted probability of outcome levels for each value of cyl, by collapsing the “3” and “4” outcome levels:\n\nnom &lt;- multinom(factor(gear) ~ mpg + factor(cyl), data = mtcars, trace = FALSE)\n\nby &lt;- expand.grid(\n    group = 3:5,\n    cyl = c(4, 6, 8),\n    stringsAsFactors = TRUE) |&gt;\n    # define labels\n    transform(by = ifelse(\n        group %in% 3:4,\n        sprintf(\"3/4 Gears & %s Cylinders\", cyl),\n        sprintf(\"5 Gears & %s Cylinders\", cyl)))\n\npredictions(nom, by = by)\n#&gt; \n#&gt;                       By Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders    0.429     0.0661 6.49   &lt;0.001 33.4  0.2991  0.558\n#&gt;  3/4 Gears & 4 Cylinders    0.409     0.0580 7.06   &lt;0.001 39.1  0.2956  0.523\n#&gt;  3/4 Gears & 8 Cylinders    0.429     0.0458 9.35   &lt;0.001 66.6  0.3387  0.518\n#&gt;  5 Gears & 6 Cylinders      0.143     0.1321 1.08    0.280  1.8 -0.1161  0.402\n#&gt;  5 Gears & 4 Cylinders      0.182     0.1159 1.57    0.117  3.1 -0.0457  0.409\n#&gt;  5 Gears & 8 Cylinders      0.143     0.0917 1.56    0.119  3.1 -0.0368  0.323\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nAnd we can then compare the different groups using the hypothesis argument:\n\npredictions(nom, by = by, hypothesis = \"pairwise\")\n#&gt; \n#&gt;                                               Term  Estimate Std. Error         z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 4 Cylinders  1.93e-02     0.0879  0.219839   0.8260 0.3 -0.15294  0.192\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 8 Cylinders  2.84e-05     0.0804  0.000353   0.9997 0.0 -0.15757  0.158\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1982  1.441538   0.1494 2.7 -0.10275  0.674\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1334  1.851412   0.0641 4.0 -0.01449  0.509\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1130  2.527636   0.0115 6.4  0.06415  0.507\n#&gt;  3/4 Gears & 4 Cylinders - 3/4 Gears & 8 Cylinders -1.93e-02     0.0739 -0.261054   0.7941 0.3 -0.16415  0.126\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 6 Cylinders    2.66e-01     0.1443  1.846188   0.0649 3.9 -0.01642  0.549\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 4 Cylinders    2.28e-01     0.1739  1.309481   0.1904 2.4 -0.11312  0.569\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 8 Cylinders    2.66e-01     0.1085  2.455119   0.0141 6.1  0.05371  0.479\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1399  2.042632   0.0411 4.6  0.01156  0.560\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1247  1.981367   0.0476 4.4  0.00267  0.491\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1375  2.076745   0.0378 4.7  0.01606  0.555\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 4 Cylinders     -3.86e-02     0.1758 -0.219838   0.8260 0.3 -0.38317  0.306\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 8 Cylinders     -5.68e-05     0.1608 -0.000353   0.9997 0.0 -0.31526  0.315\n#&gt;  5 Gears & 4 Cylinders - 5 Gears & 8 Cylinders      3.86e-02     0.1478  0.261054   0.7941 0.3 -0.25112  0.328\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\n\n\n\nThe same strategy works for Bayesian models:\n\nlibrary(brms)\nmod &lt;- brm(am ~ mpg * vs, data = mtcars, family = bernoulli)\n\n\npredictions(mod, by = \"vs\")\n#&gt; \n#&gt;  vs Estimate 2.5 % 97.5 %\n#&gt;   0    0.327 0.182  0.507\n#&gt;   1    0.499 0.366  0.672\n#&gt; \n#&gt; Columns: vs, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThe results above show the median of the posterior distribution of group-wise means. Note that we take the mean of predicted values for each MCMC draw before computing quantiles. This is equivalent to:\n\ndraws &lt;- posterior_epred(mod)\nquantile(rowMeans(draws[, mtcars$vs == 0]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.3271836 0.1824479 0.5072074\nquantile(rowMeans(draws[, mtcars$vs == 1]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.4993250 0.3657956 0.6721267\n\n\n\n\n\nFirst, we download the ggplot2movies dataset from the RDatasets archive. Then, we create a variable called certified_fresh for movies with a rating of at least 8. Finally, we discard some outliers and fit a logistic regression model:\n\nlibrary(tidyverse)\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\") |&gt;\n    mutate(style = case_when(Action == 1 ~ \"Action\",\n                             Comedy == 1 ~ \"Comedy\",\n                             Drama == 1 ~ \"Drama\",\n                             TRUE ~ \"Other\"),\n           style = factor(style),\n           certified_fresh = rating &gt;= 8) |&gt;\n    dplyr::filter(length &lt; 240)\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nWe can plot adjusted predictions, conditional on the length variable using the plot_predictions function:\n\nmod &lt;- glm(certified_fresh ~ length, data = dat, family = binomial)\n\nplot_predictions(mod, condition = \"length\")\n\n\n\n\nWe can also introduce another condition which will display a categorical variable like style in different colors. This can be useful in models with interactions:\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"length\", \"style\"))\n\n\n\n\nSince the output of plot_predictions() is a ggplot2 object, it is very easy to customize. For example, we can add points for the actual observations of our dataset like so:\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp, data = mt)\n\nplot_predictions(mod, condition = \"hp\") +\n    geom_point(aes(x = hp, y = mpg), data = mt) +\n    geom_rug(aes(x = hp, y = mpg), data = mt) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nWe can also use plot_predictions() in models with multinomial outcomes or grouped coefficients. For example, notice that when we call draw=FALSE, the result includes a group column:\n\nlibrary(MASS)\nlibrary(ggplot2)\n\nmod &lt;- nnet::multinom(factor(gear) ~ mpg, data = mtcars, trace = FALSE)\n\np &lt;- plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\",\n    draw = FALSE)\n\nhead(p)\n#&gt;   rowid group  estimate  std.error statistic       p.value  s.value  conf.low conf.high gear      mpg\n#&gt; 1     1     3 0.9714990 0.03873404  25.08127 7.962555e-139 458.7548 0.8955817  1.047416    3 10.40000\n#&gt; 2     2     3 0.9656724 0.04394086  21.97664 4.818284e-107 353.1778 0.8795499  1.051795    3 10.87959\n#&gt; 3     3     3 0.9586759 0.04964165  19.31193  4.263532e-83 273.6280 0.8613801  1.055972    3 11.35918\n#&gt; 4     4     3 0.9502914 0.05581338  17.02623  5.247849e-65 213.5336 0.8408992  1.059684    3 11.83878\n#&gt; 5     5     3 0.9402691 0.06240449  15.06733  2.656268e-51 168.0089 0.8179586  1.062580    3 12.31837\n#&gt; 6     6     3 0.9283274 0.06932768  13.39043  6.878233e-41 133.4170 0.7924476  1.064207    3 12.79796\n\nNow we use the group column:\n\nplot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\") +\n    facet_wrap(~group)\n\n\n\n\n\n\n\nThe predictions function computes model-adjusted means on the scale of the output of the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the adjusted predictions should be interpreted on that scale. However, users can pass a string to the type argument, and predictions will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\npred &lt;- predictions(mod, type = \"response\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.598     0.1324 4.52  &lt; 0.001 17.3 0.3384  0.857\n#&gt;     0.492     0.1196 4.11  &lt; 0.001 14.6 0.2573  0.726\n#&gt;     0.297     0.1005 2.95  0.00314  8.3 0.0999  0.494\n#&gt;     0.260     0.0978 2.66  0.00788  7.0 0.0682  0.452\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  response\n\npred &lt;- predictions(mod, type = \"link\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;    0.3967      0.551  0.7204   0.4713 1.1 -0.683  1.4761\n#&gt;   -0.0331      0.479 -0.0692   0.9448 0.1 -0.971  0.9049\n#&gt;   -0.8621      0.482 -1.7903   0.0734 3.8 -1.806  0.0817\n#&gt;   -1.0463      0.509 -2.0575   0.0396 4.7 -2.043 -0.0496\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  link\n\nWe can also plot predictions on different outcome scales:\n\nplot_predictions(mod, condition = \"mpg\", type = \"response\")\n\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"link\")"
  },
  {
    "objectID": "vignettes/predictions.html#prediction-type-or-scale",
    "href": "vignettes/predictions.html#prediction-type-or-scale",
    "title": "Predictions",
    "section": "",
    "text": "Using the type argument of the predictions() function we can specify the “scale” on which to make predictions. This refers to either the scale used to estimate the model (i.e., link scale) or to a more interpretable scale (e.g., response scale). For example, when fitting a linear regression model using the lm() function, the link scale and the response scale are identical. An “Adjusted Prediction” computed on either scale will be expressed as the mean value of the response variable at the given values of the predictor variables.\nOn the other hand, when fitting a binary logistic regression model using the glm() function (which uses a binomial family and a logit link ), the link scale and the response scale will be different: an “Adjusted Prediction” computed on the link scale will be expressed as a log odds of a “successful” response at the given values of the predictor variables, whereas an “Adjusted Prediction” computed on the response scale will be expressed as a probability that the response variable equals 1.\nThe default value of the type argument for most models is “response”, which means that the predictions() function will compute predicted probabilities (binomial family), Poisson means (poisson family), etc."
  },
  {
    "objectID": "vignettes/predictions.html#prediction-grid",
    "href": "vignettes/predictions.html#prediction-grid",
    "title": "Predictions",
    "section": "",
    "text": "To compute adjusted predictions we must first specify the values of the predictors to consider: a “reference grid.” For example, if our model is a linear model fitted with the lm() function which relates the response variable Happiness with the predictor variables Age, Gender and Income, the reference grid could be a data.frame with values for Age, Gender and Income: Age = 40, Gender = Male, Income = 60000.\nThe “reference grid” may or may not correspond to actual observations in the dataset used to fit the model; the example values given above could match the mean values of each variable, or they could represent a specific observed (or hypothetical) individual. The reference grid can include many different rows if we want to make predictions for different combinations of predictors. By default, the predictions() function uses the full original dataset as a reference grid, which means it will compute adjusted predictions for each of the individuals observed in the dataset that was used to fit the model."
  },
  {
    "objectID": "vignettes/predictions.html#the-predictions-function",
    "href": "vignettes/predictions.html#the-predictions-function",
    "title": "Predictions",
    "section": "",
    "text": "By default, predictions calculates the regression-adjusted predicted values for every observation in the original dataset:\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\npred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      26.4      0.962 27.5   &lt;0.001 549.0  24.5   28.3\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      15.9      0.992 16.0   &lt;0.001 190.0  14.0   17.9\n#&gt;      20.2      1.219 16.5   &lt;0.001 201.8  17.8   22.5\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nIn many cases, this is too limiting, and researchers will want to specify a grid of “typical” values over which to compute adjusted predictions."
  },
  {
    "objectID": "vignettes/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "href": "vignettes/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "title": "Predictions",
    "section": "",
    "text": "There are two main ways to select the reference grid over which we want to compute adjusted predictions. The first is using the variables argument. The second is with the newdata argument and the datagrid() function\n\n\nThe variables argument is a handy way to create and make predictions on counterfactual datasets. For example, here the dataset that we used to fit the model has 32 rows. The counterfactual dataset with two distinct values of hp has 64 rows: each of the original rows appears twice, that is, once with each of the values that we specified in the variables argument:\n\np &lt;- predictions(mod, variables = list(hp = c(100, 120)))\nhead(p)\n#&gt; \n#&gt;  cyl  hp Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    4 100     26.2      0.986 26.63   &lt;0.001 516.6  24.3   28.2\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    8 100     17.7      1.881  9.42   &lt;0.001  67.6  14.0   21.4\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt; \n#&gt; Columns: rowid, rowidcf, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp \n#&gt; Type:  response\nnrow(p)\n#&gt; [1] 64\n\n\n\n\nA second strategy to construct grids of predictors for adjusted predictions is to combine the newdata argument and the datagrid function. Recall that this function creates a “typical” dataset with all variables at their means or modes, except those we explicitly define:\n\ndatagrid(cyl = c(4, 6, 8), model = mod)\n#&gt;        mpg       hp cyl\n#&gt; 1 20.09062 146.6875   4\n#&gt; 2 20.09062 146.6875   6\n#&gt; 3 20.09062 146.6875   8\n\nWe can also use this datagrid function in a predictions call (omitting the model argument):\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp cyl\n#&gt;      16.6       1.28 13   &lt;0.001 125.6  14.1   19.1 147   8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp\n#&gt;    4     25.1       1.37 18.4   &lt;0.001 247.5  22.4   27.8 147\n#&gt;    6     19.2       1.25 15.4   &lt;0.001 174.5  16.7   21.6 147\n#&gt;    8     16.6       1.28 13.0   &lt;0.001 125.6  14.1   19.1 147\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nUsers can change the summary function used to summarize each type of variables using the FUN_numeric, FUN_factor, and related arguments. For example:\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp drat cyl am\n#&gt;      22.0       1.29 17.0   &lt;0.001 214.0  19.4   24.5 123  3.7   6  1\n#&gt;      18.2       1.27 14.3   &lt;0.001 151.9  15.7   20.7 123  3.7   6  0\n#&gt;      25.5       1.32 19.3   &lt;0.001 274.0  23.0   28.1 123  3.7   4  1\n#&gt;      21.8       1.54 14.1   &lt;0.001 148.3  18.8   24.8 123  3.7   4  0\n#&gt;      22.6       2.14 10.6   &lt;0.001  84.2  18.4   26.8 123  3.7   8  1\n#&gt;      18.9       1.73 10.9   &lt;0.001  89.0  15.5   22.3 123  3.7   8  0\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \n#&gt; Type:  response\n\nThe data.frame produced by predictions is “tidy”, which makes it easy to manipulate with other R packages and functions:\n\nlibrary(kableExtra)\nlibrary(tidyverse)\n\npredictions(\n    mod,\n    newdata = datagrid(cyl = mtcars$cyl, hp = c(100, 110))) |&gt;\n    select(hp, cyl, estimate) |&gt;\n    pivot_wider(values_from = estimate, names_from = cyl) |&gt;\n    kbl(caption = \"A table of Adjusted Predictions\") |&gt;\n    kable_styling() |&gt;\n    add_header_above(header = c(\" \" = 1, \"cyl\" = 3))\n\n\nA table of Adjusted Predictions\n\n\n\n\n\n\n\n\n\n\ncyl\n\n\n\nhp\n4\n6\n8\n\n\n\n\n100\n26.24623\n20.27858\n17.72538\n\n\n110\n26.00585\n20.03819\n17.48500\n\n\n\n\n\n\n\n\n\n\nAn alternative approach to construct grids of predictors is to use grid_type = \"counterfactual\" argument value. This will duplicate the whole dataset, with the different values specified by the user.\nFor example, the mtcars dataset has 32 rows. This command produces a new dataset with 64 rows, with each row of the original dataset duplicated with the two values of the am variable supplied (0 and 1):\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\nnd &lt;- datagrid(model = mod, am = 0:1, grid_type = \"counterfactual\")\n\ndim(nd)\n#&gt; [1] 64  4\n\nThen, we can use this dataset and the predictions function to create interesting visualizations:\n\npred &lt;- predictions(mod, newdata = datagrid(am = 0:1, grid_type = \"counterfactual\")) |&gt;\n    select(am, estimate, rowidcf) |&gt;\n    pivot_wider(id_cols = rowidcf, \n                names_from = am,\n                values_from = estimate)\n\nggplot(pred, aes(x = `0`, y = `1`)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1) +\n    labs(x = \"Predicted Pr(vs=1), when am = 0\",\n         y = \"Predicted Pr(vs=1), when am = 1\")\n\n\n\n\nIn this graph, each dot represents the predicted probability that vs=1 for one observation of the dataset, in the counterfactual worlds where am is either 0 or 1."
  },
  {
    "objectID": "vignettes/predictions.html#adjusted-prediction-at-the-mean-apm",
    "href": "vignettes/predictions.html#adjusted-prediction-at-the-mean-apm",
    "title": "Predictions",
    "section": "",
    "text": "Some analysts may want to calculate an “Adjusted Prediction at the Mean,” that is, the predicted outcome when all the regressors are held at their mean (or mode). To achieve this, we use the datagrid function. By default, this function produces a grid of data with regressors at their means or modes, so all we need to do to get the APM is:\n\npredictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n\nThis is equivalent to calling:\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)"
  },
  {
    "objectID": "vignettes/predictions.html#average-adjusted-predictions-aap",
    "href": "vignettes/predictions.html#average-adjusted-predictions-aap",
    "title": "Predictions",
    "section": "",
    "text": "An “Average Adjusted Prediction” is the outcome of a two step process:\n\nCreate a new dataset with each of the original regressor values, but fixing some regressors to values of interest.\nTake the average of the predicted values in this new dataset.\n\nWe can obtain AAPs by applying the avg_*() functions or by argument:\n\nmodlin &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\navg_predictions(modlin)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.1      0.556 36.1   &lt;0.001 946.7    19   21.2\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis is equivalent to:\n\npred &lt;- predictions(modlin)\nmean(pred$estimate)\n#&gt; [1] 20.09062\n\nNote that in GLM models with a non-linear link function, the default type is invlink(link). This means that predictions are first made on the link scale, averaged, and then back transformed. Thus, the average prediction may not be exactly identical to the average of predictions:\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\navg_predictions(mod)$estimate\n#&gt; [1] 0.06308965\n\n## Step 1: predict on the link scale\np &lt;- predictions(mod, type = \"link\")$estimate\n## Step 2: average\np &lt;- mean(p)\n## Step 3: backtransform\nmod$family$linkinv(p)\n#&gt; [1] 0.06308965\n\nUsers who want the average of individual-level predictions on the response scale can specify the type argument explicitly:\n\navg_predictions(mod, type = \"response\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;     0.437     0.0429 10.2   &lt;0.001 78.8 0.353  0.522\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response"
  },
  {
    "objectID": "vignettes/predictions.html#average-adjusted-predictions-by-group",
    "href": "vignettes/predictions.html#average-adjusted-predictions-by-group",
    "title": "Predictions",
    "section": "",
    "text": "We can compute average adjusted predictions for different subsets of the data with the by argument.\n\npredictions(mod, by = \"am\")\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;   0   0.0591   0.1163 3.1 0.00198  0.665\n#&gt;   1   0.0694   0.0755 3.7 0.00424  0.566\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\nIn the next example, we create a “counterfactual” data grid where each observation of the dataset is repeated twice, with different values of the am variable, and all other variables held at the observed values. We also show the equivalent results using dplyr:\n\npredictions(\n    mod,\n    type = \"response\",\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0    0.526     0.0330 15.93   &lt;0.001 187.3 0.461  0.591\n#&gt;   1    0.330     0.0646  5.11   &lt;0.001  21.6 0.204  0.457\n#&gt; \n#&gt; Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    type = \"response\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mean(estimate))\n#&gt; # A tibble: 2 × 2\n#&gt;      am   AAP\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     0 0.526\n#&gt; 2     1 0.330\n\nNote that the two results are exactly identical when we specify type=\"response\" explicitly. However, they will differ slightly when we leave type unspecified, because marginaleffects will then automatically make predictions and average on the link scale, before backtransforming (\"invlink(link)\"):\n\npredictions(\n    mod,\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;   0  0.24043   0.3922 1.4 2.22e-02  0.815\n#&gt;   1  0.00696   0.0359 4.8 6.81e-05  0.419\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\npredictions(\n    mod,\n    type = \"link\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mod$family$linkinv(mean(estimate)))\n#&gt; # A tibble: 2 × 2\n#&gt;      am     AAP\n#&gt;   &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1     0 0.240  \n#&gt; 2     1 0.00696\n\n\n\nOne place where this is particularly useful is in multinomial models with different response levels. For example, here we compute the average predicted outcome for each outcome level in a multinomial logit model. Note that response levels are identified by the “group” column.\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n## first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n#&gt; \n#&gt;  Group Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n#&gt;      3 4.04e-01   1.97e-01   2.0567   0.0397 4.7  1.90e-02 7.90e-01\n#&gt;      3 1.00e+00   1.25e-03 802.4784   &lt;0.001 Inf  9.98e-01 1.00e+00\n#&gt;      3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \n#&gt; Type:  probs\n\n## average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;      3    0.469     0.0404 11.60   &lt;0.001 100.9 0.3895  0.548\n#&gt;      4    0.375     0.0614  6.11   &lt;0.001  29.9 0.2546  0.495\n#&gt;      5    0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can use custom aggregations by supplying a data frame to the by argument. All columns of this data frame must be present in the output of predictions(), and the data frame must also include a by column of labels. In this example, we “collapse” response groups:\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n#&gt; \n#&gt;   By Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3,4    0.422     0.0231 18.25   &lt;0.001 244.7 0.3766  0.467\n#&gt;  5      0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nThis can be very useful in combination with the hypothesis argument. For example, here we compute the difference between average adjusted predictions for the 3 and 4 response levels, compared to the 5 response level:\n\npredictions(nom, type = \"probs\", by = by, hypothesis = \"sequential\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  5 - 3,4   -0.266     0.0694 -3.83   &lt;0.001 12.9 -0.402  -0.13\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can also use more complicated aggregations. Here, we compute the predicted probability of outcome levels for each value of cyl, by collapsing the “3” and “4” outcome levels:\n\nnom &lt;- multinom(factor(gear) ~ mpg + factor(cyl), data = mtcars, trace = FALSE)\n\nby &lt;- expand.grid(\n    group = 3:5,\n    cyl = c(4, 6, 8),\n    stringsAsFactors = TRUE) |&gt;\n    # define labels\n    transform(by = ifelse(\n        group %in% 3:4,\n        sprintf(\"3/4 Gears & %s Cylinders\", cyl),\n        sprintf(\"5 Gears & %s Cylinders\", cyl)))\n\npredictions(nom, by = by)\n#&gt; \n#&gt;                       By Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders    0.429     0.0661 6.49   &lt;0.001 33.4  0.2991  0.558\n#&gt;  3/4 Gears & 4 Cylinders    0.409     0.0580 7.06   &lt;0.001 39.1  0.2956  0.523\n#&gt;  3/4 Gears & 8 Cylinders    0.429     0.0458 9.35   &lt;0.001 66.6  0.3387  0.518\n#&gt;  5 Gears & 6 Cylinders      0.143     0.1321 1.08    0.280  1.8 -0.1161  0.402\n#&gt;  5 Gears & 4 Cylinders      0.182     0.1159 1.57    0.117  3.1 -0.0457  0.409\n#&gt;  5 Gears & 8 Cylinders      0.143     0.0917 1.56    0.119  3.1 -0.0368  0.323\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nAnd we can then compare the different groups using the hypothesis argument:\n\npredictions(nom, by = by, hypothesis = \"pairwise\")\n#&gt; \n#&gt;                                               Term  Estimate Std. Error         z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 4 Cylinders  1.93e-02     0.0879  0.219839   0.8260 0.3 -0.15294  0.192\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 8 Cylinders  2.84e-05     0.0804  0.000353   0.9997 0.0 -0.15757  0.158\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1982  1.441538   0.1494 2.7 -0.10275  0.674\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1334  1.851412   0.0641 4.0 -0.01449  0.509\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1130  2.527636   0.0115 6.4  0.06415  0.507\n#&gt;  3/4 Gears & 4 Cylinders - 3/4 Gears & 8 Cylinders -1.93e-02     0.0739 -0.261054   0.7941 0.3 -0.16415  0.126\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 6 Cylinders    2.66e-01     0.1443  1.846188   0.0649 3.9 -0.01642  0.549\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 4 Cylinders    2.28e-01     0.1739  1.309481   0.1904 2.4 -0.11312  0.569\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 8 Cylinders    2.66e-01     0.1085  2.455119   0.0141 6.1  0.05371  0.479\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1399  2.042632   0.0411 4.6  0.01156  0.560\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1247  1.981367   0.0476 4.4  0.00267  0.491\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1375  2.076745   0.0378 4.7  0.01606  0.555\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 4 Cylinders     -3.86e-02     0.1758 -0.219838   0.8260 0.3 -0.38317  0.306\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 8 Cylinders     -5.68e-05     0.1608 -0.000353   0.9997 0.0 -0.31526  0.315\n#&gt;  5 Gears & 4 Cylinders - 5 Gears & 8 Cylinders      3.86e-02     0.1478  0.261054   0.7941 0.3 -0.25112  0.328\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\n\n\n\nThe same strategy works for Bayesian models:\n\nlibrary(brms)\nmod &lt;- brm(am ~ mpg * vs, data = mtcars, family = bernoulli)\n\n\npredictions(mod, by = \"vs\")\n#&gt; \n#&gt;  vs Estimate 2.5 % 97.5 %\n#&gt;   0    0.327 0.182  0.507\n#&gt;   1    0.499 0.366  0.672\n#&gt; \n#&gt; Columns: vs, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThe results above show the median of the posterior distribution of group-wise means. Note that we take the mean of predicted values for each MCMC draw before computing quantiles. This is equivalent to:\n\ndraws &lt;- posterior_epred(mod)\nquantile(rowMeans(draws[, mtcars$vs == 0]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.3271836 0.1824479 0.5072074\nquantile(rowMeans(draws[, mtcars$vs == 1]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.4993250 0.3657956 0.6721267"
  },
  {
    "objectID": "vignettes/predictions.html#conditional-adjusted-predictions-plot",
    "href": "vignettes/predictions.html#conditional-adjusted-predictions-plot",
    "title": "Predictions",
    "section": "",
    "text": "First, we download the ggplot2movies dataset from the RDatasets archive. Then, we create a variable called certified_fresh for movies with a rating of at least 8. Finally, we discard some outliers and fit a logistic regression model:\n\nlibrary(tidyverse)\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\") |&gt;\n    mutate(style = case_when(Action == 1 ~ \"Action\",\n                             Comedy == 1 ~ \"Comedy\",\n                             Drama == 1 ~ \"Drama\",\n                             TRUE ~ \"Other\"),\n           style = factor(style),\n           certified_fresh = rating &gt;= 8) |&gt;\n    dplyr::filter(length &lt; 240)\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nWe can plot adjusted predictions, conditional on the length variable using the plot_predictions function:\n\nmod &lt;- glm(certified_fresh ~ length, data = dat, family = binomial)\n\nplot_predictions(mod, condition = \"length\")\n\n\n\n\nWe can also introduce another condition which will display a categorical variable like style in different colors. This can be useful in models with interactions:\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"length\", \"style\"))\n\n\n\n\nSince the output of plot_predictions() is a ggplot2 object, it is very easy to customize. For example, we can add points for the actual observations of our dataset like so:\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp, data = mt)\n\nplot_predictions(mod, condition = \"hp\") +\n    geom_point(aes(x = hp, y = mpg), data = mt) +\n    geom_rug(aes(x = hp, y = mpg), data = mt) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nWe can also use plot_predictions() in models with multinomial outcomes or grouped coefficients. For example, notice that when we call draw=FALSE, the result includes a group column:\n\nlibrary(MASS)\nlibrary(ggplot2)\n\nmod &lt;- nnet::multinom(factor(gear) ~ mpg, data = mtcars, trace = FALSE)\n\np &lt;- plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\",\n    draw = FALSE)\n\nhead(p)\n#&gt;   rowid group  estimate  std.error statistic       p.value  s.value  conf.low conf.high gear      mpg\n#&gt; 1     1     3 0.9714990 0.03873404  25.08127 7.962555e-139 458.7548 0.8955817  1.047416    3 10.40000\n#&gt; 2     2     3 0.9656724 0.04394086  21.97664 4.818284e-107 353.1778 0.8795499  1.051795    3 10.87959\n#&gt; 3     3     3 0.9586759 0.04964165  19.31193  4.263532e-83 273.6280 0.8613801  1.055972    3 11.35918\n#&gt; 4     4     3 0.9502914 0.05581338  17.02623  5.247849e-65 213.5336 0.8408992  1.059684    3 11.83878\n#&gt; 5     5     3 0.9402691 0.06240449  15.06733  2.656268e-51 168.0089 0.8179586  1.062580    3 12.31837\n#&gt; 6     6     3 0.9283274 0.06932768  13.39043  6.878233e-41 133.4170 0.7924476  1.064207    3 12.79796\n\nNow we use the group column:\n\nplot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\") +\n    facet_wrap(~group)"
  },
  {
    "objectID": "vignettes/predictions.html#prediction-types",
    "href": "vignettes/predictions.html#prediction-types",
    "title": "Predictions",
    "section": "",
    "text": "The predictions function computes model-adjusted means on the scale of the output of the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the adjusted predictions should be interpreted on that scale. However, users can pass a string to the type argument, and predictions will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\npred &lt;- predictions(mod, type = \"response\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.598     0.1324 4.52  &lt; 0.001 17.3 0.3384  0.857\n#&gt;     0.492     0.1196 4.11  &lt; 0.001 14.6 0.2573  0.726\n#&gt;     0.297     0.1005 2.95  0.00314  8.3 0.0999  0.494\n#&gt;     0.260     0.0978 2.66  0.00788  7.0 0.0682  0.452\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  response\n\npred &lt;- predictions(mod, type = \"link\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;    0.3967      0.551  0.7204   0.4713 1.1 -0.683  1.4761\n#&gt;   -0.0331      0.479 -0.0692   0.9448 0.1 -0.971  0.9049\n#&gt;   -0.8621      0.482 -1.7903   0.0734 3.8 -1.806  0.0817\n#&gt;   -1.0463      0.509 -2.0575   0.0396 4.7 -2.043 -0.0496\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  link\n\nWe can also plot predictions on different outcome scales:\n\nplot_predictions(mod, condition = \"mpg\", type = \"response\")\n\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"link\")"
  }
]