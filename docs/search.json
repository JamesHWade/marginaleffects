[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Marginal Effects Zoo (0.16.0)",
    "section": "",
    "text": "üö®üö®üö® This November, the marginaleffects author (Vincent) will be giving an online seminar called: ‚ÄúInterpreting and Communicating Statistical Results with R.‚Äù If you want to become a marginaleffects expert and support the development of the package, sign up at Code Horizons! üö®üö®üö®\n\n1 Why?\nInterpreting the parameters estimated by complex statistical models is often challenging. Many applied researchers are keen to report simple quantities that carry clear scientific meaning but, in doing so, they face three primary obstacles:\n\nIntuitive estimands‚Äîand their standard errors‚Äîare often tedious to compute.\nThe terminology to describe these estimands is not standardized, and varies tremendously across disciplines.\nModeling packages in R and Python produce inconsistent objects which require users to write custom (and error-prone) code to interpret statistical results.\n\nThe ‚ÄúMarginal Effects Zoo‚Äù book and the marginaleffects packages for R and Python are designed to help analysts overcome these challenges. The free online book provides a unified framework to describe and compute a wide range of estimands. The marginaleffects package implements this framework and offers a consistent interface to interpret the estimates from over 88 classes of statistical models.\n\n2 What?\nThe marginaleffects package allows R and Python users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons (contrasts, risk ratios, odds, lift, etc.), and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, ‚Äúmarginal means‚Äù, which is a special case of averaged predictions. marginaleffects can also average (or ‚Äúmarginalize‚Äù) unit-level (or ‚Äúconditional‚Äù) estimates of all those quantities. Finally, marginaleffects can also conduct hypothesis and eequivalence tests on coefficient estimates and on any of the quantities generated by the package.\nPredictions:\n\nThe outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. predictions(), avg_predictions(), plot_predictions().\n\nComparisons:\n\nCompare the predictions made by a model for different regressor values (e.g., college graduates vs.¬†others): contrasts, differences, risk ratios, odds, lift, etc. comparisons(), avg_comparisons(), plot_comparisons().\n\nSlopes:\n\nPartial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. slopes(), avg_slopes(), plot_slopes().\n\nMarginal Means:\n\nPredictions of a model, averaged across a ‚Äúreference grid‚Äù of categorical predictors. marginalmeans().\n\nHypothesis and Equivalence Tests:\n\nHypothesis and equivalence tests can be conducted on linear or non-linear functions of model coefficients, or on any of the quantities computed by the marginaleffects packages (predictions, slopes, comparisons, marginal means, etc.). Uncertainy estimates can be obtained via the delta method (with or without robust standard errors), bootstrap, or simulation.\n\n\n\n\n\nGoal\nFunction\n\n\n\nPredictions\npredictions()\n\n\n\navg_predictions()\n\n\n\nplot_predictions()\n\n\nComparisons\ncomparisons()\n\n\n\navg_comparisons()\n\n\n\nplot_comparisons()\n\n\nSlopes\nslopes()\n\n\n\navg_slopes()\n\n\n\nplot_slopes()\n\n\nMarginal Means\nmarginal_means()\n\n\nGrids\ndatagrid()\n\n\n\ndatagridcf()\n\n\nHypothesis & Equivalence\nhypotheses()\n\n\nBayes, Bootstrap, Simulation\nposterior_draws()\n\n\n\ninferences()\n\n\n\n\n\n\n3 Benefits\nThe advantages of marginaleffects include:\n\n\nPowerful: It can compute predictions, comparisons (contrasts, risk ratios, etc.), slopes, and conduct hypothesis tests for 88 different classes of models in R and Python.\n\nSimple: All functions share a simple and unified interface.\n\nDocumented: Each function is thoroughly documented with abundant examples. The website includes 20,000+ words of vignettes and case studies.\n\nEfficient: Some operations are orders of magnitude faster than with the margins package, and the memory footprint is much smaller.\n\nThin: Few dependencies.\n\nStandards-compliant: marginaleffects follows ‚Äútidy‚Äù principles and returns objects that work with standard functions like summary(), head(), tidy(), and glance(). These objects are easy to program with and feed to other packages like modelsummary.\n\n\nValid: When possible, numerical results are checked against alternative software like Stata or other R packages. Unfortunately, it is not possible to test every model type, so users are still strongly encouraged to cross-check their results.\n\nExtensible: Adding support for new models is very easy, often requiring less than 10 lines of new code. Please submit feature requests on Github.\n\n\nActive development: Bugs are fixed promptly.\n\n4 License and Citation\nThe marginaleffects package is licensed under the GNU General Public License v3.0. The content of this website/book is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).\n\nWarning in citation(\"marginaleffects\"): no date field in DESCRIPTION file of package 'marginaleffects'\n\nTo cite package 'marginaleffects' in publications use:\n\n  Arel-Bundock V (2023). _marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests_. R package version 0.15.1.9011, &lt;https://marginaleffects.com/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests},\n    author = {Vincent Arel-Bundock},\n    year = {2023},\n    note = {R package version 0.15.1.9011},\n    url = {https://marginaleffects.com/},\n  }",
    "crumbs": [
      "marginaleffects"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#installation",
    "href": "articles/marginaleffects.html#installation",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\n\n\nR\nPython\n\n\n\nInstall the latest CRAN release:\n\ninstall.packages(\"marginaleffects\")\n\nInstall the development version:\n\ninstall.packages(\n    c(\"marginaleffects\", \"insight\"),\n    repos = c(\"https://vincentarelbundock.r-universe.dev\", \"https://easystats.r-universe.dev\"))\n\nRestart R completely before moving on.\n\n\nInstall from PyPI:\n\npip install marginaleffects",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#estimands-predictions-comparisons-and-slopes",
    "href": "articles/marginaleffects.html#estimands-predictions-comparisons-and-slopes",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.2 Estimands: Predictions, Comparisons, and Slopes",
    "text": "1.2 Estimands: Predictions, Comparisons, and Slopes\nThe marginaleffects package allows R users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons, and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, ‚Äúmarginal means‚Äù, which is a special case of averaged predictions. marginaleffects can also average (or ‚Äúmarginalize‚Äù) unit-level (or ‚Äúconditional‚Äù) estimates of all those quantities, and conduct hypothesis tests on them.\nPredictions:\n\nThe outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. predictions(), avg_predictions(), plot_predictions().\n\nComparisons:\n\nCompare the predictions made by a model for different regressor values (e.g., college graduates vs.¬†others): contrasts, differences, risk ratios, odds, etc. comparisons(), avg_comparisons(), plot_comparisons().\n\nSlopes:\n\nPartial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. slopes(), avg_slopes(), plot_slopes().\n\nMarginal Means:\n\nPredictions of a model, averaged across a ‚Äúreference grid‚Äù of categorical predictors. marginalmeans().\n\nHypothesis and Equivalence Tests:\n\nHypothesis and equivalence tests can be conducted on linear or non-linear functions of model coefficients, or on any of the quantities computed by the marginaleffects packages (predictions, slopes, comparisons, marginal means, etc.). Uncertainy estimates can be obtained via the delta method (with or without robust standard errors), bootstrap, or simulation.\n\nPredictions, comparisons, and slopes are fundamentally unit-level (or ‚Äúconditional‚Äù) quantities. Except in the simplest linear case, estimates will typically vary based on the values of all the regressors in a model. Each of the observations in a dataset is thus associated with its own prediction, comparison, and slope estimates. Below, we will see that it can be useful to marginalize (or ‚Äúaverage over‚Äù) unit-level estimates to report an ‚Äúaverage prediction‚Äù, ‚Äúaverage comparison‚Äù, or ‚Äúaverage slope‚Äù.\nOne ambiguous aspect of the definitions above is that the word ‚Äúmarginal‚Äù comes up in two different and opposite ways:\n\nIn ‚Äúmarginal effects,‚Äù we refer to the effect of a tiny (marginal) change in the regressor on the outcome. This is a slope, or derivative.\nIn ‚Äúmarginal means,‚Äù we refer to the process of marginalizing across rows of a prediction grid. This is an average, or integral.\n\nOn this website and in this package, we reserve the expression ‚Äúmarginal effect‚Äù to mean a ‚Äúslope‚Äù or ‚Äúpartial derivative‚Äù.\nThe marginaleffects package includes functions to estimate, average, plot, and summarize all of the estimands described above. The objects produced by marginaleffects are ‚Äútidy‚Äù: they produce simple data frames in ‚Äúlong‚Äù format. They are also ‚Äústandards-compliant‚Äù and work seamlessly with standard functions like summary(), head(), tidy(), and glance(), as well with external packages like modelsummary or ggplot2.\nWe now apply marginaleffects functions to compute each of the estimands described above. First, we fit a linear regression model with multiplicative interactions:\n\n\nR\nPython\n\n\n\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\n\n\n\n\nimport polars as pl\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom marginaleffects import *\n\nmtcars = pl.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmod = smf.ols(\"mpg ~ hp * wt * am\", data = mtcars).fit()\n\n\n\n\nThen, we call the predictions() function. As noted above, predictions are unit-level estimates, so there is one specific prediction per observation. By default, the predictions() function makes one prediction per observation in the dataset that was used to fit the original model. Since mtcars has 32 rows, the predictions() outcome also has 32 rows:\n\n\nR\nPython\n\n\n\n\npre &lt;- predictions(mod)\n\nnrow(mtcars)\n\n[1] 32\n\nnrow(pre)\n\n[1] 32\n\npre\n\n\n Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     22.5      0.884 25.44   &lt;0.001 471.7  20.8   24.2\n     20.8      1.194 17.42   &lt;0.001 223.3  18.5   23.1\n     25.3      0.709 35.66   &lt;0.001 922.7  23.9   26.7\n     20.3      0.704 28.75   &lt;0.001 601.5  18.9   21.6\n     17.0      0.712 23.88   &lt;0.001 416.2  15.6   18.4\n--- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n     29.6      1.874 15.80   &lt;0.001 184.3  25.9   33.3\n     15.9      1.311 12.13   &lt;0.001 110.0  13.3   18.5\n     19.4      1.145 16.95   &lt;0.001 211.6  17.2   21.7\n     14.8      2.017  7.33   &lt;0.001  42.0  10.8   18.7\n     21.5      1.072 20.02   &lt;0.001 293.8  19.4   23.6\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response \n\n\n\n\n\npre = predictions(mod)\n\nmtcars.shape\n\n(32, 12)\n\npre.shape\n\n(32, 20)\n\nprint(pre)\n\nshape: (32, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Estimate ‚îÜ Std.Error ‚îÜ z    ‚îÜ P(&gt;|z|)  ‚îÜ S    ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---      ‚îÜ ---       ‚îÜ ---  ‚îÜ ---      ‚îÜ ---  ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str      ‚îÜ str       ‚îÜ str  ‚îÜ str      ‚îÜ str  ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 22.5     ‚îÜ 0.884     ‚îÜ 25.4 ‚îÜ 0        ‚îÜ inf  ‚îÜ 20.7 ‚îÜ 24.3  ‚îÇ\n‚îÇ 20.8     ‚îÜ 1.19      ‚îÜ 17.4 ‚îÜ 4e-15    ‚îÜ 47.8 ‚îÜ 18.3 ‚îÜ 23.3  ‚îÇ\n‚îÇ 25.3     ‚îÜ 0.709     ‚îÜ 35.7 ‚îÜ 0        ‚îÜ inf  ‚îÜ 23.8 ‚îÜ 26.7  ‚îÇ\n‚îÇ 20.3     ‚îÜ 0.704     ‚îÜ 28.8 ‚îÜ 0        ‚îÜ inf  ‚îÜ 18.8 ‚îÜ 21.7  ‚îÇ\n‚îÇ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶    ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶    ‚îÜ ‚Ä¶    ‚îÜ ‚Ä¶     ‚îÇ\n‚îÇ 15.9     ‚îÜ 1.31      ‚îÜ 12.1 ‚îÜ 1e-11    ‚îÜ 36.5 ‚îÜ 13.2 ‚îÜ 18.6  ‚îÇ\n‚îÇ 19.4     ‚îÜ 1.15      ‚îÜ 16.9 ‚îÜ 7.33e-15 ‚îÜ 47   ‚îÜ 17   ‚îÜ 21.8  ‚îÇ\n‚îÇ 14.8     ‚îÜ 2.02      ‚îÜ 7.33 ‚îÜ 1.43e-07 ‚îÜ 22.7 ‚îÜ 10.6 ‚îÜ 19    ‚îÇ\n‚îÇ 21.5     ‚îÜ 1.07      ‚îÜ 20   ‚îÜ 2.22e-16 ‚îÜ 52   ‚îÜ 19.2 ‚îÜ 23.7  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\n\n\n\n\nNow, we use the comparisons() function to compute the difference in predicted outcome when each of the predictors is incremented by 1 unit (one predictor at a time, holding all others constant). Once again, comparisons are unit-level quantities. And since there are 3 predictors in the model and our data has 32 rows, we obtain 96 comparisons:\n\n\nR\nPython\n\n\n\n\ncmp &lt;- comparisons(mod)\n\nnrow(cmp)\n\n[1] 96\n\ncmp\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   am    1 - 0    0.325       1.68  0.193   0.8467 0.2  -2.97  3.622\n   am    1 - 0   -0.544       1.57 -0.347   0.7287 0.5  -3.62  2.530\n   am    1 - 0    1.201       2.35  0.511   0.6090 0.7  -3.40  5.802\n   am    1 - 0   -1.703       1.87 -0.912   0.3618 1.5  -5.36  1.957\n   am    1 - 0   -0.615       1.68 -0.366   0.7146 0.5  -3.91  2.680\n--- 86 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   wt    +1      -6.518       1.88 -3.462   &lt;0.001 10.9 -10.21 -2.828\n   wt    +1      -1.653       3.74 -0.442   0.6588  0.6  -8.99  5.683\n   wt    +1      -4.520       2.47 -1.830   0.0672  3.9  -9.36  0.321\n   wt    +1       0.635       4.89  0.130   0.8966  0.2  -8.95 10.216\n   wt    +1      -6.647       1.86 -3.572   &lt;0.001 11.5 -10.29 -2.999\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response \n\n\n\n\n\ncmp = comparisons(mod)\n\ncmp.shape\n\n(96, 25)\n\nprint(cmp)\n\nshape: (96, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%    ‚îÜ 97.5%   ‚îÇ\n‚îÇ ---  ‚îÜ ---      ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ ---   ‚îÜ ---     ‚îÜ ---     ‚îÇ\n‚îÇ str  ‚îÜ str      ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str   ‚îÜ str     ‚îÜ str     ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ hp   ‚îÜ +1       ‚îÜ -0.0369  ‚îÜ 0.0185    ‚îÜ ‚Ä¶ ‚îÜ 0.0575  ‚îÜ 4.12  ‚îÜ -0.0751 ‚îÜ 0.00128 ‚îÇ\n‚îÇ hp   ‚îÜ +1       ‚îÜ -0.0287  ‚îÜ 0.0156    ‚îÜ ‚Ä¶ ‚îÜ 0.0788  ‚îÜ 3.67  ‚îÜ -0.0609 ‚îÜ 0.00357 ‚îÇ\n‚îÇ hp   ‚îÜ +1       ‚îÜ -0.0466  ‚îÜ 0.0226    ‚îÜ ‚Ä¶ ‚îÜ 0.0502  ‚îÜ 4.32  ‚îÜ -0.0932 ‚îÜ 4.6e-05 ‚îÇ\n‚îÇ hp   ‚îÜ +1       ‚îÜ -0.0423  ‚îÜ 0.0133    ‚îÜ ‚Ä¶ ‚îÜ 0.00401 ‚îÜ 7.96  ‚îÜ -0.0697 ‚îÜ -0.0149 ‚îÇ\n‚îÇ ‚Ä¶    ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶       ‚îÇ\n‚îÇ am   ‚îÜ 1 - 0    ‚îÜ 2.11     ‚îÜ 2.29      ‚îÜ ‚Ä¶ ‚îÜ 0.367   ‚îÜ 1.45  ‚îÜ -2.62   ‚îÜ 6.83    ‚îÇ\n‚îÇ am   ‚îÜ 1 - 0    ‚îÜ 0.895    ‚îÜ 1.64      ‚îÜ ‚Ä¶ ‚îÜ 0.591   ‚îÜ 0.758 ‚îÜ -2.5    ‚îÜ 4.29    ‚îÇ\n‚îÇ am   ‚îÜ 1 - 0    ‚îÜ 4.03     ‚îÜ 3.24      ‚îÜ ‚Ä¶ ‚îÜ 0.226   ‚îÜ 2.15  ‚îÜ -2.66   ‚îÜ 10.7    ‚îÇ\n‚îÇ am   ‚îÜ 1 - 0    ‚îÜ -0.237   ‚îÜ 1.59      ‚îÜ ‚Ä¶ ‚îÜ 0.883   ‚îÜ 0.18  ‚îÜ -3.51   ‚îÜ 3.04    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\n\n\n\n\nThe comparisons() function allows customized queries. For example, what happens to the predicted outcome when the hp variable increases from 100 to 120?\n\n\nR\nPython\n\n\n\n\ncomparisons(mod, variables = list(hp = c(120, 100)))\n\n\n Term  Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 %  97.5 %\n   hp 120 - 100   -0.738      0.370 -1.995  0.04607 4.4 -1.463 -0.0129\n   hp 120 - 100   -0.574      0.313 -1.836  0.06640 3.9 -1.186  0.0388\n   hp 120 - 100   -0.931      0.452 -2.062  0.03922 4.7 -1.817 -0.0460\n   hp 120 - 100   -0.845      0.266 -3.182  0.00146 9.4 -1.366 -0.3248\n   hp 120 - 100   -0.780      0.268 -2.909  0.00362 8.1 -1.306 -0.2547\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp 120 - 100   -1.451      0.705 -2.058  0.03958 4.7 -2.834 -0.0692\n   hp 120 - 100   -0.384      0.270 -1.422  0.15498 2.7 -0.912  0.1451\n   hp 120 - 100   -0.641      0.334 -1.918  0.05513 4.2 -1.297  0.0141\n   hp 120 - 100   -0.126      0.272 -0.463  0.64360 0.6 -0.659  0.4075\n   hp 120 - 100   -0.635      0.332 -1.911  0.05598 4.2 -1.286  0.0162\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response \n\n\n\n\n\ncmp = comparisons(mod, variables = {\"hp\": [120, 100]})\nprint(cmp)\n\nshape: (32, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast  ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%      ‚îÜ 97.5% ‚îÇ\n‚îÇ ---  ‚îÜ ---       ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ ---   ‚îÜ ---       ‚îÜ ---   ‚îÇ\n‚îÇ str  ‚îÜ str       ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str   ‚îÜ str       ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.738    ‚îÜ 0.37      ‚îÜ ‚Ä¶ ‚îÜ 0.0576  ‚îÜ 4.12  ‚îÜ -0.0256   ‚îÜ 1.5   ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.574    ‚îÜ 0.313     ‚îÜ ‚Ä¶ ‚îÜ 0.0788  ‚îÜ 3.67  ‚îÜ -0.0713   ‚îÜ 1.22  ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.931    ‚îÜ 0.452     ‚îÜ ‚Ä¶ ‚îÜ 0.0502  ‚îÜ 4.32  ‚îÜ -0.000918 ‚îÜ 1.86  ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.845    ‚îÜ 0.266     ‚îÜ ‚Ä¶ ‚îÜ 0.00401 ‚îÜ 7.96  ‚îÜ 0.297     ‚îÜ 1.39  ‚îÇ\n‚îÇ ‚Ä¶    ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶     ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.384    ‚îÜ 0.27      ‚îÜ ‚Ä¶ ‚îÜ 0.168   ‚îÜ 2.57  ‚îÜ -0.173    ‚îÜ 0.941 ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.641    ‚îÜ 0.334     ‚îÜ ‚Ä¶ ‚îÜ 0.0671  ‚îÜ 3.9   ‚îÜ -0.0488   ‚îÜ 1.33  ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.126    ‚îÜ 0.272     ‚îÜ ‚Ä¶ ‚îÜ 0.648   ‚îÜ 0.626 ‚îÜ -0.436    ‚îÜ 0.688 ‚îÇ\n‚îÇ hp   ‚îÜ 100 - 120 ‚îÜ 0.635    ‚îÜ 0.332     ‚îÜ ‚Ä¶ ‚îÜ 0.068   ‚îÜ 3.88  ‚îÜ -0.0507   ‚îÜ 1.32  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\n\n\n\n\nWhat happens to the predicted outcome when the wt variable increases by 1 standard deviation about its mean?\n\n\nR\nPython\n\n\n\n\ncomparisons(mod, variables = list(hp = \"sd\"))\n\n\n Term                Contrast Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %  97.5 %\n   hp (x + sd/2) - (x - sd/2)   -2.530      1.269 -1.995  0.04607 4.4 -5.02 -0.0441\n   hp (x + sd/2) - (x - sd/2)   -1.967      1.072 -1.836  0.06640 3.9 -4.07  0.1332\n   hp (x + sd/2) - (x - sd/2)   -3.193      1.549 -2.062  0.03922 4.7 -6.23 -0.1578\n   hp (x + sd/2) - (x - sd/2)   -2.898      0.911 -3.182  0.00146 9.4 -4.68 -1.1133\n   hp (x + sd/2) - (x - sd/2)   -2.675      0.919 -2.909  0.00362 8.1 -4.48 -0.8731\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp (x + sd/2) - (x - sd/2)   -4.976      2.418 -2.058  0.03958 4.7 -9.71 -0.2373\n   hp (x + sd/2) - (x - sd/2)   -1.315      0.925 -1.422  0.15498 2.7 -3.13  0.4974\n   hp (x + sd/2) - (x - sd/2)   -2.199      1.147 -1.918  0.05513 4.2 -4.45  0.0483\n   hp (x + sd/2) - (x - sd/2)   -0.432      0.933 -0.463  0.64360 0.6 -2.26  1.3970\n   hp (x + sd/2) - (x - sd/2)   -2.177      1.139 -1.911  0.05598 4.2 -4.41  0.0556\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response \n\n\n\n\n\ncmp = comparisons(mod, variables = {\"hp\": \"sd\"})\nprint(cmp)\n\nshape: (32, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast           ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%  ‚îÜ 97.5%   ‚îÇ\n‚îÇ ---  ‚îÜ ---                ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ ---   ‚îÜ ---   ‚îÜ ---     ‚îÇ\n‚îÇ str  ‚îÜ str                ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str   ‚îÜ str   ‚îÜ str     ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -2.53    ‚îÜ 1.27      ‚îÜ ‚Ä¶ ‚îÜ 0.0576  ‚îÜ 4.12  ‚îÜ -5.15 ‚îÜ 0.0878  ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -1.97    ‚îÜ 1.07      ‚îÜ ‚Ä¶ ‚îÜ 0.0788  ‚îÜ 3.67  ‚îÜ -4.18 ‚îÜ 0.245   ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -3.19    ‚îÜ 1.55      ‚îÜ ‚Ä¶ ‚îÜ 0.0502  ‚îÜ 4.32  ‚îÜ -6.39 ‚îÜ 0.00315 ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -2.9     ‚îÜ 0.911     ‚îÜ ‚Ä¶ ‚îÜ 0.00401 ‚îÜ 7.96  ‚îÜ -4.78 ‚îÜ -1.02   ‚îÇ\n‚îÇ ‚Ä¶    ‚îÜ ‚Ä¶                  ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶       ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -1.32    ‚îÜ 0.925     ‚îÜ ‚Ä¶ ‚îÜ 0.168   ‚îÜ 2.57  ‚îÜ -3.22 ‚îÜ 0.594   ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -2.2     ‚îÜ 1.15      ‚îÜ ‚Ä¶ ‚îÜ 0.0671  ‚îÜ 3.9   ‚îÜ -4.57 ‚îÜ 0.167   ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -0.432   ‚îÜ 0.933     ‚îÜ ‚Ä¶ ‚îÜ 0.648   ‚îÜ 0.626 ‚îÜ -2.36 ‚îÜ 1.49    ‚îÇ\n‚îÇ hp   ‚îÜ +68.56286848932059 ‚îÜ -2.18    ‚îÜ 1.14      ‚îÜ ‚Ä¶ ‚îÜ 0.068   ‚îÜ 3.88  ‚îÜ -4.53 ‚îÜ 0.174   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\n\n\n\n\nThe comparisons() function also allows users to specify arbitrary functions of predictions, with the comparison argument. For example, what is the average ratio between predicted Miles per Gallon after an increase of 50 units in Horsepower?\n\n\nR\nPython\n\n\n\n\ncomparisons(\n  mod,\n  variables = list(hp = 50),\n  comparison = \"ratioavg\")\n\n\n Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n   hp mean(+50)     0.91     0.0291 31.3   &lt;0.001 711.9 0.853  0.966\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\n\n\ncmp = comparisons(\n  mod,\n  variables = {\"hp\": 50},\n  comparison = \"ratioavg\")\nprint(cmp)\n\nshape: (1, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---  ‚îÜ ---      ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str  ‚îÜ str      ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ hp   ‚îÜ +50      ‚îÜ 0.91     ‚îÜ 0.0291    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ inf ‚îÜ 0.85 ‚îÜ 0.97  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nSee the Comparisons vignette for detailed explanations and more options.\nThe slopes() function allows us to compute the partial derivative of the outcome equation with respect to each of the predictors. Once again, we obtain a data frame with 96 rows:\n\n\nR\nPython\n\n\n\n\nmfx &lt;- slopes(mod)\n\nnrow(mfx)\n\n[1] 96\n\nmfx\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   am    1 - 0    0.325       1.68  0.193   0.8467 0.2  -2.97  3.622\n   am    1 - 0   -0.544       1.57 -0.347   0.7287 0.5  -3.62  2.530\n   am    1 - 0    1.201       2.35  0.511   0.6090 0.7  -3.40  5.802\n   am    1 - 0   -1.703       1.87 -0.912   0.3618 1.5  -5.36  1.957\n   am    1 - 0   -0.615       1.68 -0.366   0.7146 0.5  -3.91  2.680\n--- 86 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n   wt    dY/dX   -6.518       1.88 -3.462   &lt;0.001 10.9 -10.21 -2.827\n   wt    dY/dX   -1.653       3.74 -0.442   0.6588  0.6  -8.99  5.683\n   wt    dY/dX   -4.520       2.47 -1.830   0.0673  3.9  -9.36  0.321\n   wt    dY/dX    0.635       4.89  0.130   0.8966  0.2  -8.95 10.215\n   wt    dY/dX   -6.647       1.86 -3.571   &lt;0.001 11.5 -10.29 -2.999\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response \n\n\n\n\n\nmfx = slopes(mod)\n\nmfx.shape\n\n(96, 25)\n\nprint(mfx)\n\nshape: (96, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast ‚îÜ Estimate  ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|)  ‚îÜ S     ‚îÜ 2.5%      ‚îÜ 97.5%    ‚îÇ\n‚îÇ ---  ‚îÜ ---      ‚îÜ ---       ‚îÜ ---       ‚îÜ   ‚îÜ ---      ‚îÜ ---   ‚îÜ ---       ‚îÜ ---      ‚îÇ\n‚îÇ str  ‚îÜ str      ‚îÜ str       ‚îÜ str       ‚îÜ   ‚îÜ str      ‚îÜ str   ‚îÜ str       ‚îÜ str      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -6.61     ‚îÜ 1.87      ‚îÜ ‚Ä¶ ‚îÜ 0.00164  ‚îÜ 9.25  ‚îÜ -10.5     ‚îÜ -2.76    ‚îÇ\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -6.61     ‚îÜ 1.87      ‚îÜ ‚Ä¶ ‚îÜ 0.00165  ‚îÜ 9.25  ‚îÜ -10.5     ‚îÜ -2.76    ‚îÇ\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -7.16     ‚îÜ 1.8       ‚îÜ ‚Ä¶ ‚îÜ 0.000558 ‚îÜ 10.8  ‚îÜ -10.9     ‚îÜ -3.45    ‚îÇ\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -3.21     ‚îÜ 2.01      ‚îÜ ‚Ä¶ ‚îÜ 0.123    ‚îÜ 3.02  ‚îÜ -7.35     ‚îÜ 0.939    ‚îÇ\n‚îÇ ‚Ä¶    ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶        ‚îÇ\n‚îÇ am   ‚îÜ dY/dX    ‚îÜ 2.11e+04  ‚îÜ 2.29e+04  ‚îÜ ‚Ä¶ ‚îÜ 0.367    ‚îÜ 1.45  ‚îÜ -2.62e+04 ‚îÜ 6.83e+04 ‚îÇ\n‚îÇ am   ‚îÜ dY/dX    ‚îÜ 8.95e+03  ‚îÜ 1.64e+04  ‚îÜ ‚Ä¶ ‚îÜ 0.591    ‚îÜ 0.758 ‚îÜ -2.5e+04  ‚îÜ 4.29e+04 ‚îÇ\n‚îÇ am   ‚îÜ dY/dX    ‚îÜ 4.03e+04  ‚îÜ 3.24e+04  ‚îÜ ‚Ä¶ ‚îÜ 0.226    ‚îÜ 2.15  ‚îÜ -2.66e+04 ‚îÜ 1.07e+05 ‚îÇ\n‚îÇ am   ‚îÜ dY/dX    ‚îÜ -2.37e+03 ‚îÜ 1.59e+04  ‚îÜ ‚Ä¶ ‚îÜ 0.883    ‚îÜ 0.18  ‚îÜ -3.51e+04 ‚îÜ 3.04e+04 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#grid",
    "href": "articles/marginaleffects.html#grid",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.3 Grid",
    "text": "1.3 Grid\nPredictions, comparisons, and slopes are typically ‚Äúconditional‚Äù quantities which depend on the values of all the predictors in the model. By default, marginaleffects functions estimate quantities of interest for the empirical distribution of the data (i.e., for each row of the original dataset). However, users can specify the exact values of the predictors they want to investigate by using the newdata argument.\nnewdata accepts data frames, shortcut strings, or a call to the datagrid() function. For example, to compute the predicted outcome for a hypothetical car with all predictors equal to the sample mean or median, we can do:\n\n\nR\nPython\n\n\n\n\npredictions(mod, newdata = \"mean\")\n\n\n Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp   wt    am\n     18.4       0.68 27   &lt;0.001 531.7    17   19.7 147 3.22 0.406\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response \n\npredictions(mod, newdata = \"median\")\n\n\n Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp   wt am\n     19.4      0.646 30   &lt;0.001 653.2  18.1   20.6 123 3.33  0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response \n\n\n\n\n\np = predictions(mod, newdata = \"mean\")\nprint(p)\n\nshape: (1, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Estimate ‚îÜ Std.Error ‚îÜ z   ‚îÜ P(&gt;|z|) ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---      ‚îÜ ---       ‚îÜ --- ‚îÜ ---     ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str      ‚îÜ str       ‚îÜ str ‚îÜ str     ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 18.4     ‚îÜ 0.68      ‚îÜ 27  ‚îÜ 0       ‚îÜ inf ‚îÜ 17   ‚îÜ 19.8  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\np = predictions(mod, newdata = \"median\")\nprint(p)\n\nshape: (1, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Estimate ‚îÜ Std.Error ‚îÜ z   ‚îÜ P(&gt;|z|) ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---      ‚îÜ ---       ‚îÜ --- ‚îÜ ---     ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str      ‚îÜ str       ‚îÜ str ‚îÜ str     ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 19.4     ‚îÜ 0.646     ‚îÜ 30  ‚îÜ 0       ‚îÜ inf ‚îÜ 18   ‚îÜ 20.7  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\n\n\n\n\nThe datagrid function gives us a powerful way to define a grid of predictors. All the variables not mentioned explicitly in datagrid() are fixed to their mean or mode:\n\n\nR\nPython\n\n\n\n\npredictions(\n  mod,\n  newdata = datagrid(\n    am = c(0, 1),\n    wt = range))\n\n\n am   wt Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %  hp\n  0 1.51     23.3       2.71 8.60   &lt;0.001 56.7 17.96   28.6 147\n  0 5.42     12.8       2.98 4.30   &lt;0.001 15.8  6.96   18.6 147\n  1 1.51     27.1       2.85 9.52   &lt;0.001 69.0 21.56   32.7 147\n  1 5.42      5.9       5.81 1.01     0.31  1.7 -5.50   17.3 147\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt \nType:  response \n\n\n\n\n\np = predictions(\n  mod,\n  newdata = datagrid(\n    mod,\n    am = [0, 1],\n    wt = [mtcars[\"wt\"].min(), mtcars[\"wt\"].max()]))\nprint(p)\n\nshape: (4, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Estimate ‚îÜ Std.Error ‚îÜ z    ‚îÜ P(&gt;|z|)  ‚îÜ S    ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---      ‚îÜ ---       ‚îÜ ---  ‚îÜ ---      ‚îÜ ---  ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str      ‚îÜ str       ‚îÜ str  ‚îÜ str      ‚îÜ str  ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 23.3     ‚îÜ 2.71      ‚îÜ 8.6  ‚îÜ 8.65e-09 ‚îÜ 26.8 ‚îÜ 17.7 ‚îÜ 28.8  ‚îÇ\n‚îÇ 12.8     ‚îÜ 2.98      ‚îÜ 4.3  ‚îÜ 0.000249 ‚îÜ 12   ‚îÜ 6.65 ‚îÜ 18.9  ‚îÇ\n‚îÇ 27.1     ‚îÜ 2.85      ‚îÜ 9.52 ‚îÜ 1.27e-09 ‚îÜ 29.5 ‚îÜ 21.3 ‚îÜ 33    ‚îÇ\n‚îÇ 5.9      ‚îÜ 5.81      ‚îÜ 1.01 ‚îÜ 0.32     ‚îÜ 1.64 ‚îÜ -6.1 ‚îÜ 17.9  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, am, wt, rownames, mpg, cyl, disp, hp, drat, qsec, vs, gear, carb\n\n\n\n\n\nThe same mechanism is available in comparisons() and slopes(). To estimate the partial derivative of mpg with respect to wt, when am is equal to 0 and 1, while other predictors are held at their means:\n\n\nR\nPython\n\n\n\n\nslopes(\n  mod,\n  variables = \"wt\",\n  newdata = datagrid(am = 0:1))\n\n\n Term am Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n   wt  0    -2.68       1.42 -1.89   0.0593 4.1 -5.46  0.106\n   wt  1    -5.43       2.15 -2.52   0.0116 6.4 -9.65 -1.214\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, predicted_lo, predicted_hi, predicted, mpg, hp, wt \nType:  response \n\n\n\n\n\ns = slopes(\n  mod,\n  variables = \"wt\",\n  newdata = datagrid(mod, am = [0, 1]))\nprint(s)\n\nshape: (2, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S    ‚îÜ 2.5%  ‚îÜ 97.5%  ‚îÇ\n‚îÇ ---  ‚îÜ ---      ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ ---  ‚îÜ ---   ‚îÜ ---    ‚îÇ\n‚îÇ str  ‚îÜ str      ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str  ‚îÜ str   ‚îÜ str    ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -2.68    ‚îÜ 1.42      ‚îÜ ‚Ä¶ ‚îÜ 0.072   ‚îÜ 3.8  ‚îÜ -5.61 ‚îÜ 0.258  ‚îÇ\n‚îÇ wt   ‚îÜ dY/dX    ‚îÜ -5.43    ‚îÜ 2.15      ‚îÜ ‚Ä¶ ‚îÜ 0.0186  ‚îÜ 5.75 ‚îÜ -9.87 ‚îÜ -0.993 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, am, rownames, mpg, cyl, disp, hp, drat, wt, qsec, vs, gear, carb\n\n\n\n\n\nWe can also plot how predictions, comparisons, or slopes change across different values of the predictors using three powerful plotting functions:\n\n\nplot_predictions: Conditional Adjusted Predictions\n\nplot_comparisons: Conditional Comparisons\n\nplot_slopes: Conditional Marginal Effects\n\nFor example, this plot shows the outcomes predicted by our model for different values of the wt and am variables:\n\nplot_predictions(mod, condition = list(\"hp\", \"wt\" = \"threenum\", \"am\"))\n\n\n\n\nThis plot shows how the derivative of mpg with respect to am varies as a function of wt and hp:\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"wt\" = \"minmax\"))\n\n\n\n\nSee this vignette for more information: Plots, interactions, predictions, contrasts, and slopes",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#averaging",
    "href": "articles/marginaleffects.html#averaging",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.4 Averaging",
    "text": "1.4 Averaging\nSince predictions, comparisons, and slopes are conditional quantities, they can be a bit unwieldy. Often, it can be useful to report a one-number summary instead of one estimate per observation. Instead of presenting ‚Äúconditional‚Äù estimates, some methodologists recommend reporting ‚Äúmarginal‚Äù estimates, that is, an average of unit-level estimates.\n(This use of the word ‚Äúmarginal‚Äù as ‚Äúaveraging‚Äù should not be confused with the term ‚Äúmarginal effect‚Äù which, in the econometrics tradition, corresponds to a partial derivative, or the effect of a ‚Äúsmall/marginal‚Äù change.)\nTo marginalize (average over) our unit-level estimates, we can use the by argument or the one of the convenience functions: avg_predictions(), avg_comparisons(), or avg_slopes(). For example, both of these commands give us the same result: the average predicted outcome in the mtcars dataset:\n\n\nR\nPython\n\n\n\n\navg_predictions(mod)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n     20.1       0.39 51.5   &lt;0.001 Inf  19.3   20.9\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\np = avg_predictions(mod)\nprint(p)\n\nshape: (1, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Estimate ‚îÜ Std.Error ‚îÜ z    ‚îÜ P(&gt;|z|) ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---      ‚îÜ ---       ‚îÜ ---  ‚îÜ ---     ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str      ‚îÜ str       ‚îÜ str  ‚îÜ str     ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 20.1     ‚îÜ 0.39      ‚îÜ 51.5 ‚îÜ 0       ‚îÜ inf ‚îÜ 19.3 ‚îÜ 20.9  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nThis is equivalent to manual computation by:\n\n\nR\nPython\n\n\n\n\nmean(predict(mod))\n\n[1] 20.09062\n\n\n\n\n\nnp.mean(mod.predict())\n\n20.090624999999992\n\n\n\n\n\nThe main marginaleffects functions all include a by argument, which allows us to marginalize within sub-groups of the data. For example,\n\n\nR\nPython\n\n\n\n\navg_comparisons(mod, by = \"am\")\n\n\n Term          Contrast am Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 %   97.5 %\n   am mean(1) - mean(0)  0  -1.3830     2.5250 -0.548  0.58388 0.8 -6.3319  3.56589\n   am mean(1) - mean(0)  1   1.9029     2.3086  0.824  0.40980 1.3 -2.6219  6.42773\n   hp mean(+1)           0  -0.0343     0.0159 -2.160  0.03079 5.0 -0.0654 -0.00317\n   hp mean(+1)           1  -0.0436     0.0213 -2.050  0.04039 4.6 -0.0854 -0.00191\n   wt mean(+1)           0  -2.4799     1.2316 -2.014  0.04406 4.5 -4.8939 -0.06595\n   wt mean(+1)           1  -6.0718     1.9762 -3.072  0.00212 8.9 -9.9451 -2.19846\n\nColumns: term, contrast, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\n\n\ncmp = avg_comparisons(mod, by = \"am\")\nprint(cmp)\n\nshape: (6, 10)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ am  ‚îÜ Term ‚îÜ Contrast          ‚îÜ Estimate ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%    ‚îÜ 97.5%    ‚îÇ\n‚îÇ --- ‚îÜ ---  ‚îÜ ---               ‚îÜ ---      ‚îÜ   ‚îÜ ---     ‚îÜ ---   ‚îÜ ---     ‚îÜ ---      ‚îÇ\n‚îÇ str ‚îÜ str  ‚îÜ str               ‚îÜ str      ‚îÜ   ‚îÜ str     ‚îÜ str   ‚îÜ str     ‚îÜ str      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1   ‚îÜ hp   ‚îÜ +1                ‚îÜ -0.0436  ‚îÜ ‚Ä¶ ‚îÜ 0.0515  ‚îÜ 4.28  ‚îÜ -0.0876 ‚îÜ 0.000301 ‚îÇ\n‚îÇ 0   ‚îÜ hp   ‚îÜ +1                ‚îÜ -0.0343  ‚îÜ ‚Ä¶ ‚îÜ 0.041   ‚îÜ 4.61  ‚îÜ -0.067  ‚îÜ -0.00152 ‚îÇ\n‚îÇ 1   ‚îÜ wt   ‚îÜ +1                ‚îÜ -6.07    ‚îÜ ‚Ä¶ ‚îÜ 0.00522 ‚îÜ 7.58  ‚îÜ -10.2   ‚îÜ -1.99    ‚îÇ\n‚îÇ 0   ‚îÜ wt   ‚îÜ +1                ‚îÜ -2.48    ‚îÜ ‚Ä¶ ‚îÜ 0.0554  ‚îÜ 4.17  ‚îÜ -5.02   ‚îÜ 0.0621   ‚îÇ\n‚îÇ 1   ‚îÜ am   ‚îÜ mean(1) - mean(0) ‚îÜ 1.9      ‚îÜ ‚Ä¶ ‚îÜ 0.418   ‚îÜ 1.26  ‚îÜ -2.86   ‚îÜ 6.67     ‚îÇ\n‚îÇ 0   ‚îÜ am   ‚îÜ mean(1) - mean(0) ‚îÜ -1.38    ‚îÜ ‚Ä¶ ‚îÜ 0.589   ‚îÜ 0.764 ‚îÜ -6.59   ‚îÜ 3.83     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: am, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nMarginal Means are a special case of predictions, which are marginalized (or averaged) across a balanced grid of categorical predictors. To illustrate, we estimate a new model with categorical predictors:\n\n\nR\nPython\n\n\n\n\ndat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$cyl &lt;- as.factor(dat$cyl)\nmod_cat &lt;- lm(mpg ~ am + cyl + hp, data = dat)\n\n\n\n\ndat = mtcars \\\n  .with_columns(pl.col(\"am\").cast(pl.Boolean),\n                pl.col(\"cyl\").cast(pl.Utf8))\nmod_cat = smf.ols('mpg ~ am + cyl + hp', data=dat).fit()\n\n\n\n\nWe can compute marginal means manually using the functions already described:\n\n\nR\nPython\n\n\n\n\navg_predictions(\n  mod_cat,\n  newdata = datagrid(cyl = unique, am = unique),\n  by = \"am\")\n\n\n    am Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n FALSE     18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n  TRUE     22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\np = avg_predictions(\n  mod_cat,\n  newdata = datagrid(mod_cat, cyl = dat[\"cyl\"].unique(), am = dat[\"am\"].unique()),\n  by = \"am\")\nprint(p)\n\nshape: (2, 8)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ am    ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ z    ‚îÜ P(&gt;|z|) ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---   ‚îÜ ---      ‚îÜ ---       ‚îÜ ---  ‚îÜ ---     ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ bool  ‚îÜ str      ‚îÜ str       ‚îÜ str  ‚îÜ str     ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ true  ‚îÜ 22.5     ‚îÜ 0.834     ‚îÜ 26.9 ‚îÜ 0       ‚îÜ inf ‚îÜ 20.8 ‚îÜ 24.2  ‚îÇ\n‚îÇ false ‚îÜ 18.3     ‚îÜ 0.785     ‚îÜ 23.3 ‚îÜ 0       ‚îÜ inf ‚îÜ 16.7 ‚îÜ 19.9  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: am, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nFor convenience, the marginaleffects package for R also includes a marginal_means() function:\n\nmarginal_means(mod_cat, variables = \"am\")\n\n\n Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n   am FALSE 18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n   am  TRUE 22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n\nResults averaged over levels of: cyl, am \nColumns: term, value, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nThe Marginal Means vignette offers more detail.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#hypothesis-and-equivalence-tests",
    "href": "articles/marginaleffects.html#hypothesis-and-equivalence-tests",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.5 Hypothesis and equivalence tests",
    "text": "1.5 Hypothesis and equivalence tests\nThe hypotheses() function and the hypothesis argument can be used to conduct linear and non-linear hypothesis tests on model coefficients, or on any of the quantities computed by the functions introduced above.\nConsider this model:\n\n\nR\nPython\n\n\n\n\nmod &lt;- lm(mpg ~ qsec * drat, data = mtcars)\ncoef(mod)\n\n(Intercept)        qsec        drat   qsec:drat \n 12.3371987  -1.0241183  -3.4371461   0.5973153 \n\n\n\n\n\nmod = smf.ols('mpg ~ qsec * drat', data=mtcars).fit()\nprint(mod.params)\n\nIntercept    12.337199\nqsec         -1.024118\ndrat         -3.437146\nqsec:drat     0.597315\ndtype: float64\n\n\n\n\n\nCan we reject the null hypothesis that the drat coefficient is 2 times the size of the qsec coefficient?\n\nhypotheses(mod, \"drat = 2 * qsec\")\n\n\n            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n drat = 2 * qsec    -1.39       10.8 -0.129    0.897 0.2 -22.5   19.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nWe can ask the same question but refer to parameters by position, with indices b1, b2, b3, etc.:\n\n\nR\nPython\n\n\n\n\nhypotheses(mod, \"b3 = 2 * b2\")\n\n\n        Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b3 = 2 * b2    -1.39       10.8 -0.129    0.897 0.2 -22.5   19.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\n\n\n\nh = hypotheses(mod, \"b3 = 2 * b2\")\nprint(h)\n\nshape: (1, 8)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term    ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ z      ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%  ‚îÜ 97.5% ‚îÇ\n‚îÇ ---     ‚îÜ ---      ‚îÜ ---       ‚îÜ ---    ‚îÜ ---     ‚îÜ ---   ‚îÜ ---   ‚îÜ ---   ‚îÇ\n‚îÇ str     ‚îÜ str      ‚îÜ str       ‚îÜ str    ‚îÜ str     ‚îÜ str   ‚îÜ str   ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ b3=2*b2 ‚îÜ -1.39    ‚îÜ 10.8      ‚îÜ -0.129 ‚îÜ 0.898   ‚îÜ 0.155 ‚îÜ -23.5 ‚îÜ 20.7  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nThe main functions in marginaleffects all have a hypothesis argument, which means that we can do complex model testing. For example, consider two slope estimates:\n\n\nR\nPython\n\n\n\n\nslopes(\n  mod,\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n\n\n Term qsec Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n drat 14.5     5.22       3.80 1.38   0.1690 2.6 -2.221   12.7\n drat 22.9    10.24       5.15 1.99   0.0469 4.4  0.142   20.3\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, qsec, predicted_lo, predicted_hi, predicted, mpg, drat \nType:  response \n\n\n\n\n\ns = slopes(\n  mod,\n  variables = \"drat\",\n  newdata = datagrid(mod, qsec = [mtcars[\"qsec\"].min(), mtcars[\"qsec\"].max()]))\nprint(s)\n\nshape: (2, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|) ‚îÜ S    ‚îÜ 2.5%   ‚îÜ 97.5% ‚îÇ\n‚îÇ ---  ‚îÜ ---      ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---     ‚îÜ ---  ‚îÜ ---    ‚îÜ ---   ‚îÇ\n‚îÇ str  ‚îÜ str      ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str     ‚îÜ str  ‚îÜ str    ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ drat ‚îÜ dY/dX    ‚îÜ 5.22     ‚îÜ 3.8       ‚îÜ ‚Ä¶ ‚îÜ 0.18    ‚îÜ 2.47 ‚îÜ -2.56  ‚îÜ 13    ‚îÇ\n‚îÇ drat ‚îÜ dY/dX    ‚îÜ 10.2     ‚îÜ 5.16      ‚îÜ ‚Ä¶ ‚îÜ 0.0573  ‚îÜ 4.13 ‚îÜ -0.338 ‚îÜ 20.8  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: rowid, term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, predicted, predicted_lo, predicted_hi, qsec, rownames, mpg, cyl, disp, hp, drat, wt, vs, am, gear, carb\n\n\n\n\n\nAre these two slopes significantly different from one another? To test this, we can use the hypothesis argument:\n\n\nR\nPython\n\n\n\n\nslopes(\n  mod,\n  hypothesis = \"b1 = b2\",\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n\n\n  Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b1=b2    -5.02       8.52 -0.589    0.556 0.8 -21.7   11.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\ns = slopes(\n  mod,\n  hypothesis = \"b1 = b2\",\n  variables = \"drat\",\n  newdata = datagrid(mod, qsec = [mtcars[\"qsec\"].min(), mtcars[\"qsec\"].max()]))\nprint(s)\n\nshape: (1, 8)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term  ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ z      ‚îÜ P(&gt;|z|) ‚îÜ S     ‚îÜ 2.5%  ‚îÜ 97.5% ‚îÇ\n‚îÇ ---   ‚îÜ ---      ‚îÜ ---       ‚îÜ ---    ‚îÜ ---     ‚îÜ ---   ‚îÜ ---   ‚îÜ ---   ‚îÇ\n‚îÇ str   ‚îÜ str      ‚îÜ str       ‚îÜ str    ‚îÜ str     ‚îÜ str   ‚îÜ str   ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ b1=b2 ‚îÜ -5.02    ‚îÜ 8.53      ‚îÜ -0.588 ‚îÜ 0.561   ‚îÜ 0.834 ‚îÜ -22.5 ‚îÜ 12.5  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nAlternatively, we can also refer to values with term names (when they are unique):\n\n\nR\nPython\n\n\n\n\navg_slopes(mod)\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n drat     7.22      1.365 5.29  &lt; 0.001 23.0 4.549   9.90\n qsec     1.12      0.433 2.60  0.00942  6.7 0.276   1.97\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_slopes(mod, hypothesis = \"drat = qsec\")\n\n\n      Term Estimate Std. Error   z Pr(&gt;|z|)    S 2.5 % 97.5 %\n drat=qsec      6.1       1.45 4.2   &lt;0.001 15.2  3.25   8.95\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\ns = avg_slopes(mod)\nprint(s)\n\nshape: (2, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast    ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|)  ‚îÜ S    ‚îÜ 2.5%  ‚îÜ 97.5% ‚îÇ\n‚îÇ ---  ‚îÜ ---         ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---      ‚îÜ ---  ‚îÜ ---   ‚îÜ ---   ‚îÇ\n‚îÇ str  ‚îÜ str         ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str      ‚îÜ str  ‚îÜ str   ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ qsec ‚îÜ mean(dY/dX) ‚îÜ 1.12     ‚îÜ 0.432     ‚îÜ ‚Ä¶ ‚îÜ 0.0147   ‚îÜ 6.09 ‚îÜ 0.239 ‚îÜ 2.01  ‚îÇ\n‚îÇ drat ‚îÜ mean(dY/dX) ‚îÜ 7.22     ‚îÜ 1.37      ‚îÜ ‚Ä¶ ‚îÜ 1.25e-05 ‚îÜ 16.3 ‚îÜ 4.43  ‚îÜ 10    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\ns = avg_slopes(mod, hypothesis = \"drat = qsec\")\nprint(s)\n\nshape: (1, 8)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term      ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ z   ‚îÜ P(&gt;|z|)  ‚îÜ S   ‚îÜ 2.5% ‚îÜ 97.5% ‚îÇ\n‚îÇ ---       ‚îÜ ---      ‚îÜ ---       ‚îÜ --- ‚îÜ ---      ‚îÜ --- ‚îÜ ---  ‚îÜ ---   ‚îÇ\n‚îÇ str       ‚îÜ str      ‚îÜ str       ‚îÜ str ‚îÜ str      ‚îÜ str ‚îÜ str  ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ drat=qsec ‚îÜ 6.1      ‚îÜ 1.45      ‚îÜ 4.2 ‚îÜ 0.000245 ‚îÜ 12  ‚îÜ 3.13 ‚îÜ 9.07  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high\n\n\n\n\n\nNow, imagine that for theoretical (or substantive or clinical) reasons, we only care about slopes larger than 2. We can use the equivalence argument to conduct an equivalence test:\n\n\nR\nPython\n\n\n\n\navg_slopes(mod, equivalence = c(-2, 2))\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)\n drat     7.22      1.365 5.29  &lt; 0.001 23.0 4.549   9.90     0.9999     &lt;0.001    0.9999\n qsec     1.12      0.433 2.60  0.00942  6.7 0.276   1.97     0.0215     &lt;0.001    0.0215\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\n\n\n\n\ns = avg_slopes(mod, equivalence = [-2., 2.])\nprint(s)\n\nshape: (2, 9)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Term ‚îÜ Contrast    ‚îÜ Estimate ‚îÜ Std.Error ‚îÜ ‚Ä¶ ‚îÜ P(&gt;|z|)  ‚îÜ S    ‚îÜ 2.5%  ‚îÜ 97.5% ‚îÇ\n‚îÇ ---  ‚îÜ ---         ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ---      ‚îÜ ---  ‚îÜ ---   ‚îÜ ---   ‚îÇ\n‚îÇ str  ‚îÜ str         ‚îÜ str      ‚îÜ str       ‚îÜ   ‚îÜ str      ‚îÜ str  ‚îÜ str   ‚îÜ str   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ drat ‚îÜ mean(dY/dX) ‚îÜ 7.22     ‚îÜ 1.37      ‚îÜ ‚Ä¶ ‚îÜ 1.25e-05 ‚îÜ 16.3 ‚îÜ 4.43  ‚îÜ 10    ‚îÇ\n‚îÇ qsec ‚îÜ mean(dY/dX) ‚îÜ 1.12     ‚îÜ 0.432     ‚îÜ ‚Ä¶ ‚îÜ 0.0147   ‚îÜ 6.09 ‚îÜ 0.239 ‚îÜ 2.01  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nColumns: term, contrast, estimate, std_error, statistic, p_value, s_value, conf_low, conf_high, statistic_noninf, statistic_nonsup, p_value_noninf, p_value_nonsup, p_value_equiv\n\n\n\n\n\nSee the Hypothesis Tests and Custom Contrasts vignette for background, details, and for instructions on how to conduct hypothesis tests in more complex situations.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/marginaleffects.html#more",
    "href": "articles/marginaleffects.html#more",
    "title": "\n1¬† Get Started\n",
    "section": "\n1.6 More!",
    "text": "1.6 More!\nThere is much more you can do with marginaleffects. Click through he Table of Contents to read the vignettes, learn how to report marginal effects in nice tables with the modelsummary package, how to define your own prediction ‚Äúgrid‚Äù, and much more. ****",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Get Started</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#prediction-type-or-scale",
    "href": "articles/predictions.html#prediction-type-or-scale",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.1 Prediction type (or scale)",
    "text": "2.1 Prediction type (or scale)\nUsing the type argument of the predictions() function we can specify the ‚Äúscale‚Äù on which to make predictions. This refers to either the scale used to estimate the model (i.e., link scale) or to a more interpretable scale (e.g., response scale). For example, when fitting a linear regression model using the lm() function, the link scale and the response scale are identical. An ‚ÄúAdjusted Prediction‚Äù computed on either scale will be expressed as the mean value of the response variable at the given values of the predictor variables.\nOn the other hand, when fitting a binary logistic regression model using the glm() function (which uses a binomial family and a logit link ), the link scale and the response scale will be different: an ‚ÄúAdjusted Prediction‚Äù computed on the link scale will be expressed as a log odds of a ‚Äúsuccessful‚Äù response at the given values of the predictor variables, whereas an ‚ÄúAdjusted Prediction‚Äù computed on the response scale will be expressed as a probability that the response variable equals 1.\nThe default value of the type argument for most models is ‚Äúresponse‚Äù, which means that the predictions() function will compute predicted probabilities (binomial family), Poisson means (poisson family), etc.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#prediction-grid",
    "href": "articles/predictions.html#prediction-grid",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.2 Prediction grid",
    "text": "2.2 Prediction grid\nTo compute adjusted predictions we must first specify the values of the predictors to consider: a ‚Äúreference grid.‚Äù For example, if our model is a linear model fitted with the lm() function which relates the response variable Happiness with the predictor variables Age, Gender and Income, the reference grid could be a data.frame with values for Age, Gender and Income: Age = 40, Gender = Male, Income = 60000.\nThe ‚Äúreference grid‚Äù may or may not correspond to actual observations in the dataset used to fit the model; the example values given above could match the mean values of each variable, or they could represent a specific observed (or hypothetical) individual. The reference grid can include many different rows if we want to make predictions for different combinations of predictors. By default, the predictions() function uses the full original dataset as a reference grid, which means it will compute adjusted predictions for each of the individuals observed in the dataset that was used to fit the model.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#the-predictions-function",
    "href": "articles/predictions.html#the-predictions-function",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.3 The predictions() function",
    "text": "2.3 The predictions() function\nBy default, predictions() calculates the regression-adjusted predicted values for every observation in the original dataset:\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\npred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      26.4      0.962 27.5   &lt;0.001 549.0  24.5   28.3\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      15.9      0.992 16.0   &lt;0.001 190.0  14.0   17.9\n#&gt;      20.2      1.219 16.5   &lt;0.001 201.8  17.8   22.5\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nIn many cases, this is too limiting, and researchers will want to specify a grid of ‚Äútypical‚Äù values over which to compute adjusted predictions.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "href": "articles/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.4 Adjusted Predictions at User-Specified values (aka Adjusted Predictions at Representative values, APR)",
    "text": "2.4 Adjusted Predictions at User-Specified values (aka Adjusted Predictions at Representative values, APR)\nThere are two main ways to select the reference grid over which we want to compute adjusted predictions. The first is using the variables argument. The second is with the newdata argument and the datagrid() function\n\n2.4.1 variables: Counterfactual predictions\nThe variables argument is a handy way to create and make predictions on counterfactual datasets. For example, here the dataset that we used to fit the model has 32 rows. The counterfactual dataset with two distinct values of hp has 64 rows: each of the original rows appears twice, that is, once with each of the values that we specified in the variables argument:\n\np &lt;- predictions(mod, variables = list(hp = c(100, 120)))\nhead(p)\n#&gt; \n#&gt;  cyl  hp Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    4 100     26.2      0.986 26.63   &lt;0.001 516.6  24.3   28.2\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    8 100     17.7      1.881  9.42   &lt;0.001  67.6  14.0   21.4\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt; \n#&gt; Columns: rowid, rowidcf, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp \n#&gt; Type:  response\nnrow(p)\n#&gt; [1] 64\n\n\n2.4.2 newdata and datagrid\n\nA second strategy to construct grids of predictors for adjusted predictions is to combine the newdata argument and the datagrid() function. Recall that this function creates a ‚Äútypical‚Äù dataset with all variables at their means or modes, except those we explicitly define:\n\ndatagrid(cyl = c(4, 6, 8), model = mod)\n#&gt;        mpg       hp cyl\n#&gt; 1 20.09062 146.6875   4\n#&gt; 2 20.09062 146.6875   6\n#&gt; 3 20.09062 146.6875   8\n\nWe can also use this datagrid() function in a predictions() call (omitting the model argument):\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp cyl\n#&gt;      16.6       1.28 13   &lt;0.001 125.6  14.1   19.1 147   8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp\n#&gt;    4     25.1       1.37 18.4   &lt;0.001 247.5  22.4   27.8 147\n#&gt;    6     19.2       1.25 15.4   &lt;0.001 174.5  16.7   21.6 147\n#&gt;    8     16.6       1.28 13.0   &lt;0.001 125.6  14.1   19.1 147\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\nUsers can change the summary function used to summarize each type of variables using the FUN_numeric, FUN_factor, and related arguments. For example:\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp drat cyl am\n#&gt;      22.0       1.29 17.0   &lt;0.001 214.0  19.4   24.5 123  3.7   6  1\n#&gt;      18.2       1.27 14.3   &lt;0.001 151.9  15.7   20.7 123  3.7   6  0\n#&gt;      25.5       1.32 19.3   &lt;0.001 274.0  23.0   28.1 123  3.7   4  1\n#&gt;      21.8       1.54 14.1   &lt;0.001 148.3  18.8   24.8 123  3.7   4  0\n#&gt;      22.6       2.14 10.6   &lt;0.001  84.2  18.4   26.8 123  3.7   8  1\n#&gt;      18.9       1.73 10.9   &lt;0.001  89.0  15.5   22.3 123  3.7   8  0\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \n#&gt; Type:  response\n\nThe data.frame produced by predictions() is ‚Äútidy‚Äù, which makes it easy to manipulate with other R packages and functions:\n\nlibrary(kableExtra)\nlibrary(tidyverse)\n\npredictions(\n    mod,\n    newdata = datagrid(cyl = mtcars$cyl, hp = c(100, 110))) |&gt;\n    select(hp, cyl, estimate) |&gt;\n    pivot_wider(values_from = estimate, names_from = cyl) |&gt;\n    kbl(caption = \"A table of Adjusted Predictions\") |&gt;\n    kable_styling() |&gt;\n    add_header_above(header = c(\" \" = 1, \"cyl\" = 3))\n\n\nA table of Adjusted Predictions\n\n\n\n\n\n\n\n\n\n\ncyl\n\n\n\nhp\n4\n6\n8\n\n\n\n\n100\n26.24623\n20.27858\n17.72538\n\n\n110\n26.00585\n20.03819\n17.48500\n\n\n\n\n\n\n2.4.3 counterfactual data grid\nAn alternative approach to construct grids of predictors is to use grid_type = \"counterfactual\" argument value. This will duplicate the whole dataset, with the different values specified by the user.\nFor example, the mtcars dataset has 32 rows. This command produces a new dataset with 64 rows, with each row of the original dataset duplicated with the two values of the am variable supplied (0 and 1):\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\nnd &lt;- datagrid(model = mod, am = 0:1, grid_type = \"counterfactual\")\n\ndim(nd)\n#&gt; [1] 64  4\n\nThen, we can use this dataset and the predictions() function to create interesting visualizations:\n\npred &lt;- predictions(mod, newdata = datagrid(am = 0:1, grid_type = \"counterfactual\")) |&gt;\n    select(am, estimate, rowidcf) |&gt;\n    pivot_wider(id_cols = rowidcf, \n                names_from = am,\n                values_from = estimate)\n\nggplot(pred, aes(x = `0`, y = `1`)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1) +\n    labs(x = \"Predicted Pr(vs=1), when am = 0\",\n         y = \"Predicted Pr(vs=1), when am = 1\")\n\n\n\n\nIn this graph, each dot represents the predicted probability that vs=1 for one observation of the dataset, in the counterfactual worlds where am is either 0 or 1.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#adjusted-prediction-at-the-mean-apm",
    "href": "articles/predictions.html#adjusted-prediction-at-the-mean-apm",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.5 Adjusted Prediction at the Mean (APM)",
    "text": "2.5 Adjusted Prediction at the Mean (APM)\nSome analysts may want to calculate an ‚ÄúAdjusted Prediction at the Mean,‚Äù that is, the predicted outcome when all the regressors are held at their mean (or mode). To achieve this, we use the datagrid() function. By default, this function produces a grid of data with regressors at their means or modes, so all we need to do to get the APM is:\n\npredictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n\nThis is equivalent to calling:\n\npredictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#average-adjusted-predictions-aap",
    "href": "articles/predictions.html#average-adjusted-predictions-aap",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.6 Average Adjusted Predictions (AAP)",
    "text": "2.6 Average Adjusted Predictions (AAP)\nAn ‚ÄúAverage Adjusted Prediction‚Äù is the outcome of a two step process:\n\nCreate a new dataset with each of the original regressor values, but fixing some regressors to values of interest.\nTake the average of the predicted values in this new dataset.\n\nWe can obtain AAPs by applying the avg_*() functions or by argument:\n\nmodlin &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\navg_predictions(modlin)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.1      0.556 36.1   &lt;0.001 946.7    19   21.2\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis is equivalent to:\n\npred &lt;- predictions(modlin)\nmean(pred$estimate)\n#&gt; [1] 20.09062\n\nNote that in GLM models with a non-linear link function, the default type is invlink(link). This means that predictions are first made on the link scale, averaged, and then back transformed. Thus, the average prediction may not be exactly identical to the average of predictions:\n\nmod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\navg_predictions(mod)$estimate\n#&gt; [1] 0.06308965\n\n## Step 1: predict on the link scale\np &lt;- predictions(mod, type = \"link\")$estimate\n## Step 2: average\np &lt;- mean(p)\n## Step 3: backtransform\nmod$family$linkinv(p)\n#&gt; [1] 0.06308965\n\nUsers who want the average of individual-level predictions on the response scale can specify the type argument explicitly:\n\navg_predictions(mod, type = \"response\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;     0.437     0.0429 10.2   &lt;0.001 78.8 0.353  0.522\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#average-adjusted-predictions-by-group",
    "href": "articles/predictions.html#average-adjusted-predictions-by-group",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.7 Average Adjusted Predictions by Group",
    "text": "2.7 Average Adjusted Predictions by Group\nWe can compute average adjusted predictions for different subsets of the data with the by argument.\n\npredictions(mod, by = \"am\")\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;   0   0.0591   0.1163 3.1 0.00198  0.665\n#&gt;   1   0.0694   0.0755 3.7 0.00424  0.566\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\nIn the next example, we create a ‚Äúcounterfactual‚Äù data grid where each observation of the dataset is repeated twice, with different values of the am variable, and all other variables held at the observed values. We also show the equivalent results using dplyr:\n\npredictions(\n    mod,\n    type = \"response\",\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0    0.526     0.0330 15.93   &lt;0.001 187.3 0.461  0.591\n#&gt;   1    0.330     0.0646  5.11   &lt;0.001  21.6 0.204  0.457\n#&gt; \n#&gt; Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    type = \"response\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mean(estimate))\n#&gt; # A tibble: 2 √ó 2\n#&gt;      am   AAP\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     0 0.526\n#&gt; 2     1 0.330\n\nNote that the two results are exactly identical when we specify type=\"response\" explicitly. However, they will differ slightly when we leave type unspecified, because marginaleffects will then automatically make predictions and average on the link scale, before backtransforming (\"invlink(link)\"):\n\npredictions(\n    mod,\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;   0  0.24043   0.3922 1.4 2.22e-02  0.815\n#&gt;   1  0.00696   0.0359 4.8 6.81e-05  0.419\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\npredictions(\n    mod,\n    type = \"link\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mod$family$linkinv(mean(estimate)))\n#&gt; # A tibble: 2 √ó 2\n#&gt;      am     AAP\n#&gt;   &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1     0 0.240  \n#&gt; 2     1 0.00696\n\n\n2.7.1 Multinomial models\nOne place where this is particularly useful is in multinomial models with different response levels. For example, here we compute the average predicted outcome for each outcome level in a multinomial logit model. Note that response levels are identified by the ‚Äúgroup‚Äù column.\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n## first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n#&gt; \n#&gt;  Group Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n#&gt;      3 4.04e-01   1.97e-01   2.0567   0.0397 4.7  1.90e-02 7.90e-01\n#&gt;      3 1.00e+00   1.25e-03 802.4784   &lt;0.001 Inf  9.98e-01 1.00e+00\n#&gt;      3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \n#&gt; Type:  probs\n\n## average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;      3    0.469     0.0404 11.60   &lt;0.001 100.9 0.3895  0.548\n#&gt;      4    0.375     0.0614  6.11   &lt;0.001  29.9 0.2546  0.495\n#&gt;      5    0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can use custom aggregations by supplying a data frame to the by argument. All columns of this data frame must be present in the output of predictions(), and the data frame must also include a by column of labels. In this example, we ‚Äúcollapse‚Äù response groups:\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n#&gt; \n#&gt;   By Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3,4    0.422     0.0231 18.25   &lt;0.001 244.7 0.3766  0.467\n#&gt;  5      0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nThis can be very useful in combination with the hypothesis argument. For example, here we compute the difference between average adjusted predictions for the 3 and 4 response levels, compared to the 5 response level:\n\npredictions(nom, type = \"probs\", by = by, hypothesis = \"sequential\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  5 - 3,4   -0.266     0.0694 -3.83   &lt;0.001 12.9 -0.402  -0.13\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nWe can also use more complicated aggregations. Here, we compute the predicted probability of outcome levels for each value of cyl, by collapsing the ‚Äú3‚Äù and ‚Äú4‚Äù outcome levels:\n\nnom &lt;- multinom(factor(gear) ~ mpg + factor(cyl), data = mtcars, trace = FALSE)\n\nby &lt;- expand.grid(\n    group = 3:5,\n    cyl = c(4, 6, 8),\n    stringsAsFactors = TRUE) |&gt;\n    # define labels\n    transform(by = ifelse(\n        group %in% 3:4,\n        sprintf(\"3/4 Gears & %s Cylinders\", cyl),\n        sprintf(\"5 Gears & %s Cylinders\", cyl)))\n\npredictions(nom, by = by)\n#&gt; \n#&gt;                       By Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders    0.429     0.0661 6.49   &lt;0.001 33.4  0.2991  0.558\n#&gt;  3/4 Gears & 4 Cylinders    0.409     0.0580 7.06   &lt;0.001 39.1  0.2956  0.523\n#&gt;  3/4 Gears & 8 Cylinders    0.429     0.0458 9.35   &lt;0.001 66.6  0.3387  0.518\n#&gt;  5 Gears & 6 Cylinders      0.143     0.1321 1.08    0.280  1.8 -0.1161  0.402\n#&gt;  5 Gears & 4 Cylinders      0.182     0.1159 1.57    0.117  3.1 -0.0457  0.409\n#&gt;  5 Gears & 8 Cylinders      0.143     0.0917 1.56    0.119  3.1 -0.0368  0.323\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n\nAnd we can then compare the different groups using the hypothesis argument:\n\npredictions(nom, by = by, hypothesis = \"pairwise\")\n#&gt; \n#&gt;                                               Term  Estimate Std. Error         z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 4 Cylinders  1.93e-02     0.0879  0.219839   0.8260 0.3 -0.15294  0.192\n#&gt;  3/4 Gears & 6 Cylinders - 3/4 Gears & 8 Cylinders  2.84e-05     0.0804  0.000353   0.9997 0.0 -0.15757  0.158\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1982  1.441538   0.1494 2.7 -0.10275  0.674\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1334  1.851412   0.0641 4.0 -0.01449  0.509\n#&gt;  3/4 Gears & 6 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1130  2.527636   0.0115 6.4  0.06415  0.507\n#&gt;  3/4 Gears & 4 Cylinders - 3/4 Gears & 8 Cylinders -1.93e-02     0.0739 -0.261054   0.7941 0.3 -0.16415  0.126\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 6 Cylinders    2.66e-01     0.1443  1.846188   0.0649 3.9 -0.01642  0.549\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 4 Cylinders    2.28e-01     0.1739  1.309481   0.1904 2.4 -0.11312  0.569\n#&gt;  3/4 Gears & 4 Cylinders - 5 Gears & 8 Cylinders    2.66e-01     0.1085  2.455119   0.0141 6.1  0.05371  0.479\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1399  2.042632   0.0411 4.6  0.01156  0.560\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1247  1.981367   0.0476 4.4  0.00267  0.491\n#&gt;  3/4 Gears & 8 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1375  2.076745   0.0378 4.7  0.01606  0.555\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 4 Cylinders     -3.86e-02     0.1758 -0.219838   0.8260 0.3 -0.38317  0.306\n#&gt;  5 Gears & 6 Cylinders - 5 Gears & 8 Cylinders     -5.68e-05     0.1608 -0.000353   0.9997 0.0 -0.31526  0.315\n#&gt;  5 Gears & 4 Cylinders - 5 Gears & 8 Cylinders      3.86e-02     0.1478  0.261054   0.7941 0.3 -0.25112  0.328\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\n\n2.7.2 Bayesian models\nThe same strategy works for Bayesian models:\n\nlibrary(brms)\nmod &lt;- brm(am ~ mpg * vs, data = mtcars, family = bernoulli)\n\n\npredictions(mod, by = \"vs\")\n#&gt; \n#&gt;  vs Estimate 2.5 % 97.5 %\n#&gt;   0    0.327 0.182  0.507\n#&gt;   1    0.499 0.366  0.672\n#&gt; \n#&gt; Columns: vs, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThe results above show the median of the posterior distribution of group-wise means. Note that we take the mean of predicted values for each MCMC draw before computing quantiles. This is equivalent to:\n\ndraws &lt;- posterior_epred(mod)\nquantile(rowMeans(draws[, mtcars$vs == 0]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.3271836 0.1824479 0.5072074\nquantile(rowMeans(draws[, mtcars$vs == 1]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.4993250 0.3657956 0.6721267",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#conditional-adjusted-predictions-plot",
    "href": "articles/predictions.html#conditional-adjusted-predictions-plot",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.8 Conditional Adjusted Predictions (Plot)",
    "text": "2.8 Conditional Adjusted Predictions (Plot)\nFirst, we download the ggplot2movies dataset from the RDatasets archive. Then, we create a variable called certified_fresh for movies with a rating of at least 8. Finally, we discard some outliers and fit a logistic regression model:\n\nlibrary(tidyverse)\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\") |&gt;\n    mutate(style = case_when(Action == 1 ~ \"Action\",\n                             Comedy == 1 ~ \"Comedy\",\n                             Drama == 1 ~ \"Drama\",\n                             TRUE ~ \"Other\"),\n           style = factor(style),\n           certified_fresh = rating &gt;= 8) |&gt;\n    dplyr::filter(length &lt; 240)\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nWe can plot adjusted predictions, conditional on the length variable using the plot_predictions() function:\n\nmod &lt;- glm(certified_fresh ~ length, data = dat, family = binomial)\n\nplot_predictions(mod, condition = \"length\")\n\n\n\n\nWe can also introduce another condition which will display a categorical variable like style in different colors. This can be useful in models with interactions:\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"length\", \"style\"))\n\n\n\n\nSince the output of plot_predictions() is a ggplot2 object, it is very easy to customize. For example, we can add points for the actual observations of our dataset like so:\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp, data = mt)\n\nplot_predictions(mod, condition = \"hp\") +\n    geom_point(aes(x = hp, y = mpg), data = mt) +\n    geom_rug(aes(x = hp, y = mpg), data = mt) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nWe can also use plot_predictions() in models with multinomial outcomes or grouped coefficients. For example, notice that when we call draw=FALSE, the result includes a group column:\n\nlibrary(MASS)\nlibrary(ggplot2)\n\nmod &lt;- nnet::multinom(factor(gear) ~ mpg, data = mtcars, trace = FALSE)\n\np &lt;- plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\",\n    draw = FALSE)\n\nhead(p)\n#&gt;   rowid group  estimate  std.error statistic       p.value  s.value  conf.low conf.high gear      mpg\n#&gt; 1     1     3 0.9714990 0.03873404  25.08127 7.962555e-139 458.7548 0.8955817  1.047416    3 10.40000\n#&gt; 2     2     3 0.9656724 0.04394086  21.97664 4.818284e-107 353.1778 0.8795499  1.051795    3 10.87959\n#&gt; 3     3     3 0.9586759 0.04964165  19.31193  4.263532e-83 273.6280 0.8613801  1.055972    3 11.35918\n#&gt; 4     4     3 0.9502914 0.05581338  17.02623  5.247849e-65 213.5336 0.8408992  1.059684    3 11.83878\n#&gt; 5     5     3 0.9402691 0.06240449  15.06733  2.656268e-51 168.0089 0.8179586  1.062580    3 12.31837\n#&gt; 6     6     3 0.9283274 0.06932768  13.39043  6.878233e-41 133.4170 0.7924476  1.064207    3 12.79796\n\nNow we use the group column:\n\nplot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\") +\n    facet_wrap(~group)",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/predictions.html#prediction-types",
    "href": "articles/predictions.html#prediction-types",
    "title": "\n2¬† Predictions\n",
    "section": "\n2.9 Prediction types",
    "text": "2.9 Prediction types\nThe predictions() function computes model-adjusted means on the scale of the output of the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the adjusted predictions should be interpreted on that scale. However, users can pass a string to the type argument, and predictions() will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\npred &lt;- predictions(mod, type = \"response\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.598     0.1324 4.52  &lt; 0.001 17.3 0.3384  0.857\n#&gt;     0.492     0.1196 4.11  &lt; 0.001 14.6 0.2573  0.726\n#&gt;     0.297     0.1005 2.95  0.00314  8.3 0.0999  0.494\n#&gt;     0.260     0.0978 2.66  0.00788  7.0 0.0682  0.452\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  response\n\npred &lt;- predictions(mod, type = \"link\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;    0.3967      0.551  0.7204   0.4713 1.1 -0.683  1.4761\n#&gt;   -0.0331      0.479 -0.0692   0.9448 0.1 -0.971  0.9049\n#&gt;   -0.8621      0.482 -1.7903   0.0734 3.8 -1.806  0.0817\n#&gt;   -1.0463      0.509 -2.0575   0.0396 4.7 -2.043 -0.0496\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  link\n\nWe can also plot predictions on different outcome scales:\n\nplot_predictions(mod, condition = \"mpg\", type = \"response\")\n\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"link\")",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#simple-example-titanic",
    "href": "articles/comparisons.html#simple-example-titanic",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.1 Simple example: Titanic",
    "text": "3.1 Simple example: Titanic\nConsider a logistic regression model estimated using the Titanic mortality data:\n\nlibrary(marginaleffects)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ndat$PClass[dat$PClass == \"*\"] &lt;- NA\nmod &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\n\n3.1.1 Step 1: Quantity\nThe question that interests us is:\n\nHow does the probability of survival (outcome) change if a passenger travels in 1st class vs.¬†3rd class?\n\nSince we are comparing two predicted outcomes, we will use comparisons(). To indicate that our focal variable is PClass and that we are interested in the comparison between 1st and 3rd class, we will use the variables argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n\n\n3.1.2 Step 2: Grid\nIn GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above.\nBy default, comparisons() will compute estimates for every row of the original dataset that was used to fit a model. There are 1313 observations in the titanic dataset. Therefore, if we just execute the code in the previous section, we will obtain 1313 estimates of the difference between the probability of survival in 3rd and 1st class:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.496     0.0610 -8.13  &lt; 0.001 51.0 -0.616 -0.376\n#&gt;  PClass 3rd - 1st   -0.472     0.1247 -3.79  &lt; 0.001 12.7 -0.716 -0.228\n#&gt;  PClass 3rd - 1st   -0.353     0.0641 -5.51  &lt; 0.001 24.7 -0.478 -0.227\n#&gt;  PClass 3rd - 1st   -0.493     0.0583 -8.45  &lt; 0.001 55.0 -0.607 -0.379\n#&gt;  PClass 3rd - 1st   -0.445     0.1452 -3.07  0.00216  8.9 -0.730 -0.161\n#&gt; --- 746 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;  PClass 3rd - 1st   -0.377     0.0703 -5.36  &lt; 0.001 23.5 -0.515 -0.239\n#&gt;  PClass 3rd - 1st   -0.384     0.0726 -5.30  &lt; 0.001 23.0 -0.527 -0.242\n#&gt;  PClass 3rd - 1st   -0.412     0.0821 -5.02  &lt; 0.001 20.9 -0.573 -0.251\n#&gt;  PClass 3rd - 1st   -0.399     0.0773 -5.16  &lt; 0.001 22.0 -0.550 -0.247\n#&gt;  PClass 3rd - 1st   -0.361     0.0661 -5.47  &lt; 0.001 24.4 -0.490 -0.232\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Survived, PClass, SexCode, Age \n#&gt; Type:  response\n\nNotice that the contrast between 3rd and 1st is different from row to row. This reflects the fact that, in our model, moving from 1st to 3rd would have a different effect on the predicted probability of survival for different individuals.\nWe can be more specific in our query. Instead of using the empirical distribution as our ‚Äúgrid‚Äù, we can specify exactly where we want to evaluate the comparison in the predictor space, by using the newdata argument and the datagrid() function. For example, say I am interested in:\n\nThe effect of moving from 1st to 3rd class on the probability of survival for a 50 year old man and a 50 year old woman.\n\nI can type:\n\ncmp &lt;- comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\ncmp\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nWe now know that moving from 1st to 3rd changes by -0.184 the probability of survival for 50 year old men (SexCode=0), and by -0.511 the probability of survival for 50 year old women (SexCode=1).\n\n3.1.3 Step 3: Averaging\nAgain, by default comparisons() estimates quantities for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an ‚Äúaverage contrast‚Äù:\n\navg_comparisons(mod,                          # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.396     0.0425 -9.3   &lt;0.001 66.0 -0.479 -0.312\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nAlternatively, we could also take the average, but just of the two estimates that we computed above for the 50 year old man and 50 year old woman.\n\navg_comparisons(mod,                           # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.348     0.0676 -5.15   &lt;0.001 21.8 -0.48 -0.215\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that this is exactly the same as the average in the estimates from the previous section, which we had stored as cmp:\n\ncmp$estimate\n#&gt; [1] -0.1844289 -0.5113098\n\nmean(cmp$estimate)\n#&gt; [1] -0.3478694\n\n\n3.1.4 Hypothesis\nFinally, imagine we are interested in this question:\n\nDoes moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?\n\nTo answer this, we use the hypothesis argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1), # Step 2: Grid\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result maps directly onto the estimates we had above. It is the difference in the contrast for 50-men and 50-women:\n\ndiff(cmp$estimate)\n#&gt; [1] -0.3268809\n\nThis result can be interpreted as a ‚Äúdifference-in-differences‚Äù: Moving from 1st to 3rd has a much larger negative effect on the probability of survival for a 50 year old woman than for a 50 year old man. This difference is statistically significant.\nWe can do a similar comparison, but instead of fixing a conditional grid, we can average over subgroups of the empirical distribution, using the by argument:\n\navg_comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  by = \"SexCode\",                              # Step 3: Average\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  b1=b2    0.162     0.0845 1.91   0.0558 4.2 -0.00402  0.327\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n3.1.5 Manual computation\nNow we show how to use the base R predict() function to compute some of the same quantities as above. This exercise may be clarifying for some users.\n\ngrid_50_1_3 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"3rd\")\ngrid_50_1_1 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"1st\")\ngrid_50_0_3 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"3rd\")\ngrid_50_0_1 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"1st\")\n\n\nyhat_50_1_3 &lt;- predict(mod, newdata = grid_50_1_3, type = \"response\")\nyhat_50_1_1 &lt;- predict(mod, newdata = grid_50_1_1, type = \"response\")\nyhat_50_0_3 &lt;- predict(mod, newdata = grid_50_0_3, type = \"response\")\nyhat_50_0_1 &lt;- predict(mod, newdata = grid_50_0_1, type = \"response\")\n\n## prediction on a grid\npredictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = \"3rd\"))\n#&gt; \n#&gt;  Age SexCode PClass Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   50       1    3rd    0.446    0.661 0.6 0.235  0.679\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, Survived, Age, SexCode, PClass \n#&gt; Type:  invlink(link)\nyhat_50_1_3\n#&gt;         1 \n#&gt; 0.4463379\n\n## contrast on a grid\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1))\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nyhat_50_0_3 - yhat_50_0_1\n#&gt;          1 \n#&gt; -0.1844289\nyhat_50_1_3 - yhat_50_1_1\n#&gt;          1 \n#&gt; -0.5113098\n\n## difference-in-differences \ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1),\n  hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)\n#&gt;         1 \n#&gt; 0.3268809\n\n## average of the empirical distribution of contrasts\navg_comparisons(mod, variables = list(PClass = c(\"1st\", \"3rd\")), by = \"SexCode\")\n#&gt; \n#&gt;    Term              Contrast SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass mean(3rd) - mean(1st)       0   -0.334     0.0570 -5.86   &lt;0.001 27.7 -0.446 -0.222\n#&gt;  PClass mean(3rd) - mean(1st)       1   -0.496     0.0623 -7.95   &lt;0.001 49.0 -0.618 -0.374\n#&gt; \n#&gt; Columns: term, contrast, SexCode, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\ngrid_empirical_1_3 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_1_1 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"1st\")\ngrid_empirical_0_3 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_0_1 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"1st\")\nyhat_empirical_0_1 &lt;- predict(mod, newdata = grid_empirical_0_1, type = \"response\")\nyhat_empirical_0_3 &lt;- predict(mod, newdata = grid_empirical_0_3, type = \"response\")\nyhat_empirical_1_1 &lt;- predict(mod, newdata = grid_empirical_1_1, type = \"response\")\nyhat_empirical_1_3 &lt;- predict(mod, newdata = grid_empirical_1_3, type = \"response\")\nmean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)\n#&gt; [1] -0.3341426\nmean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)\n#&gt; [1] -0.4956673",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#predictor-types",
    "href": "articles/comparisons.html#predictor-types",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.2 Predictor types",
    "text": "3.2 Predictor types\n\n3.2.1 Logical and factor predictors\nConsider a simple model with a logical and a factor variable:\n\nlibrary(marginaleffects)\n\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\n\nThe comparisons() function automatically computes contrasts for each level of the categorical variables, relative to the baseline category (FALSE for logicals, and the reference level for factors), while holding all other values at their observed values. The avg_comparisons() does the same, but then marginalizes by taking the average of unit-level estimates:\n\ncmp &lt;- avg_comparisons(mod)\ncmp\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 % 97.5 %\n#&gt;   am  TRUE - FALSE     2.56       1.30  1.97   0.0485  4.4   0.0167   5.10\n#&gt;   cyl 6 - 4           -6.16       1.54 -4.01   &lt;0.001 14.0  -9.1661  -3.15\n#&gt;   cyl 8 - 4          -10.07       1.45 -6.93   &lt;0.001 37.8 -12.9136  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe summary printed above says that moving from the reference category 4 to the level 6 on the cyl factor variable is associated with a change of -6.156 in the adjusted prediction. Similarly, the contrast from FALSE to TRUE on the am variable is equal to 2.560.\nWe can obtain different contrasts by using the comparisons() function. For example:\n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0 -9.17  -3.15\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93  &lt; 0.001 37.8 -12.91  -7.22\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01   &lt;0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93   &lt;0.001 37.8 -12.91  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nFor comparison, this code produces the same results using the emmeans package:\n\nlibrary(emmeans)\nemm &lt;- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast    estimate   SE df t.ratio p.value\n#&gt;  cyl6 - cyl4    -6.16 1.54 28  -4.009  0.0012\n#&gt;  cyl8 - cyl4   -10.07 1.45 28  -6.933  &lt;.0001\n#&gt;  cyl8 - cyl6    -3.91 1.47 28  -2.660  0.0331\n#&gt; \n#&gt; Results are averaged over the levels of: am \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\nemm &lt;- emmeans(mod, specs = \"am\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast     estimate  SE df t.ratio p.value\n#&gt;  TRUE - FALSE     2.56 1.3 28   1.973  0.0585\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n\nNote that these commands also work on for other types of models, such as GLMs, on different scales:\n\nmod_logit &lt;- glm(am ~ factor(gear), data = mtcars, family = binomial)\n\navg_comparisons(mod_logit)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  gear    4 - 3    0.667   1.36e-01 4.9e+00   &lt;0.001 20.0   0.4  0.933\n#&gt;  gear    5 - 3    1.000   9.95e-06 1.0e+05   &lt;0.001  Inf   1.0  1.000\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod_logit, type = \"link\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  gear    4 - 3     21.3       4578 0.00464    0.996 0.0  -8951   8994\n#&gt;  gear    5 - 3     41.1       9156 0.00449    0.996 0.0 -17904  17986\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n\n\n3.2.1.1 Observation-Wise Categorical Marginal Effect\nFor categorical predictors, Scholbeck et al.¬†2023 recommend that analysts report what they call the ‚Äúobservation-wise categorical marginal effects.‚Äù They describe the procedure as follows:\n\nRecall that the common definition of categorical MEs is based on first changing all observations‚Äô value of \\(x_j\\) to each category and then computing the difference in predictions when changing it to the reference category. However, one is often interested in prediction changes if aspects of an actual observation change. We therefore propose an observation-wise categorical ME. We first select a single reference category \\(c_h\\). For each observation whose feature value \\(x_j \\neq c_h\\), we predict once with the observed value \\(x_j\\) and once where \\(x_j\\) has been replaced by \\(c_h\\).\n\nTo achieve this with marginaleffects, we proceed in three simple steps:\n\nUse the factor() function to set the reference level of the categorical variable.\nUse the newdata argument to take the subset of data where the observed \\(x_j\\) is different from the reference level we picked in 1.\nApply the avg_comparisons() with the \"revreference\" option.\n\n\ndat &lt;- transform(mtcars, cyl = factor(cyl, levels = c(6, 4, 8)))\n\nmod &lt;- glm(vs ~ mpg * factor(cyl), data = dat, family = binomial)\n\navg_comparisons(mod,\n  variables = list(cyl = \"revreference\"),\n  newdata = subset(dat, cyl != 6))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4   -0.323     0.2170 -1.49    0.137   2.9 -0.748  0.103\n#&gt;   cyl    6 - 8    0.561     0.0357 15.69   &lt;0.001 181.9  0.491  0.631\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n3.2.2 Character predictors\nAll functions of the marginaleffects package attempt to treat character predictors as factor predictors. However, using factors instead of characters when modeling is strongly encouraged, because they are much safer and faster. This is because factors hold useful information about the full list of levels, which makes them easier to track and handle internally by marginaleffects. Users are strongly encouraged to convert their character variables to factor before fitting their models and using slopes() functions.\n\n3.2.3 Numeric predictors\nWe can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the hp variable by 1 unit (default) or by 5 units about the original value:\n\nmod &lt;- lm(mpg ~ hp, data = mtcars)\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp       +1  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = 5))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp       +5   -0.341     0.0506 -6.74   &lt;0.001 35.9 -0.44 -0.242\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions for a change in the regressor between two arbitrary values:\n\navg_comparisons(mod, variables = list(hp = c(90, 110)))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp 110 - 90    -1.36      0.202 -6.74   &lt;0.001 35.9 -1.76 -0.968\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nCompare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:\n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp  Q3 - Q1     -5.7      0.845 -6.74   &lt;0.001 35.9 -7.35  -4.04\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n#&gt; \n#&gt;  Term                Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   &lt;0.001 35.9 -6.04  -3.32\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"2sd\"))\n#&gt; \n#&gt;  Term            Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd) - (x - sd)    -9.36       1.39 -6.74   &lt;0.001 35.9 -12.1  -6.64\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n#&gt; \n#&gt;  Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp Max - Min    -19.3       2.86 -6.74   &lt;0.001 35.9 -24.9  -13.7\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#interactions-and-cross-contrasts",
    "href": "articles/comparisons.html#interactions-and-cross-contrasts",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.3 Interactions and Cross-Contrasts",
    "text": "3.3 Interactions and Cross-Contrasts\nIn some contexts we are interested in whether the ‚Äúeffect‚Äù of a variable changes, as a function of another variable. A very simple strategy to tackle this question is to estimate a model with a multiplicative interaction like this one:\n\nmod &lt;- lm(mpg ~ am * factor(cyl), data = mtcars)\n\nCalling avg_comparisons() with the by argument shows that the estimated comparisons differ based on cyl:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\")\n#&gt; \n#&gt;  Term          Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;    am mean(1) - mean(0)   4     5.18       2.05 2.521   0.0117 6.4  1.15   9.20\n#&gt;    am mean(1) - mean(0)   6     1.44       2.32 0.623   0.5336 0.9 -3.10   5.98\n#&gt;    am mean(1) - mean(0)   8     0.35       2.32 0.151   0.8799 0.2 -4.19   4.89\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nHowever, using the hypothesis argument for pairwise contrasts between the above comparisons reveals that the heterogeneity is not statistically significant:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  4 - 6     3.73       3.09 1.206    0.228 2.1 -2.33   9.80\n#&gt;  4 - 8     4.82       3.09 1.559    0.119 3.1 -1.24  10.89\n#&gt;  6 - 8     1.09       3.28 0.333    0.739 0.4 -5.33   7.51\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nIn other contexts, we are interested in a ‚Äúcross-contrast‚Äù or ‚Äúcross-comparisons‚Äù; we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the variables argument, and set the cross=TRUE:\n\navg_comparisons(mod, variables = c(\"cyl\", \"am\"), cross = TRUE)\n#&gt; \n#&gt;  Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 % C: am C: cyl\n#&gt;     -2.33       2.48 -0.942  0.34596 1.5  -7.19   2.52 1 - 0  6 - 4\n#&gt;     -7.50       2.77 -2.709  0.00674 7.2 -12.93  -2.07 1 - 0  8 - 4\n#&gt; \n#&gt; Columns: term, contrast_am, contrast_cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#quantities-of-interest",
    "href": "articles/comparisons.html#quantities-of-interest",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.4 Quantities of interest",
    "text": "3.4 Quantities of interest\nThis section compares 4 quantities:\n\nUnit-Level Contrasts\nAverage Contrast\nContrast at the Mean\nContrast Between Marginal Means\n\nThe ideas discussed in this section focus on contrasts, but they carry over directly to analogous types of marginal effects.\n\n3.4.1 Unit-level contrasts\nIn models with interactions or non-linear components (e.g., link function), the value of a contrast or marginal effect can depend on the value of all the predictors in the model. As a result, contrasts and marginal effects are fundamentally unit-level quantities. The effect of a 1 unit increase in \\(X\\) can be different for Mary or John. Every row of a dataset has a different contrast and marginal effect.\nThe mtcars dataset has 32 rows, so the comparisons() function produces 32 contrast estimates:\n\nlibrary(marginaleffects)\nmod &lt;- glm(vs ~ factor(gear) + mpg, family = binomial, data = mtcars)\ncmp &lt;- comparisons(mod, variables = \"mpg\")\nnrow(cmp)\n#&gt; [1] 32\n\n\n3.4.2 Average contrasts\nBy default, the slopes() and comparisons() functions compute marginal effects and contrasts for every row of the original dataset. These unit-level estimates can be of great interest, as discussed in another vignette. Nevertheless, one may want to focus on one-number summaries: the avg_*() functions or the by argument compute the ‚ÄúAverage Marginal Effect‚Äù or ‚ÄúAverage Contrast,‚Äù by taking the mean of all the unit-level estimates.\n\navg_comparisons(mod, variables = \"mpg\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0608     0.0128 4.74   &lt;0.001 18.8 0.0356  0.086\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, variables = \"mpg\", by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0608     0.0128 4.74   &lt;0.001 18.8 0.0356  0.086\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nwhich are equivalent to:\n\nmean(cmp$estimate)\n#&gt; [1] 0.06080995\n\nWe could also show the full distribution of contrasts across our dataset with a histogram:\n\nlibrary(ggplot2)\n\ncmp &lt;- comparisons(mod, variables = \"gear\")\n\nggplot(cmp, aes(estimate)) +\n    geom_histogram(bins = 30) +\n    facet_wrap(~contrast, scale = \"free_x\") +\n    labs(x = \"Distribution of unit-level contrasts\")\n\n\n\n\nThis graph displays the effect of a change of 1 unit in the gear variable, for each individual in the observed data.\n\n3.4.3 Contrasts at the mean\nAn alternative which used to be very common but has now fallen into a bit of disfavor is to compute ‚ÄúContrasts at the mean.‚Äù The idea is to create a ‚Äúsynthetic‚Äù or ‚Äúhypothetical‚Äù individual (row of the dataset) whose characteristics are completely average. Then, we compute and report the contrast for this specific hypothetical individual.\nThis can be achieved by setting newdata=\"mean\" or to newdata=datagrid(), both of which fix variables to their means or modes:\n\ncomparisons(mod, variables = \"mpg\", newdata = \"mean\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % gear  mpg\n#&gt;   mpg       +1    0.166     0.0629 2.65  0.00811 6.9 0.0432   0.29    3 20.1\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, gear, mpg \n#&gt; Type:  response\n\nContrasts at the mean can differ substantially from average contrasts.\nThe advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.\n\n3.4.4 Contrasts between marginal means\nYet another type of contrast is the ‚ÄúContrast between marginal means.‚Äù This type of contrast is closely related to the ‚ÄúContrast at the mean‚Äù, with a few wrinkles. It is the default approach used by the emmeans package for R.\nRoughly speaking, the procedure is as follows:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nThe contrast obtained through this approach has two critical characteristics:\n\nIt is the contrast for a synthetic individual with perfectly average qualities on every (numeric) predictor.\nIt is a weighted average of unit-level contrasts, where weights assume a perfectly balanced dataset across every categorical predictor.\n\nWith respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted as if the data were perfectly balanced?\nFor example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.\nIf the contrasts between marginal means is really the quantity of interest, it is easy to use the comparisons() to estimate contrasts between marginal means. The newdata determines the values of the predictors at which we want to compute contrasts. We can set newdata=\"marginalmeans\" to emulate the emmeans behavior. For example, here we compute contrasts in a model with an interaction:\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\nmod &lt;- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)\n\navg_comparisons(\n    mod,\n    newdata = \"marginalmeans\",\n    variables = c(\"species\", \"island\"))\n#&gt; \n#&gt;     Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  island  Dream - Biscoe       -0.434      0.451 -0.962    0.336   1.6 -1.318  0.450\n#&gt;  island  Torgersen - Biscoe    0.060      0.467  0.128    0.898   0.2 -0.856  0.976\n#&gt;  species Chinstrap - Adelie   10.563      0.418 25.272   &lt;0.001 465.7  9.744 11.382\n#&gt;  species Gentoo - Adelie       5.792      0.798  7.257   &lt;0.001  41.2  4.228  7.356\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWhich is equivalent to this in emmeans:\n\nemm &lt;- emmeans(\n    mod,\n    specs = c(\"species\", \"island\"))\ncontrast(emm, method = \"trt.vs.ctrl1\")\n#&gt;  contrast                            estimate    SE  df t.ratio p.value\n#&gt;  Chinstrap Biscoe - Adelie Biscoe      nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Biscoe - Adelie Biscoe          5.792 0.798 331   7.257  &lt;.0001\n#&gt;  Adelie Dream - Adelie Biscoe          -0.434 0.451 331  -0.962  0.8476\n#&gt;  Chinstrap Dream - Adelie Biscoe       nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Dream - Adelie Biscoe           5.358 1.035 331   5.177  &lt;.0001\n#&gt;  Adelie Torgersen - Adelie Biscoe       0.060 0.467 331   0.128  0.9998\n#&gt;  Chinstrap Torgersen - Adelie Biscoe   nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Torgersen - Adelie Biscoe       5.852 1.070 331   5.468  &lt;.0001\n#&gt; \n#&gt; Results are averaged over the levels of: sex \n#&gt; P value adjustment: dunnettx method for 8 tests\n\nThe emmeans section of the Alternative Software vignette shows further examples.\nThe excellent vignette of the emmeans package discuss the same issues in a slightly different (and more positive) way:\n\nThe point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.\n\n\nAll this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#conditional-contrasts",
    "href": "articles/comparisons.html#conditional-contrasts",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.5 Conditional contrasts",
    "text": "3.5 Conditional contrasts\nConsider a model with an interaction term. What happens to the dependent variable when the hp variable increases by 10 units?\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt, data = mtcars)\n\nplot_comparisons(\n    mod,\n    variables = list(hp = 10),\n    condition = \"wt\")",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#transformations",
    "href": "articles/comparisons.html#transformations",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.6 Transformations",
    "text": "3.6 Transformations\nSo far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:\n\ncomparison\ntransform\n\nConsider the case of a model with a single predictor \\(x\\). To compute average contrasts, we proceed as follows:\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x\\): \\(\\hat{y}_x\\)\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x + 1\\): \\(\\hat{y}_{x+1}\\)\n\n\ncomparison: Compute unit-level contrasts by taking the difference between (or some other function of) adjusted predictions: \\(\\hat{y}_{x+1} - \\hat{y}_x\\)\n\nCompute the average contrast by taking the mean of unit-level contrasts: \\(1/N \\sum_{i=1}^N \\hat{y}_{x+1} - \\hat{y}_x\\)\n\n\ntransform: Transform the average contrast or return them as-is.\n\nThe comparison argument of the comparisons() function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with hi value of \\(x\\), and predictions with a lo value of \\(x\\): function(hi, lo) hi-lo.\nThe transform argument of the comparisons() function applies a custom transformation to the unit-level contrasts.\nThe transform argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without transform.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#differences",
    "href": "articles/comparisons.html#differences",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.7 Differences",
    "text": "3.7 Differences\nThe default contrast calculate by the comparisons() function is a (untransformed) difference between two adjusted predictions. For instance, to estimate the effect of a change of 1 unit, we do:\n\nlibrary(marginaleffects)\n\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\n\n## construct data\n\nmtcars_minus &lt;- mtcars_plus &lt;- mtcars\nmtcars_minus$mpg &lt;- mtcars_minus$mpg - 0.5\nmtcars_plus$mpg &lt;- mtcars_plus$mpg + 0.5\n\n## adjusted predictions\nyhat_minus &lt;- predict(mod, newdata = mtcars_minus, type = \"response\")\nyhat_plus &lt;- predict(mod, newdata = mtcars_plus, type = \"response\")\n\n## unit-level contrasts\ncon &lt;- yhat_plus - yhat_minus\n\n## average contrasts\nmean(con)\n#&gt; [1] 0.05540227\n\nWe can use the avg_comparisons() function , or the by argument to obtain the same results:\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1   0.0554    0.00835 6.64   &lt;0.001 34.9 0.039 0.0718\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1   0.0554    0.00835 6.64   &lt;0.001 34.9 0.039 0.0718\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#difference-in-differences-in-differences",
    "href": "articles/comparisons.html#difference-in-differences-in-differences",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.8 Difference-in-Differences(-in-Differences)",
    "text": "3.8 Difference-in-Differences(-in-Differences)\nGoing back to our Titanic example:\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\nIn this case, a contrast is a difference between predicted probabilities. We can compute that contrast for different types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Contrast PClass Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % SexCode  Age\n#&gt;  SexCode    1 - 0    1st    0.483     0.0631 7.65   &lt;0.001 45.5 0.359  0.606   0.381 30.4\n#&gt;  SexCode    1 - 0    3rd    0.335     0.0634 5.29   &lt;0.001 22.9 0.211  0.459   0.381 30.4\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, predicted_lo, predicted_hi, predicted, Survived, SexCode, Age \n#&gt; Type:  response\n\nOne we can notice above, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. Being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?\nTo answer this question, we can compute a difference-in-difference using the hypothesis argument (see the Hypothesis vignette for details). For example, using b1 and b2 to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:\n\ncomparisons(\n  titanic,\n  hypothesis = \"b1 - b2 = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  b1-b2=0    0.148     0.0894 1.65   0.0987 3.3 -0.0276  0.323\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNow, let‚Äôs say we consider more types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;     Term Contrast PClass   Age Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 % 97.5 % SexCode\n#&gt;  SexCode    1 - 0    1st  0.17   0.1081      0.122  0.883   0.3774   1.4 -0.1319  0.348   0.381\n#&gt;  SexCode    1 - 0    1st 71.00   0.8795      0.057 15.437   &lt;0.001 176.2  0.7679  0.991   0.381\n#&gt;  SexCode    1 - 0    3rd  0.17   0.0805      0.157  0.513   0.6081   0.7 -0.2272  0.388   0.381\n#&gt;  SexCode    1 - 0    3rd 71.00   0.4265      0.203  2.101   0.0356   4.8  0.0287  0.824   0.381\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, Age, predicted_lo, predicted_hi, predicted, Survived, SexCode \n#&gt; Type:  response\n\nWith these results, we could compute a triple difference:\n\ncomparisons(\n  titanic,\n  hypothesis = \"(b1 - b3) - (b2 - b4) = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;               Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b1-b3)-(b2-b4)=0   -0.425      0.359 -1.19    0.236 2.1 -1.13  0.278\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#ratios",
    "href": "articles/comparisons.html#ratios",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.9 Ratios",
    "text": "3.9 Ratios\nInstead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, the adjrr function the Stata software package can compute ‚Äúadjusted risk ratios‚Äù, which are ratios of adjusted predictions. To do this in R, we use the comparison argument:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.29      0.133 9.69   &lt;0.001 71.4  1.03   1.55\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis result is the average adjusted risk ratio for an increment of 1, that is, the adjusted predictions when the mpg are incremented by 0.5, divided by the adjusted predictions when mpg is decremented by 0.5.\nThe comparison accepts different values for common types of contrasts: ‚Äòdifference‚Äô, ‚Äòratio‚Äô, ‚Äòlnratio‚Äô, ‚Äòratioavg‚Äô, ‚Äòlnratioavg‚Äô, ‚Äòlnoravg‚Äô, ‚Äòdifferenceavg‚Äô. These strings are shortcuts for functions that accept two vectors of adjusted predictions and returns a single vector of contrasts. For example, these two commands yield identical results:\n\navg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.29      0.133 9.69   &lt;0.001 71.4  1.03   1.55\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, comparison = function(hi, lo) hi / lo)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.29      0.133 9.69   &lt;0.001 71.4  1.03   1.55\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThis mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:\n\navg_comparisons(mod, comparison = function(hi, lo) sqrt(hi) / log(lo + 10))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1    0.264     0.0261 10.1   &lt;0.001 77.4 0.213  0.315\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe same arguments work in the plotting function plot_comparisons() as well, which allows us to plot various custom contrasts. Here is a comparison of Adjusted Risk Ratio and Adjusted Risk Difference in a model of the probability of survival aboard the Titanic:\n\nlibrary(ggplot2)\nlibrary(patchwork)\ntitanic &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ntitanic &lt;- read.csv(titanic)\nmod_titanic &lt;- glm(\n    Survived ~ Sex * PClass + Age + I(Age^2),\n    family = binomial,\n    data = titanic)\n\navg_comparisons(mod_titanic)\n#&gt; \n#&gt;    Term      Contrast Estimate Std. Error      z Pr(&gt;|z|)     S    2.5 %   97.5 %\n#&gt;  Age    +1             -0.0065    0.00108  -6.03   &lt;0.001  29.2 -0.00862 -0.00439\n#&gt;  PClass 2nd - 1st      -0.2058    0.03954  -5.20   &lt;0.001  22.3 -0.28328 -0.12828\n#&gt;  PClass 3rd - 1st      -0.4043    0.03958 -10.21   &lt;0.001  78.9 -0.48187 -0.32670\n#&gt;  Sex    male - female  -0.4847    0.03004 -16.14   &lt;0.001 192.2 -0.54355 -0.42580\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np1 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survival | Age + 1) / P(Survival | Age)\")\n\np2 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\") +\n    ylab(\"Adjusted Risk Difference\\nP(Survival | Age + 1) - P(Survival | Age)\")\n\np1 + p2\n\n\n\n\nBy default, the standard errors around contrasts are computed using the delta method on the scale determined by the type argument (e.g., ‚Äúlink‚Äù or ‚Äúresponse‚Äù). Some analysts may prefer to proceed differently. For example, in Stata, the adjrr computes adjusted risk ratios (ARR) in two steps:\n\nCompute the natural log of the ratio between the mean of adjusted predictions with \\(x+1\\) and the mean of adjusted predictions with \\(x\\).\nExponentiate the estimate and confidence interval bounds.\n\nStep 1 is easy to achieve with the comparison argument described above. Step 2 can be achieved with the transform argument:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27  0.00937 6.7  1.06   1.53\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that we can use the lnratioavg shortcut instead of defining the function ourselves.\nThe order of operations in previous command was:\n\nCompute the custom unit-level log ratios\nExponentiate them\nTake the average using the avg_comparisons()\n\n\nThere is a very subtle difference between the procedure above and this code:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27  0.00937 6.7  1.06   1.53\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nSince the exp function is now passed to the transform argument of the comparisons() function, the exponentiation is now done only after unit-level contrasts have been averaged. This is what Stata appears to do under the hood, and the results are slightly different.\n\ncomparisons(\n    mod,\n    comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.14   &lt;0.001 31.9  1.09   1.18\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nNote that equivalent results can be obtained using shortcut strings in the comparison argument: ‚Äúratio‚Äù, ‚Äúlnratio‚Äù, ‚Äúlnratioavg‚Äù.\n\ncomparisons(\n    mod,\n    comparison = \"lnratioavg\",\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg mean(+1)     1.14   &lt;0.001 31.9  1.09   1.18\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nAll the same arguments apply to the plotting functions of the marginaleffects package as well. For example we can plot the Adjusted Risk Ratio in a model with a quadratic term:\n\nlibrary(ggplot2)\ndat_titanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\nmod2 &lt;- glm(Survived  ~ Age, data = dat_titanic, family = binomial)\nplot_comparisons(\n    mod2,\n    variables = list(\"Age\" = 10),\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survived = 1 | Age + 10) / P(Survived = 1 | Age)\")",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#forward-backward-centered-and-custom-differences",
    "href": "articles/comparisons.html#forward-backward-centered-and-custom-differences",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.10 Forward, Backward, Centered, and Custom Differences",
    "text": "3.10 Forward, Backward, Centered, and Custom Differences\nBy default, the comparisons() function computes a ‚Äúcentered‚Äù difference. For example, if we ask comparisons() to estimate the effect of a 10-unit change in predictor x on outcome y, comparisons() will compare the predicted values with x-5 and x+5.\n\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (mtcars$hp - min(mtcars$hp)) / (max(mtcars$hp) - min(mtcars$hp)) + 1\nmod &lt;- lm(mpg ~ log(new_hp), data = dat)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = 10))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp      +10    -4.06      0.464 -8.74   &lt;0.001 58.6 -4.97  -3.15\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:\n\nforward_diff &lt;- \\(x) data.frame(x, x + 10)\nbackward_diff &lt;- \\(x) data.frame(x - 10, x)\ncenter_diff &lt;- \\(x) data.frame(x - 5, x + 5)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = forward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = backward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -6.51      0.744 -8.74   &lt;0.001 58.6 -7.97  -5.05\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = center_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -4.06      0.464 -8.74   &lt;0.001 58.6 -4.97  -3.15\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that the last ‚Äúcentered‚Äù difference gives the same results as the default comparisons() call.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#lognormal-hurdle-model",
    "href": "articles/comparisons.html#lognormal-hurdle-model",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.11 Lognormal hurdle model",
    "text": "3.11 Lognormal hurdle model\nWith hurdle models, we can fit two separate models simultaneously:\n\nA model that predicts if the outcome is zero or not zero\nIf the outcome is not zero, a model that predicts what the value of the outcome is\n\nWe can calculate predictions and marginal effects for each of these hurdle model processes, but doing so requires some variable transformation since the stages of these models use different link functions.\nThe hurdle_lognormal() family in brms uses logistic regression (with a logit link) for the hurdle part of the model and lognormal regression (where the outcome is logged before getting used in the model) for the non-hurdled part. Let‚Äôs look at an example of predicting GDP per capita (which is distributed exponentially) using life expectancy. We‚Äôll add some artificial zeros so that we can work with a hurdle stage of the model.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(gapminder)\n\n## Build some 0s into the GDP column\nset.seed(1234)\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap0 = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero)\n\nmod &lt;- brm(\n  bf(gdpPercap0 ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = 4, cores = 4, seed = 1234)\n\nWe have two different sets of coefficients here for the two different processes. The hurdle part (hu) uses a logit link, and the non-hurdle part (mu) uses an identity link. However, that‚Äôs a slight misnomer‚Äîa true identity link would show the coefficients on a non-logged dollar value scale. Because we‚Äôre using a lognormal family, GDP per capita is pre-logged, so the ‚Äúoriginal‚Äù identity scale is actually logged dollars.\n\nsummary(mod)\n\n\n#&gt;  Family: hurdle_lognormal \n#&gt;   Links: mu = identity; sigma = identity; hu = logit \n#&gt; Formula: gdpPercap0 ~ lifeExp \n#&gt;          hu ~ lifeExp\n#&gt;    Data: gapminder (Number of observations: 1680) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept        3.47      0.09     3.29     3.65 1.00     4757     3378\n#&gt; hu_Intercept     3.16      0.40     2.37     3.96 1.00     2773     2679\n#&gt; lifeExp          0.08      0.00     0.08     0.08 1.00     5112     3202\n#&gt; hu_lifeExp      -0.10      0.01    -0.12    -0.08 1.00     2385     2652\n#&gt; ...\n\nWe can get predictions for the hu part of the model on the link (logit) scale:\n\npredictions(mod, dpar = \"hu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40   -0.817 -1.03 -0.604\n#&gt;       60   -2.805 -3.06 -2.555\n#&gt;       80   -4.790 -5.34 -4.275\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\n\n‚Ä¶or on the response (percentage point) scale:\n\npredictions(mod, dpar = \"hu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate   2.5 % 97.5 %\n#&gt;       40  0.30630 0.26231 0.3534\n#&gt;       60  0.05703 0.04466 0.0721\n#&gt;       80  0.00824 0.00478 0.0137\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can also get slopes for the hu part of the model on the link (logit) or response (percentage point) scales:\n\nslopes(mod, dpar = \"hu\", type = \"link\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp Estimate  2.5 %  97.5 %\n#&gt;  lifeExp      40  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      60  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      80  -0.0993 -0.116 -0.0837\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  link\n\nslopes(mod, dpar = \"hu\", type = \"response\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp  Estimate    2.5 %    97.5 %\n#&gt;  lifeExp      40 -0.021078 -0.02591 -0.016588\n#&gt;  lifeExp      60 -0.005321 -0.00615 -0.004561\n#&gt;  lifeExp      80 -0.000812 -0.00115 -0.000543\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWorking with the mu part of the model is trickier. Switching between type = \"link\" and type = \"response\" doesn‚Äôt change anything, since the outcome is pre-logged:\n\npredictions(mod, dpar = \"mu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\npredictions(mod, dpar = \"mu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nFor predictions, we need to exponentiate the results to scale them back up to dollar amounts. We can do this by post-processing the results (e.g.¬†with dplyr::mutate(predicted = exp(predicted))), or we can use the transform argument in predictions() to pass the results to exp() after getting calculated:\n\npredictions(mod, dpar = \"mu\", \n            newdata = datagrid(lifeExp = seq(40, 80, 20)),\n            transform = exp)\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40      744   694    801\n#&gt;       60     3581  3449   3718\n#&gt;       80    17215 16110  18410\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n\nWe can pass transform = exp to plot_predictions() too:\n\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"link\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       title = \"Hurdle part (hu)\",\n       subtitle = \"Logit-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"response\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       subtitle = \"Percentage point-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       title = \"Non-hurdle part (mu)\",\n       subtitle = \"Log-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  transform = exp,\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       subtitle = \"Dollar-scale predictions\")\n\n\n\n\nFor marginal effects, we need to transform the predictions before calculating the instantaneous slopes. We also can‚Äôt use the slopes() function directly‚Äîwe need to use comparisons() and compute the numerical derivative ourselves (i.e.¬†predict gdpPercap at lifeExp of 40 and 40.001 and calculate the slope between those predictions). We can use the comparison argument to pass the pair of predicted values to exp() before calculating the slopes:\n\n## step size of the numerical derivative\neps &lt;- 0.001\n\ncomparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  # rescale the elements of the slope\n  # (exp(40.001) - exp(40)) / exp(0.001)\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps\n)\n#&gt; \n#&gt;     Term Contrast lifeExp Estimate  2.5 % 97.5 %\n#&gt;  lifeExp   +0.001      40     58.4   55.8     61\n#&gt;  lifeExp   +0.001      60    280.9  266.6    296\n#&gt;  lifeExp   +0.001      80   1349.4 1222.6   1490\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n\nWe can visually confirm that these are the instantaneous slopes at each of these levels of life expectancy:\n\npredictions_data &lt;- predictions(\n  mod,\n  newdata = datagrid(lifeExp = seq(30, 80, 1)),\n  dpar = \"mu\",\n  transform = exp) |&gt;\n  select(lifeExp, prediction = estimate)\n\nslopes_data &lt;- comparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps) |&gt;\n  select(lifeExp, estimate) |&gt;\n  left_join(predictions_data, by = \"lifeExp\") |&gt;\n  # Point-slope formula: (y - y1) = m(x - x1)\n  mutate(intercept = estimate * (-lifeExp) + prediction)\n\nggplot(predictions_data, aes(x = lifeExp, y = prediction)) +\n  geom_line(size = 1) + \n  geom_abline(data = slopes_data, aes(slope = estimate, intercept = intercept), \n              size = 0.5, color = \"red\") +\n  geom_point(data = slopes_data) +\n  geom_label(data = slopes_data, aes(label = paste0(\"Slope: \", round(estimate, 1))),\n             nudge_x = -1, hjust = 1) +\n  theme_minimal()\n\n\n\n\nWe now have this in the experiments section",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/comparisons.html#visual-examples",
    "href": "articles/comparisons.html#visual-examples",
    "title": "\n3¬† Comparisons\n",
    "section": "\n3.12 Visual examples",
    "text": "3.12 Visual examples\n\nokabeito &lt;- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\n\nset.seed(1024)\nn &lt;- 200\nd &lt;- data.frame(\n  y = rnorm(n),\n  cond = as.factor(sample(0:1, n, TRUE)),\n  episode = as.factor(sample(0:4, n, TRUE)))\n\nmodel1 &lt;- lm(y ~ cond * episode, data = d)\n\np &lt;- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point()\n\n\n\n\n## do episodes 1 and 2 differ when `cond=0`\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = \"black\") +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0))    # grid\n#&gt; \n#&gt;     Term Contrast cond Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 % episode\n#&gt;  episode    2 - 1    0    0.241      0.396 0.609    0.542 0.9 -0.535   1.02       0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cond, predicted_lo, predicted_hi, predicted, y, episode \n#&gt; Type:  response\n\n## do cond=0 and cond=1 differ when episode = 1\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = \"cond\",              # comparison of interest\n  newdata = datagrid(episode = 1)) # grid\n#&gt; \n#&gt;  Term Contrast episode Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % cond\n#&gt;  cond    1 - 0       1    0.546      0.347 1.57    0.115 3.1 -0.134   1.23    0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, episode, predicted_lo, predicted_hi, predicted, y, cond \n#&gt; Type:  response\n\n## Is the difference between episode 1 and 2 larger in cond=0 or cond=1? \n## try this without the `hypothesis` argument to see what we are comparing more clearly\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  annotate(\"rect\", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = \"green\") +\n  annotate(\"rect\", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = \"orange\")  +\n  ggtitle(\"Is the green box taller than the orange box?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0:1),  # grid\n  hypothesis = \"b1 = b2\")          # hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.413      0.508 0.812    0.417 1.3 -0.583   1.41\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Comparisons</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#definition",
    "href": "articles/slopes.html#definition",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.1 Definition",
    "text": "4.1 Definition\nSlopes are defined as:\n\nPartial derivatives of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends.\n\nThis vignette follows the econometrics tradition by referring to ‚Äúslopes‚Äù and ‚Äúmarginal effects‚Äù interchangeably. In this context, the word ‚Äúmarginal‚Äù refers to the idea of a ‚Äúsmall change,‚Äù in the calculus sense.\nA marginal effect measures the association between a change in a regressor \\(x\\), and a change in the response \\(y\\). Put differently, differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\nMarginal effects are extremely useful, because they are intuitive and easy to interpret. They are often the main quantity of interest in an empirical analysis.\nIn scientific practice, the ‚ÄúMarginal Effect‚Äù falls in the same toolbox as the ‚ÄúContrast.‚Äù Both try to answer a counterfactual question: What would happen to \\(y\\) if \\(x\\) were different? They allow us to model the ‚Äúeffect‚Äù of a change/difference in the regressor \\(x\\) on the response \\(y\\).1\nTo illustrate the concept, consider this quadratic function:\n\\[y = -x^2\\]\nFrom the definition above, we know that the marginal effect is the partial derivative of \\(y\\) with respect to \\(x\\):\n\\[\\frac{\\partial y}{\\partial x} = -2x\\]\nTo get intuition about how to interpret this quantity, consider the response of \\(y\\) to \\(x\\). It looks like this:\n\n\n\n\n\nWhen \\(x\\) increases, \\(y\\) starts to increase. But then, as \\(x\\) increases further, \\(y\\) creeps back down in negative territory.\nA marginal effect is the slope of this response function at a certain value of \\(x\\). The next plot adds three tangent lines, highlighting the slopes of the response function for three values of \\(x\\). The slopes of these tangents tell us three things:\n\nWhen \\(x&lt;0\\), the slope is positive: an increase in \\(x\\) is associated with an increase in \\(y\\): The marginal effect is positive.\nWhen \\(x=0\\), the slope is null: a (small) change in \\(x\\) is associated with no change in \\(y\\). The marginal effect is null.\nWhen \\(x&gt;0\\), the slope is negative: an increase in \\(x\\) is associated with a decrease in \\(y\\). The marginal effect is negative.\n\n\n\n\n\n\nBelow, we show how to reach the same conclusions in an estimation context, with simulated data and the slopes function.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#slopes-function",
    "href": "articles/slopes.html#slopes-function",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.2 slopes function",
    "text": "4.2 slopes function\nThe marginal effect is a unit-level measure of association between changes in a regressor and changes in the response. Except in the simplest linear models, the value of the marginal effect will be different from individual to individual, because it will depend on the values of the other covariates for each individual.\nThe slopes function thus produces distinct estimates of the marginal effect for each row of the data used to fit the model. The output of marginaleffects is a simple data.frame, which can be inspected with all the usual R commands.\nTo show this, we load the library, download the Palmer Penguins, and estimate a GLM model:\n\nlibrary(marginaleffects)\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin &lt;- ifelse(dat$body_mass_g &gt; median(dat$body_mass_g, na.rm = TRUE), 1, 0)\n\nmod &lt;- glm(large_penguin ~ bill_length_mm + flipper_length_mm + species,\n           data = dat, family = binomial)\n\n\nmfx &lt;- slopes(mod)\nhead(mfx)\n#&gt; \n#&gt;            Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  bill_length_mm    dY/dX   0.0176    0.00837 2.11  0.03520  4.8 0.00122 0.0340\n#&gt;  bill_length_mm    dY/dX   0.0359    0.01236 2.90  0.00371  8.1 0.01164 0.0601\n#&gt;  bill_length_mm    dY/dX   0.0844    0.02110 4.00  &lt; 0.001 14.0 0.04309 0.1258\n#&gt;  bill_length_mm    dY/dX   0.0347    0.00642 5.41  &lt; 0.001 23.9 0.02214 0.0473\n#&gt;  bill_length_mm    dY/dX   0.0509    0.01352 3.77  &lt; 0.001 12.6 0.02440 0.0774\n#&gt;  bill_length_mm    dY/dX   0.0165    0.00778 2.12  0.03367  4.9 0.00128 0.0318\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm, flipper_length_mm, species \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#the-marginal-effects-zoo",
    "href": "articles/slopes.html#the-marginal-effects-zoo",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.3 The Marginal Effects Zoo",
    "text": "4.3 The Marginal Effects Zoo\nA dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. There are ways to make this information easier to digest, by computing various quantities of interest. In a characteristically excellent blog post, Professor Andrew Heiss introduces many such quantities:\n\nAverage Marginal Effects\nGroup-Average Marginal Effects\nMarginal Effects at User-Specified Values (or Representative Values)\nMarginal Effects at the Mean\nCounterfactual Marginal Effects\nConditional Marginal Effects\n\nThe rest of this vignette defines each of those quantities and explains how to use the slopes() and plot_slopes() functions to compute them. The main differences between these quantities pertain to (a) the regressor values at which we estimate marginal effects, and (b) the way in which unit-level marginal effects are aggregated.\nHeiss drew this exceedingly helpful graph which summarizes the information in the rest of this vignette:\n\n\n4.3.1 Average Marginal Effect (AME)\nA dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. Many analysts like to report the ‚ÄúAverage Marginal Effect‚Äù, that is, the average of all the observation-specific marginal effects. These are easy to compute based on the full data.frame shown above, but the avg_slopes() function is convenient:\n\navg_slopes(mod)\n#&gt; \n#&gt;               Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;  bill_length_mm    dY/dX                0.0276    0.00578  4.773   &lt;0.001 19.1  0.01625  0.0389\n#&gt;  flipper_length_mm dY/dX                0.0106    0.00235  4.512   &lt;0.001 17.3  0.00599  0.0152\n#&gt;  species           Chinstrap - Adelie  -0.4148    0.05654 -7.336   &lt;0.001 42.0 -0.52561 -0.3040\n#&gt;  species           Gentoo - Adelie      0.0617    0.10688  0.577    0.564  0.8 -0.14779  0.2712\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that since marginal effects are derivatives, they are only properly defined for continuous numeric variables. When the model also includes categorical regressors, the summary function will try to display relevant (regression-adjusted) contrasts between different categories, as shown above.\nYou can also extract average marginal effects using tidy and glance methods which conform to the broom package specification:\n\ntidy(mfx)\n#&gt; # A tibble: 4 √ó 8\n#&gt;   term              contrast                       estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;             &lt;chr&gt;                             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 bill_length_mm    mean(dY/dX)                      0.0276   0.00578     4.77  1.82e- 6  0.0163     0.0389\n#&gt; 2 flipper_length_mm mean(dY/dX)                      0.0106   0.00235     4.51  6.42e- 6  0.00599    0.0152\n#&gt; 3 species           mean(Chinstrap) - mean(Adelie)  -0.415    0.0565     -7.34  2.20e-13 -0.526     -0.304 \n#&gt; 4 species           mean(Gentoo) - mean(Adelie)      0.0617   0.107       0.577 5.64e- 1 -0.148      0.271\n\nglance(mfx)\n#&gt; # A tibble: 1 √ó 7\n#&gt;     aic   bic r2.tjur  rmse  nobs     F logLik   \n#&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;logLik&gt; \n#&gt; 1  180.  199.   0.695 0.276   342  15.7 -84.92257\n\n\n4.3.2 Group-Average Marginal Effect (G-AME)\nWe can also use the by argument the average marginal effects within different subgroups of the observed data, based on values of the regressors. For example, to compute the average marginal effects of Bill Length for each Species, we do:\n\navg_slopes(\n  mod,\n  by = \"species\",\n  variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term    Contrast   species Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;  bill_length_mm mean(dY/dX) Adelie     0.04354    0.00878 4.96   &lt;0.001 20.4  0.0263 0.06076\n#&gt;  bill_length_mm mean(dY/dX) Chinstrap  0.03680    0.00976 3.77   &lt;0.001 12.6  0.0177 0.05593\n#&gt;  bill_length_mm mean(dY/dX) Gentoo     0.00287    0.00284 1.01    0.312  1.7 -0.0027 0.00844\n#&gt; \n#&gt; Columns: term, contrast, species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nThis is equivalent to manually taking the mean of the observation-level marginal effect for each species sub-group:\n\naggregate(\n  mfx$estimate,\n  by = list(mfx$species, mfx$term),\n  FUN = mean)\n#&gt;     Group.1           Group.2            x\n#&gt; 1    Adelie    bill_length_mm  0.043539914\n#&gt; 2 Chinstrap    bill_length_mm  0.036801185\n#&gt; 3    Gentoo    bill_length_mm  0.002871562\n#&gt; 4    Adelie flipper_length_mm  0.016710631\n#&gt; 5 Chinstrap flipper_length_mm  0.014124217\n#&gt; 6    Gentoo flipper_length_mm  0.001102231\n#&gt; 7    Adelie           species -0.054519623\n#&gt; 8 Chinstrap           species -0.313337522\n#&gt; 9    Gentoo           species -0.250726004\n\nNote that marginaleffects follows Stata and the margins package in computing standard errors using the group-wise averaged Jacobian.\n\n4.3.3 Marginal Effect at User-Specified Values\nSometimes, we are not interested in all the unit-specific marginal effects, but would rather look at the estimated marginal effects for certain ‚Äútypical‚Äù individuals, or for user-specified values of the regressors. The datagrid function helps us build a data grid full of ‚Äútypical‚Äù rows. For example, to generate artificial Adelies and Gentoos with 180mm flippers:\n\ndatagrid(flipper_length_mm = 180,\n         species = c(\"Adelie\", \"Gentoo\"),\n         model = mod)\n#&gt;   large_penguin bill_length_mm flipper_length_mm species\n#&gt; 1     0.4853801       43.92193               180  Adelie\n#&gt; 2     0.4853801       43.92193               180  Gentoo\n\nThe same command can be used (omitting the model argument) to marginaleffects‚Äôs newdata argument to compute marginal effects for those (fictional) individuals:\n\nslopes(\n  mod,\n  newdata = datagrid(\n    flipper_length_mm = 180,\n    species = c(\"Adelie\", \"Gentoo\")))\n#&gt; \n#&gt;               Term           Contrast flipper_length_mm species Estimate Std. Error      z Pr(&gt;|z|)    S    2.5 %   97.5 %\n#&gt;  bill_length_mm    dY/dX                            180  Adelie   0.0607    0.03322  1.827   0.0677  3.9 -0.00442  0.12580\n#&gt;  bill_length_mm    dY/dX                            180  Gentoo   0.0847    0.03923  2.158   0.0309  5.0  0.00778  0.16156\n#&gt;  flipper_length_mm dY/dX                            180  Adelie   0.0233    0.00551  4.231   &lt;0.001 15.4  0.01250  0.03408\n#&gt;  flipper_length_mm dY/dX                            180  Gentoo   0.0325    0.00850  3.822   &lt;0.001 12.9  0.01583  0.04917\n#&gt;  species           Chinstrap - Adelie               180  Adelie  -0.2111    0.10668 -1.978   0.0479  4.4 -0.42013 -0.00197\n#&gt;  species           Chinstrap - Adelie               180  Gentoo  -0.2111    0.10668 -1.978   0.0479  4.4 -0.42013 -0.00197\n#&gt;  species           Gentoo - Adelie                  180  Adelie   0.1591    0.30225  0.526   0.5986  0.7 -0.43328  0.75152\n#&gt;  species           Gentoo - Adelie                  180  Gentoo   0.1591    0.30225  0.526   0.5986  0.7 -0.43328  0.75152\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, flipper_length_mm, species, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm \n#&gt; Type:  response\n\nWhen variables are omitted from the datagrid call, they will automatically be set at their mean or mode (depending on variable type).\n\n4.3.4 Marginal Effect at the Mean (MEM)\nThe ‚ÄúMarginal Effect at the Mean‚Äù is a marginal effect calculated for a hypothetical observation where each regressor is set at its mean or mode. By default, the datagrid function that we used in the previous section sets all regressors to their means or modes. To calculate the MEM, we can set the newdata argument, which determines the values of predictors at which we want to compute marginal effects:\n\nslopes(mod, newdata = \"mean\")\n#&gt; \n#&gt;               Term           Contrast Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;  bill_length_mm    dY/dX                0.0502    0.01244   4.038   &lt;0.001 14.2  0.02585  0.0746\n#&gt;  flipper_length_mm dY/dX                0.0193    0.00553   3.489   &lt;0.001 11.0  0.00845  0.0301\n#&gt;  species           Chinstrap - Adelie  -0.8070    0.07690 -10.494   &lt;0.001 83.2 -0.95776 -0.6563\n#&gt;  species           Gentoo - Adelie      0.0829    0.11469   0.722     0.47  1.1 -0.14193  0.3076\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm, flipper_length_mm, species \n#&gt; Type:  response\n\n\n4.3.5 Counterfactual Marginal Effects\nThe datagrid function allowed us look at completely fictional individuals. Setting the grid_type argument of this function to \"counterfactual\" lets us compute the marginal effects for the actual observations in our dataset, but with a few manipulated values. For example, this code will create a data.frame twice as long as the original dat, where each observation is repeated with different values of the flipper_length_mm variable:\n\nnd &lt;- datagrid(flipper_length_mm = c(160, 180),\n               model = mod,\n               grid_type = \"counterfactual\")\n\nWe see that the rows 1, 2, and 3 of the original dataset have been replicated twice, with different values of the flipper_length_mm variable:\n\nnd[nd$rowid %in% 1:3,]\n#&gt;     rowidcf large_penguin bill_length_mm species flipper_length_mm\n#&gt; 1         1             0           39.1  Adelie               160\n#&gt; 2         2             0           39.5  Adelie               160\n#&gt; 3         3             0           40.3  Adelie               160\n#&gt; 343       1             0           39.1  Adelie               180\n#&gt; 344       2             0           39.5  Adelie               180\n#&gt; 345       3             0           40.3  Adelie               180\n\nWe can use the observation-level marginal effects to compute average (or median, or anything else) marginal effects over the counterfactual individuals:\n\nlibrary(dplyr)\n\nslopes(mod, newdata = nd) |&gt;\n    group_by(term) |&gt;\n    summarize(estimate = median(estimate))\n#&gt; # A tibble: 3 √ó 2\n#&gt;   term               estimate\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;\n#&gt; 1 bill_length_mm    0.00985  \n#&gt; 2 flipper_length_mm 0.00378  \n#&gt; 3 species           0.0000226\n\n\n4.3.6 Conditional Marginal Effects (Plot)\nThe plot_slopes function can be used to draw ‚ÄúConditional Marginal Effects.‚Äù This is useful when a model includes interaction terms and we want to plot how the marginal effect of a variable changes as the value of a ‚Äúcondition‚Äù (or ‚Äúmoderator‚Äù) variable changes:\n\nmod &lt;- lm(mpg ~ hp * wt + drat, data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"wt\")\n\n\n\n\nThe marginal effects in the plot above were computed with values of all regressors ‚Äì except the variables and the condition ‚Äì held at their means or modes, depending on variable type.\nSince plot_slopes() produces a ggplot2 object, it is easy to customize. For example:\n\nplot_slopes(mod, variables = \"hp\", condition = \"wt\") +\n    geom_rug(aes(x = wt), data = mtcars) +\n    theme_classic()",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#example-quadratic",
    "href": "articles/slopes.html#example-quadratic",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.4 Example: Quadratic",
    "text": "4.4 Example: Quadratic\nIn the ‚ÄúDefinition‚Äù section of this vignette, we considered how marginal effects can be computed analytically in a simple quadratic equation context. We can now use the slopes function to replicate our analysis of the quadratic function in a regression application.\nSay you estimate a linear regression model with a quadratic term:\n\\[Y = \\beta_0 + \\beta_1 X^2 + \\varepsilon\\]\nand obtain estimates of \\(\\beta_0=1\\) and \\(\\beta_1=2\\). Taking the partial derivative with respect to \\(X\\) and plugging in our estimates gives us the marginal effect of \\(X\\) on \\(Y\\):\n\\[\\partial Y / \\partial X = \\beta_0 + 2 \\cdot \\beta_1 X\\] \\[\\partial Y / \\partial X = 1 + 4X\\]\nThis result suggests that the effect of a change in \\(X\\) on \\(Y\\) depends on the level of \\(X\\). When \\(X\\) is large and positive, an increase in \\(X\\) is associated to a large increase in \\(Y\\). When \\(X\\) is small and positive, an increase in \\(X\\) is associated to a small increase in \\(Y\\). When \\(X\\) is a large negative value, an increase in \\(X\\) is associated with a decrease in \\(Y\\).\nmarginaleffects arrives at the same conclusion in simulated data:\n\nlibrary(tidyverse)\nN &lt;- 1e5\nquad &lt;- data.frame(x = rnorm(N))\nquad$y &lt;- 1 + 1 * quad$x + 2 * quad$x^2 + rnorm(N)\nmod &lt;- lm(y ~ x + I(x^2), quad)\n\nslopes(mod, newdata = datagrid(x = -2:2))  |&gt;\n    mutate(truth = 1 + 4 * x) |&gt;\n    select(estimate, truth)\n#&gt; \n#&gt;  Estimate\n#&gt;    -6.989\n#&gt;    -2.996\n#&gt;     0.997\n#&gt;     4.990\n#&gt;     8.983\n#&gt; \n#&gt; Columns: estimate, truth\n\nWe can plot conditional adjusted predictions with plot_predictions function:\n\nplot_predictions(mod, condition = \"x\")\n\n\n\n\nWe can plot conditional marginal effects with the plot_slopes function (see section below):\n\nplot_slopes(mod, variables = \"x\", condition = \"x\")\n\n\n\n\nAgain, the conclusion is the same. When \\(x&lt;0\\), an increase in \\(x\\) is associated with an decrease in \\(y\\). When \\(x&gt;1/4\\), the marginal effect is positive, which suggests that an increase in \\(x\\) is associated with an increase in \\(y\\).",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#slopes-vs-predictions-a-visual-interpretation",
    "href": "articles/slopes.html#slopes-vs-predictions-a-visual-interpretation",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.5 Slopes vs Predictions: A Visual Interpretation",
    "text": "4.5 Slopes vs Predictions: A Visual Interpretation\nOften, analysts will plot predicted values of the outcome with a best fit line:\n\nlibrary(ggplot2)\n\nmod &lt;- lm(mpg ~ hp * qsec, data = mtcars)\n\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_point(data = mtcars, aes(hp, mpg)) \n\n\n\n\nThe slope of this line is calculated using the same technique we all learned in grade school: dividing rise over run.\n\np &lt;- plot_predictions(mod, condition = \"hp\", vcov = TRUE, draw = FALSE)\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[10], y = p$estimate[10], yend = p$estimate[20])) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[20], y = p$estimate[20], yend = p$estimate[20])) +\n  annotate(\"text\", label = \"Rise\", y = 10, x = 140) +\n  annotate(\"text\", label = \"Run\", y = 2, x = 200)\n\n\n\n\nInstead of computing this slope manually, we can just call:\n\navg_slopes(mod, variables = \"hp\")\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp   -0.112     0.0126 -8.92   &lt;0.001 61.0 -0.137 -0.0874\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNow, consider the fact that our model includes an interaction between hp and qsec. This means that the slope will actually differ based on the value of the moderator variable qsec:\n\nplot_predictions(mod, condition = list(\"hp\", \"qsec\" = \"quartile\"))\n\n\n\n\nWe can estimate the slopes of these three fit lines easily:\n\nslopes(\n  mod,\n  variables = \"hp\",\n  newdata = datagrid(qsec = quantile(mtcars$qsec, probs = c(.25, .5, .75))))\n#&gt; \n#&gt;  Term qsec Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp 16.9  -0.0934     0.0111 -8.43   &lt;0.001 54.7 -0.115 -0.0717\n#&gt;    hp 17.7  -0.1093     0.0123 -8.92   &lt;0.001 60.8 -0.133 -0.0853\n#&gt;    hp 18.9  -0.1325     0.0154 -8.60   &lt;0.001 56.8 -0.163 -0.1023\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, qsec, predicted_lo, predicted_hi, predicted, mpg, hp \n#&gt; Type:  response\n\nAs we see in the graph, all three slopes are negative, but the Q3 slope is steepest.\nWe could then push this one step further, and measure the slope of mpg with respect to hp, for all observed values of qsec. This is achieved with the plot_slopes() function:\n\nplot_slopes(mod, variables = \"hp\", condition = \"qsec\") +\n  geom_hline(yintercept = 0, linetype = 3)\n\n\n\n\nThis plot shows that the marginal effect of hp on mpg is always negative (the slope is always below zero), and that this effect becomes even more negative as qsec increases.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#prediction-types",
    "href": "articles/slopes.html#prediction-types",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.6 Prediction types",
    "text": "4.6 Prediction types\nThe marginaleffect function takes the derivative of the fitted (or predicted) values of the model, as is typically generated by the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the marginal effects should be interpreted on that scale. However, users can pass a string or a vector of strings to the type argument, and marginaleffects will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\navg_slopes(mod, type = \"response\")\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg   0.0465    0.00887 5.24   &lt;0.001 22.6 0.0291 0.0639\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, type = \"link\")\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;   mpg    0.307      0.115 2.67  0.00751 7.1 0.0819  0.532\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#minimummaximum-slope-velocity",
    "href": "articles/slopes.html#minimummaximum-slope-velocity",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.7 Minimum/Maximum slope (velocity)",
    "text": "4.7 Minimum/Maximum slope (velocity)\nIn some fields such as epidemiology, it is common to compute the minimum or maximum slope, as a measure of the ‚Äúvelocity‚Äù of the response function. To achieve this, we can use the comparisons() function and define custom functions. Here, we present a quick example without much detail, but you can refer to the comparisons vignette for more explanations on custom functions in the comparison argument.\nConsider this simple GAM model:\n\nlibrary(marginaleffects)\nlibrary(itsadug)\nlibrary(mgcv)\n\nsimdat$Subject &lt;- as.factor(simdat$Subject)\nmodel &lt;- bam(Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\"),\n             data = simdat)\n\nplot_slopes(model, variables = \"Time\", condition = c(\"Time\", \"Group\"))\n\n\n\n\nMinimum velocity, overall:\n\ncomparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  vcov = FALSE,\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;  Term Contrast Estimate\n#&gt;  Time   +1e-06  -0.0267\n#&gt; \n#&gt; Columns: term, contrast, estimate, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nMinimum velocity by group:\n\ncomparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  by = \"Group\",\n  vcov = FALSE,\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;  Term Contrast    Group Estimate\n#&gt;  Time   +1e-06 Children  -0.0153\n#&gt;  Time   +1e-06 Adults    -0.0267\n#&gt; \n#&gt; Columns: term, contrast, Group, estimate \n#&gt; Type:  response\n\nDifference between the minimum velocity of each group:\n\ncomparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  vcov = FALSE,\n  by = \"Group\",\n  hypothesis = \"pairwise\",\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;               Term Estimate\n#&gt;  Children - Adults   0.0114\n#&gt; \n#&gt; Columns: term, estimate \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/slopes.html#manual-computation",
    "href": "articles/slopes.html#manual-computation",
    "title": "\n4¬† Slopes\n",
    "section": "\n4.8 Manual computation",
    "text": "4.8 Manual computation\nNow we illustrate how to reproduce the output of slopes manually:\n\nlibrary(marginaleffects)\n\nmod &lt;- glm(am ~ hp, family = binomial, data = mtcars)\n\neps &lt;- 1e-4\nd1 &lt;- transform(mtcars, hp = hp - eps / 2)\nd2 &lt;- transform(mtcars, hp = hp + eps / 2)\np1 &lt;- predict(mod, type = \"response\", newdata = d1)\np2 &lt;- predict(mod, type = \"response\", newdata = d2)\ns &lt;- (p2 - p1) / eps\ntail(s)\n#&gt;  Porsche 914-2   Lotus Europa Ford Pantera L   Ferrari Dino  Maserati Bora     Volvo 142E \n#&gt;  -0.0020285496  -0.0020192814  -0.0013143243  -0.0018326764  -0.0008900012  -0.0020233577\n\nWhich is equivalent to:\n\nslopes(mod, eps = eps) |&gt; tail()\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 %    97.5 %\n#&gt;    hp -0.00203   0.001612 -1.26  0.20818  2.3 -0.00519  0.001130\n#&gt;    hp -0.00202   0.001584 -1.27  0.20236  2.3 -0.00512  0.001085\n#&gt;    hp -0.00131   0.000430 -3.06  0.00225  8.8 -0.00216 -0.000471\n#&gt;    hp -0.00183   0.001311 -1.40  0.16204  2.6 -0.00440  0.000736\n#&gt;    hp -0.00089   0.000266 -3.34  &lt; 0.001 10.2 -0.00141 -0.000368\n#&gt;    hp -0.00202   0.001571 -1.29  0.19776  2.3 -0.00510  0.001056\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp \n#&gt; Type:  response\n\nAnd we can get average marginal effects by subgroup as follows:\n\ntapply(s, mtcars$cyl, mean)\n#&gt;            4            6            8 \n#&gt; -0.002010526 -0.001990774 -0.001632681\n\nslopes(mod, eps = eps, by = \"cyl\")\n#&gt; \n#&gt;  Term    Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %   97.5 %\n#&gt;    hp mean(dY/dX)   4 -0.00201   0.001482 -1.36   0.1748 2.5 -0.00491 0.000893\n#&gt;    hp mean(dY/dX)   6 -0.00199   0.001459 -1.36   0.1723 2.5 -0.00485 0.000868\n#&gt;    hp mean(dY/dX)   8 -0.00163   0.000967 -1.69   0.0914 3.5 -0.00353 0.000263\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Slopes</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#marginal-means-vs.-average-predictions",
    "href": "articles/marginalmeans.html#marginal-means-vs.-average-predictions",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.1 Marginal Means vs.¬†Average Predictions",
    "text": "5.1 Marginal Means vs.¬†Average Predictions\nWhat should scientists report? Marginal means or average predictions?\nMany analysts ask this question, but unfortunately there isn‚Äôt a single answer. As explained above, marginal means are a special case of predictions, made on a perfectly balanced grid of categorical predictors, with numeric predictors held at their means, and marginalized with respect to some focal variables. Whether the analyst prefers to report this specific type of marginal means or another kind of average prediction will depend on the characteristics of the sample and the population to which they want to generalize.\nAfter reading this vignette and the discussion of emmeans in the Alternative Software vignette, you may want to consult with a statistician to discuss your specific real-world problem and make an informed choice.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#interactions",
    "href": "articles/marginalmeans.html#interactions",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.2 Interactions",
    "text": "5.2 Interactions\nBy default, the marginal_means() function calculates marginal means for each categorical predictor one after the other. We can also compute marginal means for combinations of categories by setting cross=TRUE:\n\nlibrary(lme4)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glmer(\n    Survived ~ Sex * PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n\nRegardless of the scale of the predictions (type argument), marginal_means() always computes standard errors using the Delta Method:\n\nmarginal_means(\n    titanic,\n    type = \"response\",\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  Sex    female 0.738     0.0207 35.68   &lt;0.001 923.8 0.698  0.779\n#&gt;  Sex    male   0.235     0.0203 11.62   &lt;0.001 101.3 0.196  0.275\n#&gt;  PClass 1st    0.708     0.0273 25.95   &lt;0.001 490.9 0.654  0.761\n#&gt;  PClass 2nd    0.511     0.0235 21.76   &lt;0.001 346.4 0.465  0.557\n#&gt;  PClass 3rd    0.242     0.0281  8.59   &lt;0.001  56.7 0.187  0.297\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWhen the model is linear or on the link scale, it also produces confidence intervals:\n\nmarginal_means(\n    titanic,\n    type = \"link\",\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value    Mean Std. Error       z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  Sex    female  1.6407      0.206   7.984   &lt;0.001 49.3  1.238  2.043\n#&gt;  Sex    male   -1.3399      0.124 -10.828   &lt;0.001 88.3 -1.582 -1.097\n#&gt;  PClass 1st     1.6307      0.271   6.028   &lt;0.001 29.2  1.100  2.161\n#&gt;  PClass 2nd     0.0997      0.211   0.472    0.637  0.7 -0.314  0.513\n#&gt;  PClass 3rd    -1.2792      0.155  -8.255   &lt;0.001 52.6 -1.583 -0.975\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n\nIt is easy to transform those link-scale marginal means with arbitrary functions using the transform argument:\n\nmarginal_means(\n    titanic,\n    type = \"link\",\n    transform = insight::link_inverse(titanic),\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  Sex    female 0.838   &lt;0.001 49.3 0.775  0.885\n#&gt;  Sex    male   0.208   &lt;0.001 88.3 0.170  0.250\n#&gt;  PClass 1st    0.836   &lt;0.001 29.2 0.750  0.897\n#&gt;  PClass 2nd    0.525    0.637  0.7 0.422  0.626\n#&gt;  PClass 3rd    0.218   &lt;0.001 52.6 0.170  0.274\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n\nmarginal_means() defaults to reporting EMMs for each category individually, without cross-margins:\n\ntitanic2 &lt;- glmer(\n    Survived ~ Sex + PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n\nmarginal_means(\n    titanic2,\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  Sex    female 0.741     0.0240 30.8   &lt;0.001 691.0 0.694  0.788\n#&gt;  Sex    male   0.253     0.0203 12.5   &lt;0.001 116.0 0.213  0.293\n#&gt;  PClass 1st    0.707     0.0289 24.5   &lt;0.001 436.5 0.650  0.763\n#&gt;  PClass 2nd    0.494     0.0287 17.2   &lt;0.001 217.5 0.437  0.550\n#&gt;  PClass 3rd    0.291     0.0268 10.9   &lt;0.001  88.8 0.238  0.344\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can force the cross:\n\nmarginal_means(\n    titanic2,\n    cross = TRUE,\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Mean Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  0.9288     0.0161 57.71   &lt;0.001   Inf 0.8973 0.9604\n#&gt;  0.7819     0.0357 21.93   &lt;0.001 351.8 0.7120 0.8518\n#&gt;  0.5118     0.0458 11.17   &lt;0.001  93.8 0.4220 0.6017\n#&gt;  0.4844     0.0468 10.35   &lt;0.001  81.0 0.3926 0.5761\n#&gt;  0.2051     0.0308  6.66   &lt;0.001  35.1 0.1448 0.2655\n#&gt;  0.0702     0.0135  5.18   &lt;0.001  22.1 0.0436 0.0967\n#&gt; \n#&gt; Columns: Sex, PClass, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#group-averages-with-the-by-argument",
    "href": "articles/marginalmeans.html#group-averages-with-the-by-argument",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.3 Group averages with the by argument",
    "text": "5.3 Group averages with the by argument\nWe can collapse marginal means via averaging using the by argument:\n\ndat &lt;- mtcars\ndat$am &lt;- factor(dat$am)\ndat$vs &lt;- factor(dat$vs)\ndat$cyl &lt;- factor(dat$cyl)\n\nmod &lt;- glm(gear ~ cyl + vs + am, data = dat, family = poisson)\n\nby &lt;- data.frame(\n    by = c(\"(4 & 6)\", \"(4 & 6)\", \"(8)\"),\n    cyl = c(4, 6, 8))\n\nmarginal_means(mod, by = by, variables = \"cyl\")\n#&gt; \n#&gt;       By Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  (4 & 6) 3.86   &lt;0.001 59.1  2.86   5.22\n#&gt;  (8)     3.59   &lt;0.001 18.5  2.11   6.13\n#&gt; \n#&gt; Results averaged over levels of: vs, am, cyl \n#&gt; Columns: by, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\nAnd we can use the hypothesis argument to compare those new collapsed subgroups:\n\nmarginal_means(mod, by = by, variables = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;           Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (4 & 6) - (8) 0.271       1.39 0.195    0.845 0.2 -2.45      3\n#&gt; \n#&gt; Results averaged over levels of: vs, am, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#custom-contrasts-and-linear-combinations",
    "href": "articles/marginalmeans.html#custom-contrasts-and-linear-combinations",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.4 Custom Contrasts and Linear Combinations",
    "text": "5.4 Custom Contrasts and Linear Combinations\nSee the vignette on Custom Contrasts and Combinations",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#tidy-summaries",
    "href": "articles/marginalmeans.html#tidy-summaries",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.5 Tidy summaries",
    "text": "5.5 Tidy summaries\nThe summary, tidy, and glance functions are also available to summarize and manipulate the results:\n\nmm &lt;- marginal_means(mod)\n\ntidy(mm)\n#&gt; # A tibble: 7 √ó 7\n#&gt;   term  value estimate  p.value s.value conf.low conf.high\n#&gt;   &lt;chr&gt; &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 cyl   4         3.83 2.34e- 8    25.4     2.39      6.13\n#&gt; 2 cyl   6         3.90 1.57e-12    39.2     2.67      5.68\n#&gt; 3 cyl   8         3.59 2.66e- 6    18.5     2.11      6.13\n#&gt; 4 vs    0         3.79 1.88e-12    39.0     2.62      5.50\n#&gt; 5 vs    1         3.75 5.82e-10    30.7     2.47      5.69\n#&gt; 6 am    0         3.27 3.96e-15    47.8     2.43      4.40\n#&gt; 7 am    1         4.34 6.83e-19    60.3     3.14      6.01\n\nglance(mm)\n#&gt; # A tibble: 1 √ó 7\n#&gt;     aic   bic r2.nagelkerke  rmse  nobs     F logLik   \n#&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;logLik&gt; \n#&gt; 1  113.  120.         0.672 0.437    32 0.737 -51.50168\n\nsummary(mm)\n#&gt; \n#&gt;  Term Value Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl     4 3.83   &lt;0.001 25.4  2.39   6.13\n#&gt;   cyl     6 3.90   &lt;0.001 39.2  2.67   5.68\n#&gt;   cyl     8 3.59   &lt;0.001 18.5  2.11   6.13\n#&gt;   vs      0 3.79   &lt;0.001 39.0  2.62   5.50\n#&gt;   vs      1 3.75   &lt;0.001 30.7  2.47   5.69\n#&gt;   am      0 3.27   &lt;0.001 47.8  2.43   4.40\n#&gt;   am      1 4.34   &lt;0.001 60.3  3.14   6.01\n#&gt; \n#&gt; Results averaged over levels of: cyl, vs, am \n#&gt; Columns: term, value, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\nThanks to those tidiers, we can also present the results in the style of a regression table using the modelsummary package.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#case-study-multinomial-logit",
    "href": "articles/marginalmeans.html#case-study-multinomial-logit",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.6 Case study: Multinomial Logit",
    "text": "5.6 Case study: Multinomial Logit\nTo begin, we generate data and estimate a large model:\n\nlibrary(nnet)\nlibrary(marginaleffects)\n\nset.seed(1839)\nn &lt;- 1200\nx &lt;- factor(sample(letters[1:3], n, TRUE))\ny &lt;- vector(length = n)\ny[x == \"a\"] &lt;- sample(letters[4:6], sum(x == \"a\"), TRUE)\ny[x == \"b\"] &lt;- sample(letters[4:6], sum(x == \"b\"), TRUE, c(1 / 4, 2 / 4, 1 / 4))\ny[x == \"c\"] &lt;- sample(letters[4:6], sum(x == \"c\"), TRUE, c(1 / 5, 3 / 5, 2 / 5))\n\ndat &lt;- data.frame(x = x, y = factor(y))\ntmp &lt;- as.data.frame(replicate(20, factor(sample(letters[7:9], n, TRUE))))\ndat &lt;- cbind(dat, tmp)\nvoid &lt;- capture.output({\n    mod &lt;- multinom(y ~ ., dat)\n})\n\nTry to compute marginal means, but realize that your grid won‚Äôt fit in memory:\n\nmarginal_means(mod, type = \"probs\")\n#&gt; Error: You are trying to create a prediction grid with more than 1 billion rows, which is likely to exceed the memory and computational power available on your local machine. Presumably this is because you are considering many variables with many levels. All of the functions in the `marginaleffects` package include arguments to specify a restricted list of variables over which to create a prediction grid.\n\nUse the variables and variables_grid arguments to compute marginal means over a more reasonably sized grid:\n\nmarginal_means(mod,\n              type = \"probs\",\n              variables = c(\"x\", \"V1\"),\n              variables_grid = paste0(\"V\", 2:3))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/marginalmeans.html#plot-conditional-marginal-means",
    "href": "articles/marginalmeans.html#plot-conditional-marginal-means",
    "title": "\n5¬† Marginal Means\n",
    "section": "\n5.7 Plot conditional marginal means",
    "text": "5.7 Plot conditional marginal means\nThe marginaleffects package offers several functions to plot how some quantities vary as a function of others:\n\n\nplot_predictions: Conditional adjusted predictions ‚Äì how does the predicted outcome change as a function of regressors?\n\nplot_comparisons: Conditional comparisons ‚Äì how do contrasts change as a function of regressors?\n\nplot_slopes: Conditional marginal effects ‚Äì how does the slope change as a function of regressors?\n\nThere is no analogous function for marginal means. However, it is very easy to achieve a similar effect using the predictions() function, its by argument, and standard plotting functions. In the example below, we take these steps:\n\nEstimate a model with one continuous (hp) and one categorical regressor (cyl).\nCreate a perfectly ‚Äúbalanced‚Äù data grid for each combination of hp and cyl. This is specified by the user in the datagrid() call.\nCompute fitted values (aka ‚Äúadjusted predictions‚Äù) for each cell of the grid.\nUse the by argument to take the average of predicted values for each value of hp, across margins of cyl.\nCompute standard errors around the averaged predicted values (i.e., marginal means).\nCreate symmetric confidence intervals in the usual manner.\nPlot the results.\n\n\nlibrary(ggplot2)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\np &lt;- predictions(mod,\n    by = \"hp\",\n    newdata = datagrid(\n        model = mod,\n        hp = seq(100, 120, length.out = 10),\n        cyl = mtcars$cyl))\n\nggplot(p) +\n    geom_ribbon(aes(hp, ymin = conf.low, ymax = conf.high), alpha = .2) +\n    geom_line(aes(hp, estimate))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Marginal Means</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#predictions",
    "href": "articles/plot.html#predictions",
    "title": "\n6¬† Plots\n",
    "section": "\n6.1 Predictions",
    "text": "6.1 Predictions\n\n6.1.1 Conditional predictions\nWe call a prediction ‚Äúconditional‚Äù when it is made on a grid of user-specified values. For example, we predict penguins‚Äô body mass for different values of flipper length and species:\n\npre &lt;- predictions(mod, newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\npre\n#&gt; \n#&gt;  flipper_length_mm   species Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;                172 Adelie        3859        204 18.9   &lt;0.001 263.0  3460   4259\n#&gt;                172 Chinstrap     3146        234 13.5   &lt;0.001 134.6  2688   3604\n#&gt;                172 Gentoo        2545        369  6.9   &lt;0.001  37.5  1822   3268\n#&gt;                231 Adelie        4764        362 13.2   &lt;0.001 128.9  4054   5474\n#&gt;                231 Chinstrap     4086        469  8.7   &lt;0.001  58.1  3166   5006\n#&gt;                231 Gentoo        5597        155 36.0   &lt;0.001 940.9  5292   5901\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, body_mass_g, bill_length_mm, island, flipper_length_mm, species \n#&gt; Type:  response\n\nThe condition argument of the plot_predictions() function can be used to build meaningful grids of predictor values somewhat more easily:\n\nplot_predictions(mod, condition = c(\"flipper_length_mm\", \"species\"))\n\n\n\n\nNote that the values at each end of the x-axis correspond to the numerical results produced above. For example, the predicted outcome for a Gentoo with 231mm flippers is 5597.\nWe can include a 3rd conditioning variable, specify what values we want to consider, supply R functions to compute summaries, and use one of several string shortcuts for common reference values (‚Äúthreenum‚Äù, ‚Äúminmax‚Äù, ‚Äúquartile‚Äù, etc.):\n\nplot_predictions(\n    mod,\n    condition = list(\n        \"flipper_length_mm\" = 180:220,\n        \"bill_length_mm\" = \"threenum\",\n        \"species\" = unique))\n\n\n\n\nSee ?plot_predictions for more information.\n\n6.1.2 Marginal predictions\nWe call a prediction ‚Äúmarginal‚Äù when it is the result of a two step process: (1) make predictions for each observed unit in the original dataset, and (2) average predictions across one or more categorical predictors. For example:\n\npredictions(mod, by = \"species\")\n#&gt; \n#&gt;    species Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Adelie        3701       27.2 136.1   &lt;0.001 Inf  3647   3754\n#&gt;  Chinstrap     3733       40.5  92.2   &lt;0.001 Inf  3654   3812\n#&gt;  Gentoo        5076       30.1 168.5   &lt;0.001 Inf  5017   5135\n#&gt; \n#&gt; Columns: species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can plot those predictions by using the analogous command:\n\nplot_predictions(mod, by = \"species\")\n\n\n\n\nWe can also make predictions at the intersections of different variables:\n\npredictions(mod, by = c(\"species\", \"island\"))\n#&gt; \n#&gt;    species    island Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Adelie    Biscoe        3710       50.4  73.7   &lt;0.001 Inf  3611   3808\n#&gt;  Adelie    Dream         3688       44.6  82.6   &lt;0.001 Inf  3601   3776\n#&gt;  Adelie    Torgersen     3706       46.8  79.2   &lt;0.001 Inf  3615   3798\n#&gt;  Chinstrap Dream         3733       40.5  92.2   &lt;0.001 Inf  3654   3812\n#&gt;  Gentoo    Biscoe        5076       30.1 168.5   &lt;0.001 Inf  5017   5135\n#&gt; \n#&gt; Columns: species, island, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that certain species only live on certain islands. Visually:\n\nplot_predictions(mod, by = c(\"species\", \"island\"))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#comparisons",
    "href": "articles/plot.html#comparisons",
    "title": "\n6¬† Plots\n",
    "section": "\n6.2 Comparisons",
    "text": "6.2 Comparisons\n\n6.2.1 Conditional comparisons\nThe syntax for conditional comparisons is the same as the syntax for conditional predictions, except that we now need to specify the variable(s) of interest using an additional argument:\n\ncomparisons(mod,\n  variables = \"flipper_length_mm\",\n  newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\n#&gt; \n#&gt;               Term Contrast flipper_length_mm   species Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % bill_length_mm island\n#&gt;  flipper_length_mm       +1               172 Adelie        15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               172 Chinstrap     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               172 Gentoo        51.7       8.70 5.95   &lt;0.001 28.5 34.68   68.8           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Adelie        15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Chinstrap     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Gentoo        51.7       8.70 5.95   &lt;0.001 28.5 34.68   68.8           43.9 Biscoe\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, flipper_length_mm, species, predicted_lo, predicted_hi, predicted, body_mass_g, bill_length_mm, island \n#&gt; Type:  response\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  condition = c(\"bill_length_mm\", \"species\"))\n\n\n\n\nWe can specify custom comparisons, as we would using the variables argument of the comparisons() function. For example, see what happens to the predicted outcome when flipper_length_mm increases by 1 standard deviation or by 10mm:\n\nplot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = \"sd\"),\n  condition = c(\"bill_length_mm\", \"species\")) +\n\nplot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = 10),\n  condition = c(\"bill_length_mm\", \"species\"))\n\n\n\n\nNotice that the vertical scale is different in the plots above, reflecting the fact that we are plotting the effect of a change of 1 standard deviation on the left vs 10 units on the right.\nLike the comparisons() function, plot_comparisons() is a very powerful tool because it allows us to compute and display custom comparisons such as differences, ratios, odds, lift, and arbitrary functions of predicted outcomes. For example, if we want to plot the ratio of predicted body mass for different species of penguins, we could do:\n\nplot_comparisons(mod,\n  variables = \"species\",\n  condition = \"bill_length_mm\",\n  comparison = \"ratio\")\n\n\n\n\nThe left panel shows that the ratio of Chinstrap body mass to Adelie body mass is approximately constant, at slightly above 0.8. The right panel shows that the ratio of Gentoo to Adelie body mass is depends on their bill length. For birds with short bills, Gentoos seem to have smaller body mass than Adelies. For birds with long bills, Gentoos seem heavier than Adelies, although the null ratio (1) is not outside the confidence interval.\n\n6.2.2 Marginal comparisons\nAs above, we can also display marginal comparisons, by subgroups:\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = \"species\") +\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = c(\"species\", \"island\"))\n\n\n\n\nMultiple contrasts at once:\n\nplot_comparisons(mod,\n  variables = c(\"flipper_length_mm\", \"bill_length_mm\"),\n  by = c(\"species\", \"island\"))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#slopes",
    "href": "articles/plot.html#slopes",
    "title": "\n6¬† Plots\n",
    "section": "\n6.3 Slopes",
    "text": "6.3 Slopes\nIf you have read the sections above, the behavior of the plot_slopes() function should not surprise. Here we give two examples in which we compute display the elasticity of body mass with respect to bill length:\n\n## conditional\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  condition = c(\"species\", \"island\"))\n\n\n\n\n## marginal\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  by = c(\"species\", \"island\"))\n\n\n\n\nAnd here is an example of a marginal effects (aka ‚Äúslopes‚Äù or ‚Äúpartial derivatives‚Äù) plot for a model with multiplicative interactions between continuous variables:\n\nmod2 &lt;- lm(mpg ~ wt * qsec * factor(gear), data = mtcars)\n\nplot_slopes(mod2, variables = \"qsec\", condition = c(\"wt\", \"gear\"))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#uncertainty-estimates",
    "href": "articles/plot.html#uncertainty-estimates",
    "title": "\n6¬† Plots\n",
    "section": "\n6.4 Uncertainty estimates",
    "text": "6.4 Uncertainty estimates\nAs with all the other functions in the package, the plot_*() functions have a conf_level argument and a vcov argument which can be used to control the size of confidence intervals and the types of standard errors used:\n\nplot_slopes(mod,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## clustered standard errors\nplot_slopes(mod,\n  vcov = ~island,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## alpha level\nplot_slopes(mod,\n  conf_level = .8,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#customization",
    "href": "articles/plot.html#customization",
    "title": "\n6¬† Plots\n",
    "section": "\n6.5 Customization",
    "text": "6.5 Customization\nA very useful feature of the plotting functions in this package is that they produce normal ggplot2 objects. So we can customize them to our heart‚Äôs content, using ggplot2 itself, or one of the many packages designed to augment its functionalities:\n\nlibrary(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp * factor(cyl), data = mt)\n\nplot_predictions(mod, condition = c(\"hp\", \"cyl\"), points = .5, rug = TRUE, vcov = FALSE) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nAll the plotting functions work with all the model supported by the marginaleffects package, so we can plot the output of a logistic regression model. This plot shows the probability of survival aboard the Titanic, for different ages and different ticket classes:\n\nlibrary(ggdist)\nlibrary(ggplot2)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\n\nmod &lt;- glm(Survived ~ Age * SexCode * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_dots(\n        alpha = .8,\n        scale = .3,\n        pch = 18,\n        data = dat, aes(\n        x = Age,\n        y = Survived,\n        side = ifelse(Survived == 1, \"bottom\", \"top\")))\n\n\n\n\nThanks to Andrew Heiss who inspired this plot.\nDesigning effective data visualizations requires a lot of customization to the specific context and data. The plotting functions in marginaleffects offer a powerful way to iterate quickly between plots and models, but they obviously cannot support all the features that users may want. Thankfully, it is very easy to use the slopes() functions to generate datasets that can then be used in ggplot2 or any other data visualization tool. Just use the draw argument:\n\np &lt;- plot_predictions(mod, condition = c(\"Age\", \"PClass\"), draw = FALSE)\nhead(p)\n#&gt;   rowid  estimate      p.value   s.value  conf.low conf.high  Survived   SexCode     Age PClass\n#&gt; 1     1 0.8679723 0.0013307148  9.553583 0.6754794 0.9540527 0.4140212 0.3809524 0.17000    1st\n#&gt; 2     2 0.8956789 0.0001333343 12.872665 0.7401973 0.9627887 0.4140212 0.3809524 0.17000    2nd\n#&gt; 3     3 0.4044513 0.2667759176  1.906300 0.2554245 0.5734603 0.4140212 0.3809524 0.17000    3rd\n#&gt; 4     4 0.8631027 0.0011563592  9.756195 0.6749549 0.9503543 0.4140212 0.3809524 1.61551    1st\n#&gt; 5     5 0.8813224 0.0001728858 12.497893 0.7228530 0.9548415 0.4140212 0.3809524 1.61551    2nd\n#&gt; 6     6 0.3934924 0.1899483119  2.396321 0.2535791 0.5533716 0.4140212 0.3809524 1.61551    3rd\n\nThis allows us to feed the data easily to other functions, such as those in the useful ggdist and distributional packages:\n\nlibrary(ggdist)\nlibrary(distributional)\nplot_slopes(mod, variables = \"SexCode\", condition = c(\"Age\", \"PClass\"), type = \"link\", draw = FALSE) |&gt;\n  ggplot() +\n  stat_lineribbon(aes(\n    x = Age,\n    ydist = dist_normal(mu = estimate, sigma = std.error),\n    fill = PClass),\n    alpha = 1 / 4)",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#fits-and-smooths",
    "href": "articles/plot.html#fits-and-smooths",
    "title": "\n6¬† Plots\n",
    "section": "\n6.6 Fits and smooths",
    "text": "6.6 Fits and smooths\nWe can compare the model predictors with fits and smoothers using the geom_smooth() function from the ggplot2 package:\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\nmod &lt;- glm(Survived ~ Age * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_smooth(data = dat, aes(Age, Survived), method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(data = dat, aes(Age, Survived), se = FALSE, color = \"black\")",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#groups-and-categorical-outcomes",
    "href": "articles/plot.html#groups-and-categorical-outcomes",
    "title": "\n6¬† Plots\n",
    "section": "\n6.7 Groups and categorical outcomes",
    "text": "6.7 Groups and categorical outcomes\nIn some models, marginaleffects functions generate different estimates for different groups, such as categorical outcomes. For example,\n\nlibrary(MASS)\nmod &lt;- polr(factor(gear) ~ mpg + hp, data = mtcars)\n\npredictions(mod)\n#&gt; \n#&gt;  Group Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;      3   0.5316     0.1127 4.72   &lt;0.001 18.7  0.3107  0.753\n#&gt;      3   0.5316     0.1127 4.72   &lt;0.001 18.7  0.3107  0.753\n#&gt;      3   0.4492     0.1200 3.74   &lt;0.001 12.4  0.2140  0.684\n#&gt;      3   0.4944     0.1111 4.45   &lt;0.001 16.8  0.2765  0.712\n#&gt;      3   0.4213     0.1142 3.69   &lt;0.001 12.1  0.1973  0.645\n#&gt; --- 86 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;      5   0.6894     0.1957 3.52   &lt;0.001 11.2  0.3059  1.073\n#&gt;      5   0.1650     0.1290 1.28   0.2009  2.3 -0.0878  0.418\n#&gt;      5   0.1245     0.0698 1.78   0.0744  3.7 -0.0123  0.261\n#&gt;      5   0.3779     0.3243 1.17   0.2439  2.0 -0.2578  1.014\n#&gt;      5   0.0667     0.0458 1.46   0.1455  2.8 -0.0231  0.157\n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp \n#&gt; Type:  probs\n\nWe can plot those estimates in the same way as before, by specifying group as one of the conditional variable, or by adding that column to a facet_wrap() call:\n\nplot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\", vcov = FALSE)\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"probs\", vcov = FALSE) +\n  facet_wrap(~ group)",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/plot.html#plot-and-marginaleffects-objects",
    "href": "articles/plot.html#plot-and-marginaleffects-objects",
    "title": "\n6¬† Plots\n",
    "section": "\n6.8 plot() and marginaleffects objects",
    "text": "6.8 plot() and marginaleffects objects\nSome users may feel inclined to call plot() on a object produced by marginaleffects object. Doing so will generate an informative error like this one:\n\nmod &lt;- lm(mpg ~ hp * wt * factor(cyl), data = mtcars)\np &lt;- predictions(mod)\nplot(p)\n#&gt; Error: Please use the `plot_predictions()` function.\n\nThe reason for this error is that the user query is underspecified. marginaleffects allows users to compute so many quantities of interest that it is not clear what the user wants when they simply call plot(). Adding several new arguments would compete with the main plotting functions, and risk sowing confusion. The marginaleffects developers thus decided to support one main path to plotting: plot_predictions(), plot_comparisons(), and plot_slopes().\nThat said, it may be useful to remind users that all marginaleffects output are standard ‚Äútidy‚Äù data frames. Although they get pretty-printed to the console, all the listed columns are accessible via standard R operators. For example:\n\np &lt;- avg_predictions(mod, by = \"cyl\")\np\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    4     26.7      0.695 38.4   &lt;0.001   Inf  25.3   28.0\n#&gt;    6     19.7      0.871 22.7   &lt;0.001 375.1  18.0   21.5\n#&gt;    8     15.1      0.616 24.5   &lt;0.001 438.2  13.9   16.3\n#&gt; \n#&gt; Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate\n#&gt; [1] 26.66364 19.74286 15.10000\n\np$std.error\n#&gt; [1] 0.6951236 0.8713835 0.6161612\n\np$conf.low\n#&gt; [1] 25.30122 18.03498 13.89235\n\nThis allows us to plot all results very easily with standard plotting functions:\n\nplot_predictions(mod, by = \"cyl\")\n\n\n\n\nplot(p$cyl, p$estimate)\n\n\n\n\nggplot(p, aes(x = cyl, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange()",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Plots</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#null-hypothesis",
    "href": "articles/hypothesis.html#null-hypothesis",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.1 Null hypothesis",
    "text": "7.1 Null hypothesis\nThe simplest way to modify a hypothesis test is to change the null hypothesis. By default, all functions in the marginaleffects package assume that the null is 0. This can be changed by changing the hypothesis argument.\nFor example, consider a logistic regression model:\n\nlibrary(marginaleffects)\nmod &lt;- glm(am ~ hp + drat, data = mtcars, family = binomial)\n\nWe can compute the predicted outcome for a hypothetical unit where all regressors are fixed to their sample means:\n\npredictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S  2.5 % 97.5 %  hp drat\n#&gt;     0.231    0.135 2.9 0.0584  0.592 147  3.6\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat \n#&gt; Type:  invlink(link)\n\nThe Z statistic and p value reported above assume that the null hypothesis equals zero. We can change the null with the hypothesis argument:\n\npredictions(mod, newdata = \"mean\", hypothesis = .5)\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S  2.5 % 97.5 %  hp drat\n#&gt;     0.231   0.0343 4.9 0.0584  0.592 147  3.6\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat \n#&gt; Type:  invlink(link)\n\nThis can obviously be useful in other contexts. For instance, if we compute risk ratios (at the mean) associated with an increase of 1 unit in hp, it makes more sense to test the null hypothesis that the ratio of predictions is 1 rather than 0:\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    variables = \"hp\",\n    comparison = \"ratio\",\n    hypothesis = 1) |&gt;\n    print(digits = 3)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %  hp drat\n#&gt;    hp       +1     1.01    0.00793 1.05    0.293 1.8 0.993   1.02 147  3.6\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp, drat \n#&gt; Type:  response\n\nWarning: Z statistics and p values are computed before applying functions in transform.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#hypothesis-tests-with-the-delta-method",
    "href": "articles/hypothesis.html#hypothesis-tests-with-the-delta-method",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.2 Hypothesis tests with the delta method",
    "text": "7.2 Hypothesis tests with the delta method\nThe marginaleffects package includes a powerful function called hypotheses(). This function emulates the behavior of the well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models, requires fewer dependencies, and offers some convenience features like shortcuts for robust standard errors.\nhypotheses() can be used to compute estimates and standard errors of arbitrary functions of model parameters. For example, it can be used to conduct tests of equality between coefficients, or to test the value of some linear or non-linear combination of quantities of interest. hypotheses() can also be used to conduct hypothesis tests on other functions of a model‚Äôs parameter, such as adjusted predictions or marginal effects.\nLet‚Äôs start by estimating a simple model:\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\nWhen the FUN and hypothesis arguments of hypotheses() equal NULL (the default), the function returns a data.frame of raw estimates:\n\nhypotheses(mod)\n#&gt; \n#&gt;          Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n#&gt;  (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n#&gt;  hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n#&gt;  wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n#&gt;  factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n#&gt;  factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nTest of equality between coefficients:\n\nhypotheses(mod, \"hp = wt\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNon-linear function of coefficients\n\nhypotheses(mod, \"exp(hp + wt) = 0.1\")\n#&gt; \n#&gt;                Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;  exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe vcov argument behaves in the same was as in the slopes() function. It allows us to easily compute robust standard errors:\n\nhypotheses(mod, \"hp = wt\", vcov = \"HC3\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can use shortcuts like b1, b2, ... to identify the position of each parameter in the output of FUN. For example, b2=b3 is equivalent to hp=wt because those term names appear in the 2nd and 3rd row when we call hypotheses(mod).\n\nhypotheses(mod, \"b2 = b3\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\nhypotheses(mod, hypothesis = \"b* / b3 = 1\")\n#&gt; \n#&gt;         Term  Estimate Std. Error         z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  b1 / b3 = 1 -12.26735    2.07340   -5.9165   &lt;0.001 28.2 -16.33 -8.204\n#&gt;  b2 / b3 = 1  -0.99273    0.00413 -240.5539   &lt;0.001  Inf  -1.00 -0.985\n#&gt;  b3 / b3 = 1   0.00000         NA        NA       NA   NA     NA     NA\n#&gt;  b4 / b3 = 1   0.05583    0.58287    0.0958    0.924  0.1  -1.09  1.198\n#&gt;  b5 / b3 = 1   0.00141    0.82981    0.0017    0.999  0.0  -1.62  1.628\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nTerm names with special characters must be enclosed in backticks:\n\nhypotheses(mod, \"`factor(cyl)6` = `factor(cyl)8`\")\n#&gt; \n#&gt;                             Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41   3.07\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n7.2.1 Arbitrary functions: FUN\n\nThe FUN argument can be used to compute standard errors for arbitrary functions of model parameters. This user-supplied function must accept a single model object, and return a numeric vector or a data.frame with two columns named term and estimate.\n\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) {\n    out &lt;- x$coefficients[\"hp\"] + x$coefficients[\"mpg\"]\n    return(out)\n}\nhypotheses(mod, FUN = f)\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;     1     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWith labels:\n\nf &lt;- function(x) {\n    out &lt;- data.frame(\n        term = \"Horsepower + Miles per Gallon\",\n        estimate = x$coefficients[\"hp\"] + x$coefficients[\"mpg\"]\n    )\n    return(out)\n}\nhypotheses(mod, FUN = f)\n#&gt; \n#&gt;                           Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Horsepower + Miles per Gallon     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nTest of equality between two predictions (row 2 vs row 3):\n\nf &lt;- function(x) predict(x, newdata = mtcars)\nhypotheses(mod, FUN = f, hypothesis = \"b2 = b3\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  b2 = b3    -1.33      0.616 -2.16   0.0305 5.0 -2.54 -0.125\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that we specified the newdata argument in the f function. This is because the predict() method associated with lm objects will automatically the original fitted values when newdata is NULL, instead of returning the slightly altered fitted values which we need to compute numerical derivatives in the delta method.\nWe can also use numeric vectors to specify linear combinations of parameters. For example, there are 3 coefficients in the last model we estimated. To test the null hypothesis that the sum of the 2nd and 3rd coefficients is equal to 0, we can do:\n\nhypotheses(mod, hypothesis = c(0, 1, 1))\n#&gt; \n#&gt;    Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nSee below for more example of how to use string formulas, numeric vectors, or matrices to calculate custom contrasts, linear combinations, and linear or non-linear hypothesis tests.\n\n7.2.2 Arbitrary quantities with data frames\nmarginaleffects can also compute uncertainty estimates for arbitrary quantities hosted in a data frame, as long as the user can supply a variance-covariance matrix. (Thanks to Kyle F Butts for this cool feature and example!)\nSay you run a monte-carlo simulation and you want to perform hypothesis of various quantities returned from each simulation. The quantities are correlated within each draw:\n\n# simulated means and medians\ndraw &lt;- function(i) { \n  x &lt;- rnorm(n = 10000, mean = 0, sd = 1)\n  out &lt;- data.frame(median = median(x), mean =  mean(x))\n  return(out)\n}\nsims &lt;- do.call(\"rbind\", lapply(1:25, draw))\n\n# average mean and average median \ncoeftable &lt;- data.frame(\n  term = c(\"median\", \"mean\"),\n  estimate = c(mean(sims$median), mean(sims$mean))\n)\n\n# variance-covariance\nvcov &lt;- cov(sims)\n\n# is the median equal to the mean?\nhypotheses(\n  coeftable,\n  vcov = vcov,\n  hypothesis = \"median = mean\"\n)\n#&gt; \n#&gt;           Term Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  median = mean  -0.0012    0.00594 -0.201    0.841 0.3 -0.0128 0.0104\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#hypotheses-formulas",
    "href": "articles/hypothesis.html#hypotheses-formulas",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.3 hypotheses() Formulas",
    "text": "7.3 hypotheses() Formulas\nEach of the 4 core functions of the package support a hypothesis argument which behaves similarly to the hypotheses() function. This argument allows users to specify custom hypothesis tests and contrasts, in order to test null hypotheses such as:\n\nThe coefficients \\(\\beta_1\\) and \\(\\beta_2\\) are equal.\nThe marginal effects of \\(X_1\\) and \\(X_2\\) equal.\nThe marginal effect of \\(X\\) when \\(W=0\\) is equal to the marginal effect of \\(X\\) when \\(W=1\\).\nA non-linear function of adjusted predictions is equal to 100.\nThe marginal mean in the control group is equal to the average of marginal means in the other 3 treatment arms.\nCross-level contrasts: In a multinomial model, the effect of \\(X\\) on the 1st outcome level is equal to the effect of \\(X\\) on the 2nd outcome level.\n\n\n7.3.1 Marginal effects\nFor example, let‚Äôs fit a model and compute some marginal effects at the mean:\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ am + vs, data = mtcars)\n\nmfx &lt;- slopes(mod, newdata = \"mean\")\nmfx\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    am    1 - 0     6.07       1.27 4.76   &lt;0.001 19.0  3.57   8.57\n#&gt;    vs    1 - 0     6.93       1.26 5.49   &lt;0.001 24.6  4.46   9.40\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, am, vs \n#&gt; Type:  response\n\nIs the marginal effect of am different from the marginal effect of vs? To answer this question we can run a linear hypothesis test using the hypotheses() function:\n\nhypotheses(mfx, hypothesis = \"am = vs\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nAlternatively, we can specify the hypothesis directly in the original call:\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ am + vs, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"am = vs\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe hypotheses() string can include any valid R expression, so we can run some silly non-linear tests:\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"exp(am) - 2 * vs = -400\")\n#&gt; \n#&gt;               Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  exp(am)-2*vs=-400      817        550 1.49    0.137 2.9  -261   1896\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nBut note that the p¬†values and confidence intervals are calculated using the delta method and are thus based on the assumption that the hypotheses() expression is approximately normally distributed. For (very) non-linear functions of the parameters, this is not realistic, and we get p¬†values with incorrect error rates and confidence intervals with incorrect coverage probabilities. For such hypotheses, it‚Äôs better to calculate the confidence intervals using the bootstrap (see inferences for details):\n\nset.seed(1234)\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"exp(am) - 2 * vs = -400\") |&gt;\n  inferences(method = \"boot\")\n#&gt; \n#&gt;               Term Estimate Std. Error 2.5 % 97.5 %\n#&gt;  exp(am)-2*vs=-400      817       1854   414   6990\n#&gt; \n#&gt; Columns: term, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response\n\nWhile the confidence interval from the delta method is symmetric (equal to the estimate¬†¬±¬†1.96 times the standard error), the more reliable confidence interval from the bootstrap is (here) highly skewed.\n\n7.3.2 Adjusted Predictions\nNow consider the case of adjusted predictions:\n\np &lt;- predictions(\n    mod,\n    newdata = datagrid(am = 0:1, vs = 0:1))\np\n#&gt; \n#&gt;  am vs Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0  0     14.6      0.926 15.8   &lt;0.001 183.4  12.8   16.4\n#&gt;   0  1     21.5      1.130 19.0   &lt;0.001 266.3  19.3   23.7\n#&gt;   1  0     20.7      1.183 17.5   &lt;0.001 224.5  18.3   23.0\n#&gt;   1  1     27.6      1.130 24.4   &lt;0.001 435.0  25.4   29.8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, am, vs \n#&gt; Type:  response\n\nSince there is no term column in the output of the predictions() function, we must use parameter identifiers like b1, b2, etc. to determine which estimates we want to compare:\n\nhypotheses(p, hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1=b2    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nOr directly:\n\npredictions(\n    mod,\n    hypothesis = \"b1 = b2\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1=b2    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate[1] - p$estimate[2]\n#&gt; [1] -6.929365\n\nIn the next section, we will see that we can get equivalent results by using a vector of contrast weights, which will be used to compute a linear combination of estimates:\n\npredictions(\n    mod,\n    hypothesis = c(1, -1, 0, 0),\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;    Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  custom    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThere are many more possibilities:\n\npredictions(\n    mod,\n    hypothesis = \"b1 + b2 = 30\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;      Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1+b2=30     6.12       1.64 3.74   &lt;0.001 12.4  2.91   9.32\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate[1] + p$estimate[2] - 30\n#&gt; [1] 6.118254\n\npredictions(\n    mod,\n    hypothesis = \"(b2 - b1) / (b3 - b2) = 0\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;               Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b2-b1)/(b3-b2)=0    -8.03         17 -0.473    0.636 0.7 -41.3   25.2\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n7.3.3 Average contrasts or marginal effects\nThe standard workflow with the marginaleffects package is to call a function like predictions(), slopes() or comparisons() to compute unit-level quantities; or one of their cousins avg_predictions(), avg_comparisons(), or avg_slopes() to aggregate the unit-level quantities into ‚ÄúAverage Marginal Effects‚Äù or ‚ÄúAverage Contrasts.‚Äù We can also use the comparison argument to emulate the behavior of the avg_*() functions.\nFirst, note that these three commands produce the same results:\n\ncomparisons(mod, variables = \"vs\")$estimate |&gt; mean()\n#&gt; [1] 6.929365\n\navg_comparisons(mod, variables = \"vs\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    vs    1 - 0     6.93       1.26 5.49   &lt;0.001 24.6  4.46    9.4\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(\n    mod,\n    variables = \"vs\",\n    comparison = \"differenceavg\")\n#&gt; \n#&gt;  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    vs mean(1) - mean(0)     6.93       1.26 5.49   &lt;0.001 24.6  4.46    9.4\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nSee the transformations section of the Contrasts vignette for more details.\nWith these results in hand, we can now conduct a linear hypothesis test between average marginal effects:\n\ncomparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    comparison = \"differenceavg\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nComputing contrasts between average marginal effects requires a little care to obtain the right scale. In particular, we need to specify both the variables and the comparison:\n\ncomparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    variables = c(\"am\", \"vs\"),\n    comparison = \"dydxavg\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n7.3.4 Generic Hypothesis for Unsupported S3 Objects\nmarginaleffects provides a generic interface for hypothesis tests for linear models by providing (1) a data.frame containing point estimates (consiting of columns term containing the names and estimate containing the point estiamtes) and (2) a variance-covariance matrix of estimates.\n\ncoeftable &lt;- data.frame(term = names(mod$coefficients), estimate = as.numeric(mod$coefficients))\nvcov &lt;- vcov(mod)\n\nhypotheses(\n  coeftable, vcov = vcov, \n  hypothesis = \"am = vs\"\n)\n#&gt; \n#&gt;     Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am = vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#hypotheses-vectors-and-matrices",
    "href": "articles/hypothesis.html#hypotheses-vectors-and-matrices",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.4 hypotheses() Vectors and Matrices",
    "text": "7.4 hypotheses() Vectors and Matrices\nThe marginal_means() function computes estimated marginal means. The hypothesis argument of that function offers a powerful mechanism to estimate custom contrasts between marginal means, by way of linear combination.\nConsider a simple example:\n\nlibrary(marginaleffects)\nlibrary(emmeans)\nlibrary(nnet)\n\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\n\nmod &lt;- lm(mpg ~ carb + cyl, dat)\nmm &lt;- marginal_means(mod, variables = \"carb\")\nmm\n#&gt; \n#&gt;  Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  carb     1 21.7       1.44 15.06   &lt;0.001 167.8  18.8   24.5\n#&gt;  carb     2 21.3       1.23 17.29   &lt;0.001 220.0  18.9   23.8\n#&gt;  carb     3 21.4       2.19  9.77   &lt;0.001  72.5  17.1   25.7\n#&gt;  carb     4 18.9       1.21 15.59   &lt;0.001 179.7  16.5   21.3\n#&gt;  carb     6 19.8       3.55  5.56   &lt;0.001  25.2  12.8   26.7\n#&gt;  carb     8 20.1       3.51  5.73   &lt;0.001  26.6  13.2   27.0\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, value, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe contrast between marginal means for carb==1 and carb==2 is:\n\n21.66232 - 21.34058 \n#&gt; [1] 0.32174\n\nor\n\n21.66232 + -(21.34058)\n#&gt; [1] 0.32174\n\nor\n\nsum(c(21.66232, 21.34058) * c(1, -1))\n#&gt; [1] 0.32174\n\nor\n\nc(21.66232, 21.34058) %*% c(1, -1)\n#&gt;         [,1]\n#&gt; [1,] 0.32174\n\nThe last two commands express the contrast of interest as a linear combination of marginal means.\n\n7.4.1 Simple contrast\nIn the marginal_means() function, we can supply a hypothesis argument to compute linear combinations of marginal means. This argument must be a numeric vector of the same length as the number of rows in the output of marginal_means(). For example, in the previous there were six rows, and the two marginal means we want to compare are at in the first two positions:\n\nlc &lt;- c(1, -1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;    Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom 0.322       1.77 0.181    0.856 0.2 -3.15    3.8\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n7.4.2 Complex contrast\nOf course, we can also estimate more complex contrasts:\n\nlc &lt;- c(0, -2, 1, 1, -1, 1)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;    Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom -2.02       6.32 -0.32    0.749 0.4 -14.4   10.4\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nemmeans produces similar results:\n\nlibrary(emmeans)\nem &lt;- emmeans(mod, \"carb\")\nlc &lt;- data.frame(custom_contrast = c(-2, 1, 1, 0, -1, 1))\ncontrast(em, method = lc)\n#&gt;  contrast        estimate   SE df t.ratio p.value\n#&gt;  custom_contrast   -0.211 6.93 24  -0.030  0.9760\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n\n\n7.4.3 Multiple contrasts\nUsers can also compute multiple linear combinations simultaneously by supplying a numeric matrix to hypotheses. This matrix must have the same number of rows as the output of slopes(), and each column represents a distinct set of weights for different linear combinations. The column names of the matrix become labels in the output. For example:\n\nlc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    1, -1, 0, 0, 0, 0\n    ), ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nlc\n#&gt;      Contrast A Contrast B\n#&gt; [1,]         -2          1\n#&gt; [2,]          1         -1\n#&gt; [3,]          1          0\n#&gt; [4,]          0          0\n#&gt; [5,]         -1          0\n#&gt; [6,]          1          0\n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;        Term   Mean Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Contrast A -0.211       6.93 -0.0304    0.976 0.0 -13.79   13.4\n#&gt;  Contrast B  0.322       1.77  0.1814    0.856 0.2  -3.15    3.8\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n7.4.4 Contrasts across response levels\nIn models with multinomial outcomes, one may be interested in comparing outcomes or contrasts across response levels. For example, in this model there are 18 estimated marginal means, across 6 outcome levels (the group column):\n\nlibrary(nnet)\nmod &lt;- multinom(carb ~ mpg + cyl, data = dat, trace = FALSE)\nmm &lt;- marginal_means(mod, type = \"probs\")\nmm\n#&gt; \n#&gt;  Group Term Value     Mean Std. Error       z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      1  cyl     4 3.68e-01   2.60e-01 1.41647   0.1566 2.7 -1.41e-01 8.77e-01\n#&gt;      1  cyl     6 2.83e-01   1.96e-01 1.44488   0.1485 2.8 -1.01e-01 6.67e-01\n#&gt;      1  cyl     8 4.63e-04   9.59e-03 0.04824   0.9615 0.1 -1.83e-02 1.93e-02\n#&gt;      2  cyl     4 6.31e-01   2.60e-01 2.42772   0.0152 6.0  1.22e-01 1.14e+00\n#&gt;      2  cyl     6 1.85e-06   2.40e-06 0.77081   0.4408 1.2 -2.85e-06 6.55e-06\n#&gt;      2  cyl     8 6.65e-01   3.74e-01 1.77977   0.0751 3.7 -6.74e-02 1.40e+00\n#&gt;      3  cyl     4 6.85e-04   1.19e-02 0.05748   0.9542 0.1 -2.27e-02 2.40e-02\n#&gt;      3  cyl     6 1.12e-05   1.32e-03 0.00848   0.9932 0.0 -2.58e-03 2.60e-03\n#&gt;      3  cyl     8 3.10e-01   3.71e-01 0.83684   0.4027 1.3 -4.17e-01 1.04e+00\n#&gt;      4  cyl     4 2.12e-04   1.75e-02 0.01211   0.9903 0.0 -3.41e-02 3.45e-02\n#&gt;      4  cyl     6 5.56e-01   2.18e-01 2.55023   0.0108 6.5  1.29e-01 9.84e-01\n#&gt;      4  cyl     8 9.58e-03   2.28e-02 0.42007   0.6744 0.6 -3.51e-02 5.43e-02\n#&gt;      6  cyl     4 8.82e-06   8.39e-05 0.10506   0.9163 0.1 -1.56e-04 1.73e-04\n#&gt;      6  cyl     6 1.61e-01   1.54e-01 1.04698   0.2951 1.8 -1.40e-01 4.62e-01\n#&gt;      6  cyl     8 4.35e-09   9.89e-08 0.04393   0.9650 0.1 -1.90e-07 1.98e-07\n#&gt;      8  cyl     4 1.50e-04   7.97e-03 0.01878   0.9850 0.0 -1.55e-02 1.58e-02\n#&gt;      8  cyl     6 9.29e-06   7.98e-04 0.01164   0.9907 0.0 -1.56e-03 1.57e-03\n#&gt;      8  cyl     8 1.41e-02   4.66e-02 0.30323   0.7617 0.4 -7.72e-02 1.05e-01\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: group, term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nLet‚Äôs contrast the marginal means in the first outcome level when cyl equals 4 and 6. These marginal means are located in rows 1 and 7 respectively:\n\nlc &lt;- rep(0, nrow(mm))\nlc[1] &lt;- -1\nlc[7] &lt;- 1\nmarginal_means(\n    mod,\n    type = \"probs\",\n    hypothesis = lc)\n#&gt; \n#&gt;    Term   Mean Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  custom -0.367       0.26 -1.41    0.158 2.7 -0.877  0.143\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nThis is indeed equal to the results we would have obtained manually:\n\n2.828726e-01 - 3.678521e-01\n#&gt; [1] -0.0849795\n\nNow let‚Äôs say we want to calculate a ‚Äúcontrast in contrasts‚Äù, that is, the outcome of a 3-step process:\n\nContrast between cyl=6 and cyl=4 in the 1st outcome level\nContrast between cyl=6 and cyl=4 in the 2nd outcome level\nContrast between the contrasts defined in steps 1 and 2.\n\nWe create the linear combination weights as follows:\n\nlc &lt;- rep(0, nrow(mm))\nlc[c(1, 8)] &lt;- -1\nlc[c(7, 2)] &lt;- 1\n\nTo make sure that the weights are correct, we can display them side by side with the original marginal_means() output:\n\ntransform(mm[, 1:3], lc = lc)\n#&gt;    group term value lc\n#&gt; 1      1  cyl     4 -1\n#&gt; 2      1  cyl     6  1\n#&gt; 3      1  cyl     8  0\n#&gt; 4      2  cyl     4  0\n#&gt; 5      2  cyl     6  0\n#&gt; 6      2  cyl     8  0\n#&gt; 7      3  cyl     4  1\n#&gt; 8      3  cyl     6 -1\n#&gt; 9      3  cyl     8  0\n#&gt; 10     4  cyl     4  0\n#&gt; 11     4  cyl     6  0\n#&gt; 12     4  cyl     8  0\n#&gt; 13     6  cyl     4  0\n#&gt; 14     6  cyl     6  0\n#&gt; 15     6  cyl     8  0\n#&gt; 16     8  cyl     4  0\n#&gt; 17     8  cyl     6  0\n#&gt; 18     8  cyl     8  0\n\nCompute the results:\n\nmarginal_means(mod, type = \"probs\", hypothesis = lc)\n#&gt; \n#&gt;    Term    Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  custom -0.0843      0.321 -0.263    0.793 0.3 -0.714  0.545\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#pairwise-contrasts-difference-in-differences",
    "href": "articles/hypothesis.html#pairwise-contrasts-difference-in-differences",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.5 Pairwise contrasts: Difference-in-Differences",
    "text": "7.5 Pairwise contrasts: Difference-in-Differences\nNow we illustrate how to use the machinery described above to do pairwise comparisons between contrasts, a type of analysis often associated with a ‚ÄúDifference-in-Differences‚Äù research design.\nFirst, we simulate data with two treatment groups and pre/post periods:\n\nlibrary(data.table)\n\nN &lt;- 1000\ndid &lt;- data.table(\n    id = 1:N,\n    pre = rnorm(N),\n    trt = sample(0:1, N, replace = TRUE))\ndid$post &lt;- did$pre + did$trt * 0.3 + rnorm(N)\ndid &lt;- melt(\n    did,\n    value.name = \"y\",\n    variable.name = \"time\",\n    id.vars = c(\"id\", \"trt\"))\nhead(did)\n#&gt;       id   trt   time           y\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;fctr&gt;       &lt;num&gt;\n#&gt; 1:     1     1    pre -1.04356113\n#&gt; 2:     2     0    pre -0.99460367\n#&gt; 3:     3     0    pre -0.16962798\n#&gt; 4:     4     1    pre -0.01854487\n#&gt; 5:     5     0    pre -1.37156492\n#&gt; 6:     6     0    pre  0.33690893\n\nThen, we estimate a linear model with a multiple interaction between the time and the treatment indicators. We also compute contrasts at the mean for each treatment level:\n\ndid_model &lt;- lm(y ~ time * trt, data = did)\n\ncomparisons(\n    did_model,\n    newdata = datagrid(trt = 0:1),\n    variables = \"time\")\n#&gt; \n#&gt;  Term   Contrast trt Estimate Std. Error      z Pr(&gt;|z|)    S  2.5 % 97.5 % time\n#&gt;  time post - pre   0   -0.035     0.0821 -0.426     0.67  0.6 -0.196  0.126  pre\n#&gt;  time post - pre   1    0.298     0.0792  3.757   &lt;0.001 12.5  0.142  0.453  pre\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, trt, predicted_lo, predicted_hi, predicted, y, time \n#&gt; Type:  response\n\nFinally, we compute pairwise differences between contrasts. This is the Diff-in-Diff estimate:\n\ncomparisons(\n    did_model,\n    variables = \"time\",\n    newdata = datagrid(trt = 0:1),\n    hypothesis = \"pairwise\")\n#&gt; \n#&gt;           Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Row 1 - Row 2   -0.333      0.114 -2.92  0.00356 8.1 -0.556 -0.109\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#joint-hypotheses-tests",
    "href": "articles/hypothesis.html#joint-hypotheses-tests",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.6 Joint hypotheses tests",
    "text": "7.6 Joint hypotheses tests\nThe hypotheses() function can also test multiple hypotheses jointly. For example, consider this model:\n\nmodel &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\ncoef(model)\n#&gt;        (Intercept)    as.factor(cyl)6    as.factor(cyl)8                 hp as.factor(cyl)6:hp as.factor(cyl)8:hp \n#&gt;        35.98302564       -15.30917451       -17.90295193        -0.11277589         0.10516262         0.09853177\n\nWe may want to test the null hypothesis that two of the coefficients are jointly (both) equal to zero.\n\n\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt; as.factor(cyl)6:hp = 0\n#&gt; as.factor(cyl)8:hp = 0\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  2.11    0.142    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\nThe joint argument allows users to flexibly specify the parameters to be tested, using character vectors, integer indices, or Perl-compatible regular expressions. We can also specify the null hypothesis for each parameter individually using the hypothesis argument.\nNaturally, the hypotheses() function also works with marginaleffects objects.\n\n# ## joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 0\n#&gt;  as.factor(cyl)8 = 0\n#&gt;  as.factor(cyl)6:hp = 0\n#&gt;  as.factor(cyl)8:hp = 0\n#&gt;  \n#&gt;    F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  5.7  0.00197    4   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 0\n#&gt;  as.factor(cyl)8 = 0\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  6.12  0.00665    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 1\n#&gt;  as.factor(cyl)8 = 1\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  6.84  0.00411    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 1\n#&gt;  as.factor(cyl)8 = 2\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  7.47  0.00273    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  cyl 6 - 4 = 0\n#&gt;  cyl 8 - 4 = 0\n#&gt;  \n#&gt;    F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  1.6    0.221    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\nWe can also combine multiple calls to hypotheses() to execute a joint test on linear combinations of coefficients:\n\n## fit model\nmod &lt;- lm(mpg ~ factor(carb), mtcars)\n\n## hypothesis matrix for linear combinations\nH &lt;- matrix(0, nrow = length(coef(mod)), ncol = 2)\nH[2:3, 1] &lt;- H[4:6, 2] &lt;- 1\n\n## test individual linear combinations\nhyp &lt;- hypotheses(mod, hypothesis = H)\nhyp\n#&gt; \n#&gt;    Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom    -12.0       4.92 -2.44  0.01477 6.1 -21.6  -2.35\n#&gt;  custom    -25.5       9.03 -2.83  0.00466 7.7 -43.2  -7.85\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n## test joint hypotheses\n#hypotheses(hyp, joint = TRUE, hypothesis = c(-10, -20))",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/hypothesis.html#complex-aggregations",
    "href": "articles/hypothesis.html#complex-aggregations",
    "title": "\n7¬† Hypothesis Tests\n",
    "section": "\n7.7 Complex aggregations",
    "text": "7.7 Complex aggregations\nThe FUN argument of the hypotheses() function can be used to conduct complex aggregations of other estimates. For example, consider this ordered logit model fitted on a dataset of cars:\n\nlibrary(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\nsummary(mod)\n#&gt; Call:\n#&gt; polr(formula = gear ~ factor(cyl) + hp, data = dat)\n#&gt; \n#&gt; Coefficients:\n#&gt;                  Value Std. Error t value\n#&gt; factor(cyl)6  -3.87912    1.52625  -2.542\n#&gt; factor(cyl)8 -14.64228    4.46072  -3.282\n#&gt; hp             0.07269    0.02422   3.001\n#&gt; \n#&gt; Intercepts:\n#&gt;     Value    Std. Error t value \n#&gt; 3|4   3.6824   1.7945     2.0521\n#&gt; 4|5   7.3814   2.3473     3.1445\n#&gt; \n#&gt; Residual Deviance: 34.40969 \n#&gt; AIC: 44.40969\n\nIf we compute fitted values with the predictions() function, we obtain one predicted probability for each individual car and for each level of the response variable:\n\npredictions(mod)\n#&gt; \n#&gt;  Group Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.0440    0.04256   1.03  0.30081 1.7 -0.0394  0.127\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.9963    0.00721 138.17  &lt; 0.001 Inf  0.9822  1.010\n#&gt; --- 86 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;      5   0.6969    0.18931   3.68  &lt; 0.001 12.1  0.3258  1.068\n#&gt;      5   0.0555    0.06851   0.81  0.41775  1.3 -0.0788  0.190\n#&gt;      5   0.8115    0.20626   3.93  &lt; 0.001 13.5  0.4073  1.216\n#&gt;      5   0.9111    0.16818   5.42  &lt; 0.001 24.0  0.5815  1.241\n#&gt;      5   0.6322    0.19648   3.22  0.00129  9.6  0.2471  1.017\n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, cyl, hp \n#&gt; Type:  probs\n\nThere are three levels to the outcome: 3, 4, and 5. Imagine that, for each car in the dataset, we want to collapse categories of the output variable into two categories (‚Äú3 & 4‚Äù and ‚Äú5‚Äù) by taking sums of predicted probabilities. Then, we want to take the average of those predicted probabilities for each level of cyl. To do so, we define a custom function, and pass it to the FUN argument of the hypotheses() function:\n\naggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        # label the new categories of outcome levels\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 & 4\", \"5\")) |&gt;\n        # sum of probabilities at the individual level\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        # average probabilities for each value of `cyl`\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        # the `FUN` argument requires a `term` column\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n#&gt; \n#&gt;  Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3 & 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n#&gt;  3 & 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n#&gt;  3 & 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n#&gt;  5        6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n#&gt;  5        4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n#&gt;  5        8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that this workflow will not work for bayesian models or with bootstrap. However, with those models it is trivial to do the same kind of aggregation by calling posterior_draws() and operating directly on draws from the posterior distribution. See the vignette on bayesian analysis for examples with the posterior_draws() function.",
    "crumbs": [
      "marginaleffects",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#logistic-regression-with-multiplicative-interactions",
    "href": "articles/brms.html#logistic-regression-with-multiplicative-interactions",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.1 Logistic regression with multiplicative interactions",
    "text": "8.1 Logistic regression with multiplicative interactions\nLoad libraries and download data on passengers of the Titanic from the Rdatasets archive:\n\nlibrary(marginaleffects)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/carData/TitanicSurvival.csv\")\ndat$survived &lt;- ifelse(dat$survived == \"yes\", 1, 0)\ndat$woman &lt;- ifelse(dat$sex == \"female\", 1, 0)\n\nFit a logit model with a multiplicative interaction:\n\nmod &lt;- brm(survived ~ woman * age + passengerClass,\n           family = bernoulli(link = \"logit\"),\n           data = dat)\n\n\n8.1.1 Adjusted predictions\nWe can compute adjusted predicted values of the outcome variable (i.e., probability of survival aboard the Titanic) using the predictions() function. By default, this function calculates predictions for each row of the dataset:\n\npredictions(mod)\n#&gt; \n#&gt;  Estimate  2.5 % 97.5 %\n#&gt;    0.9367 0.9070 0.9590\n#&gt;    0.8493 0.7453 0.9187\n#&gt;    0.9433 0.8949 0.9704\n#&gt;    0.5131 0.4302 0.6000\n#&gt;    0.9375 0.9080 0.9601\n#&gt; --- 1036 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;    0.0376 0.0235 0.0581\n#&gt;    0.5859 0.5017 0.6663\n#&gt;    0.1043 0.0801 0.1337\n#&gt;    0.1017 0.0779 0.1307\n#&gt;    0.0916 0.0691 0.1189\n#&gt; Columns: rowid, estimate, conf.low, conf.high, survived, woman, age, passengerClass \n#&gt; Type:  response\n\nTo visualize the relationship between the outcome and one of the regressors, we can plot conditional adjusted predictions with the plot_predictions() function:\n\nplot_predictions(mod, condition = \"age\")\n\n\n\n\nCompute adjusted predictions for some user-specified values of the regressors, using the newdata argument and the datagrid() function:\n\npred &lt;- predictions(mod,\n                    newdata = datagrid(woman = 0:1,\n                                       passengerClass = c(\"1st\", \"2nd\", \"3rd\")))\npred\n#&gt; \n#&gt;  woman passengerClass Estimate  2.5 % 97.5 %\n#&gt;      0            1st   0.5149 0.4319  0.602\n#&gt;      0            2nd   0.2013 0.1536  0.261\n#&gt;      0            3rd   0.0875 0.0656  0.114\n#&gt;      1            1st   0.9364 0.9066  0.959\n#&gt;      1            2nd   0.7783 0.7090  0.835\n#&gt;      1            3rd   0.5701 0.4938  0.644\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, survived, age, woman, passengerClass \n#&gt; Type:  response\n\nThe posterior_draws() function samples from the posterior distribution of the model, and produces a data frame with drawid and draw columns.\n\npred &lt;- posterior_draws(pred)\nhead(pred)\n#&gt;   drawid       draw rowid   estimate   conf.low conf.high  survived      age woman passengerClass\n#&gt; 1      1 0.46566713     1 0.51492993 0.43192231 0.6018749 0.4082218 29.88113     0            1st\n#&gt; 2      1 0.16658900     2 0.20128833 0.15362308 0.2613351 0.4082218 29.88113     0            2nd\n#&gt; 3      1 0.08750961     3 0.08750369 0.06555724 0.1141134 0.4082218 29.88113     0            3rd\n#&gt; 4      1 0.93735755     4 0.93641346 0.90660921 0.9587589 0.4082218 29.88113     1            1st\n#&gt; 5      1 0.77437334     5 0.77829290 0.70896643 0.8346419 0.4082218 29.88113     1            2nd\n#&gt; 6      1 0.62216334     6 0.57010265 0.49377997 0.6441967 0.4082218 29.88113     1            3rd\n\nThis ‚Äúlong‚Äù format makes it easy to plots results:\n\nggplot(pred, aes(x = draw, fill = factor(woman))) +\n    geom_density() +\n    facet_grid(~ passengerClass, labeller = label_both) +\n    labs(x = \"Predicted probability of survival\", y = \"\", fill = \"Woman\")\n\n\n\n\n\n8.1.2 Marginal effects\nUse slopes() to compute marginal effects (slopes of the regression equation) for each row of the dataset, and use ) to compute ‚ÄúAverage Marginal Effects‚Äù, that is, the average of all observation-level marginal effects:\n\nmfx &lt;- slopes(mod)\nmfx\n#&gt; \n#&gt;   Term Contrast  Estimate     2.5 %    97.5 %\n#&gt;  age      dY/dX -0.000237 -0.001336  0.000880\n#&gt;  age      dY/dX -0.007258 -0.008974 -0.005266\n#&gt;  age      dY/dX -0.000214 -0.000832  0.001241\n#&gt;  age      dY/dX -0.014258 -0.018487 -0.010306\n#&gt;  age      dY/dX -0.000234 -0.001242  0.000923\n#&gt; --- 4174 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n#&gt;  woman    1 - 0  0.516022  0.401674  0.630788\n#&gt;  woman    1 - 0  0.395843  0.307400  0.486515\n#&gt;  woman    1 - 0  0.468892  0.401425  0.536243\n#&gt;  woman    1 - 0  0.471069  0.403598  0.538028\n#&gt;  woman    1 - 0  0.478699  0.410060  0.547549\n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx, survived, woman, age, passengerClass \n#&gt; Type:  response\n\nCompute marginal effects with some regressors fixed at user-specified values, and other regressors held at their means:\n\nslopes(\n    mod,\n    newdata = datagrid(\n        woman = 1,\n        passengerClass = \"1st\"))\n#&gt; \n#&gt;            Term  Contrast woman passengerClass  Estimate    2.5 %    97.5 %\n#&gt;  age            dY/dX         1            1st -0.000238 -0.00136  0.000871\n#&gt;  passengerClass 2nd - 1st     1            1st -0.157442 -0.22327 -0.102890\n#&gt;  passengerClass 3rd - 1st     1            1st -0.365376 -0.43832 -0.294769\n#&gt;  woman          1 - 0         1            1st  0.420368  0.34697  0.490373\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, woman, passengerClass, predicted_lo, predicted_hi, predicted, tmp_idx, survived, age \n#&gt; Type:  response\n\nCompute and plot conditional marginal effects:\n\nplot_slopes(mod, variables = \"woman\", condition = \"age\")\n\n\n\n\nThe posterior_draws() produces a dataset with drawid and draw columns:\n\ndraws &lt;- posterior_draws(mfx)\n\ndim(draws)\n#&gt; [1] 16736000       16\n\nhead(draws)\n#&gt;   drawid          draw rowid term contrast      estimate     conf.low     conf.high predicted_lo predicted_hi predicted tmp_idx survived woman     age passengerClass\n#&gt; 1      1 -0.0001793469     1  age    dY/dX -0.0002373870 -0.001335541  0.0008802815    0.9366604    0.9366565 0.9366604       1        1     1 29.0000            1st\n#&gt; 2      1 -0.0082474471     2  age    dY/dX -0.0072583027 -0.008974296 -0.0052661472    0.8493050    0.8492454 0.8493050       2        1     0  0.9167            1st\n#&gt; 3      1 -0.0001667673     3  age    dY/dX -0.0002137479 -0.000831540  0.0012414384    0.9433293    0.9433241 0.9433293       3        0     1  2.0000            1st\n#&gt; 4      1 -0.0160431699     4  age    dY/dX -0.0142579781 -0.018486516 -0.0103056218    0.5131011    0.5130018 0.5131011       4        0     0 30.0000            1st\n#&gt; 5      1 -0.0001774337     5  age    dY/dX -0.0002336818 -0.001242047  0.0009233022    0.9374937    0.9374916 0.9374937       5        0     1 25.0000            1st\n#&gt; 6      1 -0.0108157843     6  age    dY/dX -0.0112749252 -0.014317015 -0.0085773461    0.2730542    0.2729654 0.2730542       6        1     0 48.0000            1st\n\nWe can use this dataset to plot our results. For example, to plot the posterior density of the marginal effect of age when the woman variable is equal to 0 or 1:\n\nmfx &lt;- slopes(mod,\n    variables = \"age\",\n    newdata = datagrid(woman = 0:1)) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(woman))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal Effect of Age on Survival\",\n         y = \"Posterior density\",\n         fill = \"Woman\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#random-effects-model",
    "href": "articles/brms.html#random-effects-model",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.2 Random effects model",
    "text": "8.2 Random effects model\nThis section replicates some of the analyses of a random effects model published in Andrew Heiss‚Äô blog post: ‚ÄúA guide to correctly calculating posterior predictions and average marginal effects with multilevel Bayesian models.‚Äù The objective is mainly to illustrate the use of marginaleffects. Please refer to the original post for a detailed discussion of the quantities computed below.\nLoad libraries and download data:\n\nlibrary(brms)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nvdem_2015 &lt;- read.csv(\"https://github.com/vincentarelbundock/marginaleffects/raw/main/data-raw/vdem_2015.csv\")\n\nhead(vdem_2015)\n#&gt;   country_name country_text_id year                           region media_index party_autonomy_ord polyarchy civil_liberties party_autonomy\n#&gt; 1       Mexico             MEX 2015  Latin America and the Caribbean       0.837                  3     0.631           0.704           TRUE\n#&gt; 2     Suriname             SUR 2015  Latin America and the Caribbean       0.883                  4     0.777           0.887           TRUE\n#&gt; 3       Sweden             SWE 2015 Western Europe and North America       0.956                  4     0.915           0.968           TRUE\n#&gt; 4  Switzerland             CHE 2015 Western Europe and North America       0.939                  4     0.901           0.960           TRUE\n#&gt; 5        Ghana             GHA 2015               Sub-Saharan Africa       0.858                  4     0.724           0.921           TRUE\n#&gt; 6 South Africa             ZAF 2015               Sub-Saharan Africa       0.898                  4     0.752           0.869           TRUE\n\nFit a basic model:\n\nmod &lt;- brm(\n  bf(media_index ~ party_autonomy + civil_liberties + (1 | region),\n     phi ~ (1 | region)),\n  data = vdem_2015,\n  family = Beta(),\n  control = list(adapt_delta = 0.9))\n\n\n8.2.1 Posterior predictions\nTo compute posterior predictions for specific values of the regressors, we use the newdata argument and the datagrid() function. We also use the type argument to compute two types of predictions: accounting for residual (observation-level) residual variance (prediction) or ignoring it (response).\n\nnd = datagrid(model = mod,\n              party_autonomy = c(TRUE, FALSE),\n              civil_liberties = .5,\n              region = \"Middle East and North Africa\")\np1 &lt;- predictions(mod, type = \"response\", newdata = nd) |&gt;\n    posterior_draws() |&gt;\n    transform(type = \"Response\")\np2 &lt;- predictions(mod, type = \"prediction\", newdata = nd) |&gt;\n    posterior_draws() |&gt;\n    transform(type = \"Prediction\")\npred &lt;- rbind(p1, p2)\n\nExtract posterior draws and plot them:\n\nggplot(pred, aes(x = draw, fill = party_autonomy)) +\n    stat_halfeye(alpha = .5) +\n    facet_wrap(~ type) +\n    labs(x = \"Media index (predicted)\", \n         y = \"Posterior density\",\n         fill = \"Party autonomy\")\n\n\n\n\n\n8.2.2 Marginal effects and contrasts\nAs noted in the Marginal Effects vignette, there should be one distinct marginal effect for each combination of regressor values. Here, we consider only one combination of regressor values, where region is ‚ÄúMiddle East and North Africa‚Äù, and civil_liberties is 0.5. Then, we calculate the mean of the posterior distribution of marginal effects:\n\nmfx &lt;- slopes(mod,\n                       newdata = datagrid(civil_liberties = .5,\n                                          region = \"Middle East and North Africa\"))\nmfx\n#&gt; \n#&gt;             Term     Contrast civil_liberties                       region Estimate 2.5 % 97.5 %\n#&gt;  civil_liberties dY/dX                    0.5 Middle East and North Africa    0.816 0.621  1.007\n#&gt;  party_autonomy  TRUE - FALSE             0.5 Middle East and North Africa    0.252 0.166  0.336\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, civil_liberties, region, predicted_lo, predicted_hi, predicted, tmp_idx, media_index, party_autonomy \n#&gt; Type:  response\n\nUse the posterior_draws() to extract draws from the posterior distribution of marginal effects, and plot them:\n\nmfx &lt;- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, y = term)) +\n  stat_halfeye() +\n  labs(x = \"Marginal effect\", y = \"\")\n\n\n\n\nPlot marginal effects, conditional on a regressor:\n\nplot_slopes(mod,\n         variables = \"civil_liberties\",\n         condition = \"party_autonomy\")\n\n\n\n\n\n8.2.3 Continuous predictors\n\npred &lt;- predictions(mod,\n                    newdata = datagrid(party_autonomy = FALSE,\n                                       region = \"Middle East and North Africa\",\n                                       civil_liberties = seq(0, 1, by = 0.05))) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n\n\n\n\nThe slope of this line for different values of civil liberties can be obtained with:\n\nmfx &lt;- slopes(mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\")\nmfx\n#&gt; \n#&gt;             Term civil_liberties party_autonomy                       region Estimate 2.5 % 97.5 %\n#&gt;  civil_liberties             0.2          FALSE Middle East and North Africa    0.490 0.361  0.639\n#&gt;  civil_liberties             0.5          FALSE Middle East and North Africa    0.807 0.612  0.993\n#&gt;  civil_liberties             0.8          FALSE Middle East and North Africa    0.807 0.674  0.934\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, civil_liberties, party_autonomy, region, predicted_lo, predicted_hi, predicted, tmp_idx, media_index \n#&gt; Type:  response\n\nAnd plotted:\n\nmfx &lt;- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\nThe slopes() function can use the ellipsis (...) to push any argument forward to the posterior_predict function. This can alter the types of predictions returned. For example, the re_formula=NA argument of the posterior_predict.brmsfit method will compute marginaleffects without including any group-level effects:\n\nmfx &lt;- slopes(\n    mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\",\n    re_formula = NA) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\n\n8.2.4 Global grand mean\n\npred &lt;- predictions(\n    mod,\n    re_formula = NA,\n    newdata = datagrid(party_autonomy = c(TRUE, FALSE))) |&gt;\n    posterior_draws()\n\nmfx &lt;- slopes(\n    mod,\n    re_formula = NA,\n    variables = \"party_autonomy\") |&gt;\n    posterior_draws()\n\nplot1 &lt;- ggplot(pred, aes(x = draw, fill = party_autonomy)) +\n         stat_halfeye(slab_alpha = .5) +\n         labs(x = \"Media index (Predicted)\",\n              y = \"Posterior density\",\n              fill = \"Party autonomy\")\n\nplot2 &lt;- ggplot(mfx, aes(x = draw)) +\n         stat_halfeye(slab_alpha = .5)  +\n         labs(x = \"Contrast: Party autonomy TRUE - FALSE\",\n              y = \"\",\n              fill = \"Party autonomy\")\n\n## combine plots using the `patchwork` package\nplot1 + plot2\n\n\n\n\n\n8.2.5 Region-specific predictions and contrasts\nPredicted media index by region and level of civil liberties:\n\npred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = FALSE, \n                                       civil_liberties = seq(0, 1, length.out = 100))) |&gt; \n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    facet_wrap(~ region) +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n\n\n\n\nPredicted media index by region and level of civil liberties:\n\npred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       civil_liberties = c(.2, .8),\n                                      party_autonomy = FALSE)) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    facet_wrap(~ region) +\n    labs(x = \"Media index (predicted)\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\nPredicted media index by region and party autonomy:\n\npred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = c(TRUE, FALSE),\n                                       civil_liberties = .5)) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n\n\n\n\nTRUE/FALSE contrasts (marginal effects) of party autonomy by region:\n\nmfx &lt;- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    newdata = datagrid(\n        region = vdem_2015$region,\n        civil_liberties = .5)) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n\n\n\n\n\n8.2.6 Hypothetical groups\nWe can also obtain predictions or marginal effects for a hypothetical group instead of one of the observed regions. To achieve this, we create a dataset with NA in the region column. Then we call the marginaleffects or predictions() functions with the allow_new_levels argument. This argument is pushed through via the ellipsis (...) to the posterior_epred function of the brms package:\n\ndat &lt;- data.frame(civil_liberties = .5,\n                  party_autonomy = FALSE,\n                  region = \"New Region\")\n\nmfx &lt;- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    allow_new_levels = TRUE,\n    newdata = dat)\n\ndraws &lt;- posterior_draws(mfx)\n\nggplot(draws, aes(x = draw)) +\n     stat_halfeye() +\n     labs(x = \"Marginal effect of party autonomy in a generic world region\", y = \"\")\n\n\n\n\n\n8.2.7 Averaging, marginalizing, integrating random effects\nConsider a logistic regression model with random effects:\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/plm/EmplUK.csv\")\ndat$x &lt;- as.numeric(dat$output &gt; median(dat$output))\ndat$y &lt;- as.numeric(dat$emp &gt; median(dat$emp))\nmod &lt;- brm(y ~ x + (1 | firm), data = dat, backend = \"cmdstanr\", family = \"bernoulli\")\n\nWe can compute adjusted predictions for a given value of x and for each firm (random effects) as follows:\n\np &lt;- predictions(mod, newdata = datagrid(x = 0, firm = unique))\nhead(p)\n#&gt; \n#&gt;  x firm Estimate    2.5 % 97.5 %\n#&gt;  0    1  1.0e+00 9.01e-01 1.0000\n#&gt;  0    2  1.0e+00 8.95e-01 1.0000\n#&gt;  0    3  1.0e+00 9.12e-01 1.0000\n#&gt;  0    4  1.0e+00 7.97e-01 1.0000\n#&gt;  0    5  1.0e+00 9.09e-01 1.0000\n#&gt;  0    6  4.9e-08 8.42e-21 0.0019\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, y, x, firm \n#&gt; Type:  response\n\nWe can average/marginalize/integrate across random effects with the avg_predictions() function or the by argument:\n\navg_predictions(mod, newdata = datagrid(x = 0, firm = unique))\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;     0.454  0.44  0.468\n#&gt; \n#&gt; Columns: estimate, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(x = 0:1, firm = unique), by = \"x\")\n#&gt; \n#&gt;  x Estimate 2.5 % 97.5 %\n#&gt;  0    0.454 0.440  0.468\n#&gt;  1    0.557 0.546  0.570\n#&gt; \n#&gt; Columns: x, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nWe can also draw from the (assumed gaussian) population distribution of random effects, by asking predictions() to make predictions for new ‚Äúlevels‚Äù of the random effects. If we then take an average of predictions using avg_predictions() or the by argument, we will have ‚Äúintegrated out the random effects‚Äù, as described in the brmsmargins package vignette. In the code below, we make predictions for 100 firm identifiers which were not in the original dataset. We also ask predictions() to push forward the allow_new_levels and sample_new_levels arguments to the brms::posterior_epred function:\n\npredictions(\n    mod,\n    newdata = datagrid(x = 0:1, firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\",\n    by = \"x\")\n#&gt; \n#&gt;  x Estimate 2.5 % 97.5 %\n#&gt;  0    0.454 0.338  0.565\n#&gt;  1    0.552 0.436  0.664\n#&gt; \n#&gt; Columns: x, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nWe can ‚Äúintegrate out‚Äù random effects in the other slopes() functions too. For instance,\n\navg_comparisons(\n    mod,\n    newdata = datagrid(firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n#&gt; \n#&gt;  Term Contrast Estimate  2.5 % 97.5 %\n#&gt;     x    1 - 0   0.0965 0.0494  0.162\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThis is nearly equivalent the brmsmargins command output (with slight variations due to different random seeds):\n\nlibrary(brmsmargins)\nbm &lt;- brmsmargins(\n  k = 100,\n  object = mod,\n  at = data.frame(x = c(0, 1)),\n  CI = .95,\n  CIType = \"ETI\",\n  contrasts = cbind(\"AME x\" = c(-1, 1)),\n  effects = \"integrateoutRE\")\nbm$ContrastSummary |&gt; data.frame()\n#&gt;            M        Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 0.09864399 0.09651684 0.04835076 0.1610664          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n\nSee the alternative software vignette for more information on brmsmargins.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#multinomial-logit",
    "href": "articles/brms.html#multinomial-logit",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.3 Multinomial logit",
    "text": "8.3 Multinomial logit\nFit a model with categorical outcome (heating system choice in California houses) and logit link:\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Heating.csv\"\ndat &lt;- read.csv(dat)\nmod &lt;- brm(depvar ~ ic.gc + oc.gc,\n           data = dat,\n           family = categorical(link = \"logit\"))\n\n\n8.3.1 Adjusted predictions\nCompute predicted probabilities for each level of the outcome variable:\n\npred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Group Estimate  2.5 % 97.5 %\n#&gt;     ec   0.0663 0.0447 0.0930\n#&gt;     ec   0.0768 0.0590 0.0974\n#&gt;     ec   0.1030 0.0618 0.1585\n#&gt;     ec   0.0634 0.0459 0.0838\n#&gt;     ec   0.0745 0.0574 0.0947\n#&gt;     ec   0.0709 0.0455 0.1036\n#&gt; \n#&gt; Columns: rowid, group, estimate, conf.low, conf.high, depvar, ic.gc, oc.gc \n#&gt; Type:  response\n\nExtract posterior draws and plot them:\n\ndraws &lt;- posterior_draws(pred)\n\nggplot(draws, aes(x = draw, fill = group)) +\n    geom_density(alpha = .2, color = \"white\") +\n    labs(x = \"Predicted probability\",\n         y = \"Density\",\n         fill = \"Heating system\")\n\n\n\n\nUse the plot_predictions() function to plot conditional adjusted predictions for each level of the outcome variable gear, conditional on the value of the mpg regressor:\n\nplot_predictions(mod, condition = \"oc.gc\") +\n    facet_wrap(~ group) +\n    labs(y = \"Predicted probability\")\n\n\n\n\n\n8.3.2 Marginal effects\n\navg_slopes(mod)\n#&gt; \n#&gt;  Group  Term  Estimate     2.5 %   97.5 %\n#&gt;     ec ic.gc -1.77e-04 -3.96e-04 2.37e-05\n#&gt;     er ic.gc  1.65e-05 -2.26e-04 2.51e-04\n#&gt;     gc ic.gc  1.38e-05 -3.72e-04 4.00e-04\n#&gt;     gr ic.gc  4.24e-05 -2.37e-04 3.30e-04\n#&gt;     hp ic.gc  1.07e-04 -7.73e-05 2.97e-04\n#&gt;     ec oc.gc  4.88e-04 -4.04e-04 1.45e-03\n#&gt;     er oc.gc -1.02e-03 -2.07e-03 2.98e-05\n#&gt;     gc oc.gc  1.04e-03 -7.39e-04 2.78e-03\n#&gt;     gr oc.gc  9.46e-05 -1.19e-03 1.34e-03\n#&gt;     hp oc.gc -5.85e-04 -1.45e-03 2.30e-04\n#&gt; \n#&gt; Columns: term, group, estimate, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#hurdle-models",
    "href": "articles/brms.html#hurdle-models",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.4 Hurdle models",
    "text": "8.4 Hurdle models\nThis section replicates some analyses from yet another amazing blog post by Andrew Heiss.\nTo begin, we estimate a hurdle model in brms with random effects, using data from the gapminder package: 704G\n\nlibrary(gapminder)\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(cmdstanr)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nset.seed(1024)\n\nCHAINS &lt;- 4\nITER &lt;- 2000\nWARMUP &lt;- 1000\nBAYES_SEED &lt;- 1234\n\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero) |&gt; \n  # Make a logged version of GDP per capita\n  mutate(log_gdpPercap = log1p(gdpPercap)) |&gt; \n  mutate(is_zero = gdpPercap == 0)\n\nmod &lt;- brm(\n  bf(gdpPercap ~ lifeExp + year + (1 + lifeExp + year | continent),\n     hu ~ lifeExp),\n  data = gapminder,\n  backend = \"cmdstanr\",\n  family = hurdle_lognormal(),\n  cores = 2,\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2)\n\n\n8.4.1 Adjusted predictions\nAdjusted predictions for every observation in the original data:\n\npredictions(mod) |&gt; head()\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;       143   103    219\n#&gt;       168   125    256\n#&gt;       202   153    304\n#&gt;       251   197    373\n#&gt;       312   250    454\n#&gt;       398   325    567\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  response\n\nAdjusted predictions for the hu parameter:\n\npredictions(mod, dpar = \"hu\") |&gt; head()\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;     0.574 0.475  0.652\n#&gt;     0.537 0.442  0.611\n#&gt;     0.496 0.407  0.566\n#&gt;     0.446 0.366  0.511\n#&gt;     0.396 0.325  0.454\n#&gt;     0.341 0.282  0.391\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  response\n\nPredictions on a different scale:\n\npredictions(mod, type = \"link\", dpar = \"hu\") |&gt; head()\n#&gt; \n#&gt;  Estimate  2.5 %  97.5 %\n#&gt;    0.2980 -0.101  0.6259\n#&gt;    0.1463 -0.235  0.4527\n#&gt;   -0.0178 -0.377  0.2673\n#&gt;   -0.2189 -0.551  0.0424\n#&gt;   -0.4234 -0.730 -0.1857\n#&gt;   -0.6573 -0.933 -0.4443\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  link\n\nPlot adjusted predictions as a function of lifeExp:\n\nplot_predictions(\n    mod,\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\nplot_predictions(\n    mod,\n    dpar = \"hu\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n\n\n\n\nPredictions with more than one condition and the re_formula argument from brms:\n\nplot_predictions(\n    mod,\n    re_formula = NULL,\n    condition = c(\"lifeExp\", \"continent\"))\n\n\n\n\n\n8.4.2 Extract draws with posterior_draws()\n\nThe posterior_draws() function extract raw samples from the posterior from objects produced by marginaleffects. This allows us to use richer geoms and summaries, such as those in the ggdist package:\n\npredictions(\n    mod,\n    re_formula = NULL,\n    newdata = datagrid(model = mod,\n                       continent = gapminder$continent,\n                       year = c(1952, 2007),\n                       lifeExp = seq(30, 80, 1))) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(lifeExp, draw, fill = continent, color = continent)) +\n    stat_lineribbon(alpha = .25) +\n    facet_grid(year ~ continent)\n\n\n\n\n\n8.4.3 Average Contrasts\nWhat happens to gdpPercap when lifeExp increases by one?\n\navg_comparisons(mod)\n#&gt; \n#&gt;     Term Contrast Estimate 2.5 % 97.5 %\n#&gt;  lifeExp       +1    718.9 515.6  812.0\n#&gt;  year          +1    -63.8 -84.4  -41.1\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nWhat happens to gdpPercap when lifeExp increases by one standard deviation?\n\navg_comparisons(mod, variables = list(lifeExp = \"sd\"))\n#&gt; \n#&gt;     Term                Contrast Estimate 2.5 % 97.5 %\n#&gt;  lifeExp (x + sd/2) - (x - sd/2)     4050  3718   4741\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nWhat happens to gdpPercap when lifeExp increases from 50 to 60 and year simultaneously increases its min to its max?\n\navg_comparisons(\n    mod,\n    variables = list(lifeExp = c(50, 60), year = \"minmax\"),\n    cross = TRUE)\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 % C: lifeExp   C: year\n#&gt;       835   523   1404    60 - 50 Max - Min\n#&gt; \n#&gt; Columns: term, contrast_lifeExp, contrast_year, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nPlot draws from the posterior distribution of average contrasts (not the same thing as draws from the posterior distribution of contrasts):\n\navg_comparisons(mod) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(estimate, term)) +\n    stat_dotsinterval() +\n    labs(x = \"Posterior distribution of average contrasts\", y = \"\")\n\n\n\n\n\n8.4.4 Marginal effects (slopes)\nAverage Marginal Effect of lifeExp on different scales and for different parameters:\n\navg_slopes(mod)\n#&gt; \n#&gt;     Term Estimate 2.5 % 97.5 %\n#&gt;  lifeExp    718.7 515.6    812\n#&gt;  year       -63.8 -84.4    -41\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, type = \"link\")\n#&gt; \n#&gt;     Term Estimate   2.5 %   97.5 %\n#&gt;  lifeExp  0.08249  0.0742  0.08856\n#&gt;  year    -0.00937 -0.0120 -0.00632\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  link\n\navg_slopes(mod, dpar = \"hu\")\n#&gt; \n#&gt;     Term Estimate    2.5 %   97.5 %\n#&gt;  lifeExp -0.00817 -0.00937 -0.00669\n#&gt;  year     0.00000  0.00000  0.00000\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, dpar = \"hu\", type = \"link\")\n#&gt; \n#&gt;     Term Estimate  2.5 %  97.5 %\n#&gt;  lifeExp  -0.0993 -0.113 -0.0838\n#&gt;  year      0.0000  0.000  0.0000\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  link\n\nPlot Conditional Marginal Effects\n\nplot_slopes(\n    mod,\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\n\nplot_slopes(\n    mod,\n    dpar = \"hu\",\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n\n\n\n\nOr we can call slopes() or comparisons() with posterior_draws() function to have even more control:\n\ncomparisons(\n    mod,\n    type = \"link\",\n    variables = \"lifeExp\",\n    newdata = datagrid(lifeExp = c(40, 70), continent = gapminder$continent)) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(draw, continent, fill = continent)) +\n    stat_dotsinterval() +\n    facet_grid(lifeExp ~ .) +\n    labs(x = \"Effect of a 1 unit change in Life Expectancy\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#bayesian-estimates-and-credible-intervals",
    "href": "articles/brms.html#bayesian-estimates-and-credible-intervals",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.5 Bayesian estimates and credible intervals",
    "text": "8.5 Bayesian estimates and credible intervals\nFor bayesian models like those produced by the brms or rstanarm packages, the marginaleffects package functions report the median of the posterior distribution as their main estimates.\nThe default credible intervals are equal-tailed intervals (quantiles), and the default function to identify the center of the distribution is the median. Users can customize the type of intervals reported by setting global options. Note that both the reported estimate and the intervals change slightly:\n\nlibrary(insight)\nlibrary(marginaleffects)\n\nmod &lt;- insight::download_model(\"brms_1\")\n\noptions(marginaleffects_posterior_interval = \"hdi\")\noptions(marginaleffects_posterior_center = mean)\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   cyl       +1    -1.50 -2.38 -0.677\n#&gt;   wt        +1    -3.21 -4.70 -1.570\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_posterior_interval = \"eti\")\noptions(marginaleffects_posterior_center = stats::median)\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   cyl       +1    -1.49 -2.36 -0.636\n#&gt;   wt        +1    -3.20 -4.79 -1.645\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#random-variables-posterior-and-ggdist",
    "href": "articles/brms.html#random-variables-posterior-and-ggdist",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.6 Random variables: posterior and ggdist\n",
    "text": "8.6 Random variables: posterior and ggdist\n\nRecent versions of the posterior, brms, and ggdist packages make it easy to draw, summarize and plot random variables. The posterior_draws() can produce objects of class rvar which make it easy to use those features by returning a data frame with a column of type rvar:\n\nlibrary(brms)\nlibrary(ggdist)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nmod &lt;- brm(am ~ mpg + hp, data = mtcars, family = bernoulli)\n\n\navg_comparisons(mod) |&gt;\n  posterior_draws(shape = \"rvar\") |&gt;\n  ggplot(aes(y = term, xdist = rvar)) + \n  stat_slabinterval()",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/brms.html#non-linear-hypothesis-testing",
    "href": "articles/brms.html#non-linear-hypothesis-testing",
    "title": "\n8¬† Bayes\n",
    "section": "\n8.7 Non-linear hypothesis testing",
    "text": "8.7 Non-linear hypothesis testing\nWe begin by estimating a model:\n\nmod &lt;- brm(am ~ mpg + hp, data = mtcars, family = bernoulli(),\n           seed = 1024, silent = 2, chains = 4, iter = 1000)\n\nNotice that we can compute average contrasts in two different ways, using the avg_comparisons() function or the comparison argument:\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate   2.5 %  97.5 %\n#&gt;   hp        +1  0.00599 0.00288 0.00886\n#&gt;   mpg       +1  0.13547 0.07871 0.17472\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, comparison = \"differenceavg\")\n#&gt; \n#&gt;  Term Contrast Estimate   2.5 %  97.5 %\n#&gt;   hp  mean(+1)  0.00599 0.00288 0.00886\n#&gt;   mpg mean(+1)  0.13547 0.07871 0.17472\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx \n#&gt; Type:  response\n\nNow, we use the hypothesis argument to compare the first to the second rows of the comparisons() output:\n\ncomparisons(\n    mod,\n    comparison = \"differenceavg\",\n    hypothesis = \"b2 - b1 = 0.2\")\n#&gt; \n#&gt;       Term Estimate  2.5 %  97.5 %\n#&gt;  b2-b1=0.2  -0.0702 -0.125 -0.0334\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThe hypotheses() function of the brms package can also perform non-linear hypothesis testing, and it generates some convenient statistics and summaries. This function accepts a D-by-P matrix of draws from the posterior distribution, where D is the number of draws and N is the number of parameters. We can obtain such a matrix using the posterior_draws(x, shape = \"DxP\"), and we can simply add a couple calls to our chain of operations:\n\navg_comparisons(mod, comparison = \"differenceavg\") |&gt;\n    posterior_draws(shape = \"DxP\") |&gt;\n    brms::hypothesis(\"b2 - b1 &gt; .2\")\n#&gt; Hypothesis Tests for class :\n#&gt;         Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n#&gt; 1 (b2-b1)-(.2) &gt; 0    -0.07      0.02    -0.12    -0.04          0         0     \n#&gt; ---\n#&gt; 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n#&gt; '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n#&gt; for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n#&gt; Posterior probabilities of point hypotheses assume equal prior probabilities.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Bayes</span>"
    ]
  },
  {
    "objectID": "articles/bootstrap.html#delta-method",
    "href": "articles/bootstrap.html#delta-method",
    "title": "\n9¬† Bootstrap & Simulation\n",
    "section": "\n9.1 Delta method",
    "text": "9.1 Delta method\nThe default strategy to compute standard errors and confidence intervals is the delta method. This is what we obtain by calling:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nSince this is the default method, we obtain the same results if we add the inferences() call in the chain:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"delta\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Bootstrap & Simulation</span>"
    ]
  },
  {
    "objectID": "articles/bootstrap.html#bootstrap",
    "href": "articles/bootstrap.html#bootstrap",
    "title": "\n9¬† Bootstrap & Simulation\n",
    "section": "\n9.2 Bootstrap",
    "text": "9.2 Bootstrap\nmarginaleffects supports three bootstrap frameworks in R: the well-established boot package, the newer rsample package, and the so-called ‚Äúbayesian bootstrap‚Äù in fwb.\n\n9.2.1 boot\n\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"boot\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.267 -0.625  0.443\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.162 -0.340  0.327\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.182 -0.335  0.368\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, std.error, conf.low, conf.high \n#&gt; Type:  response\n\nAll unknown arguments that we feed to inferences() are pushed forward to boot::boot():\n\nest &lt;- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"boot\", sim = \"balanced\", R = 500, conf_type = \"bca\")\nest\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.266 -0.662  0.404\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.162 -0.335  0.298\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.184 -0.344  0.377\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, std.error, conf.low, conf.high \n#&gt; Type:  response\n\nWe can extract the original boot object from an attribute:\n\nattr(est, \"inferences\")\n#&gt; \n#&gt; BALANCED BOOTSTRAP\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; bootstrap_boot(model = model, INF_FUN = INF_FUN, newdata = ..1, \n#&gt;     vcov = ..2, variables = ..3, type = ..4, by = ..5, conf_level = ..6, \n#&gt;     cross = ..7, comparison = ..8, transform = ..9, wts = ..10, \n#&gt;     hypothesis = ..11, eps = ..12)\n#&gt; \n#&gt; \n#&gt; Bootstrap Statistics :\n#&gt;        original      bias    std. error\n#&gt; t1* -0.11025325 0.003230574   0.2663606\n#&gt; t2* -0.02006005 0.003873671   0.1615387\n#&gt; t3*  0.02158742 0.004170627   0.1837267\n\nOr we can extract the individual draws with the posterior_draws() function:\n\nposterior_draws(est) |&gt; head()\n#&gt;   drawid         draw        term contrast    Species    estimate predicted_lo predicted_hi predicted std.error   conf.low conf.high\n#&gt; 1      1 -0.031097605 Petal.Width mean(+1)     setosa -0.11025325     5.013640     4.901389  4.957514 0.2663606 -0.6618164 0.4041951\n#&gt; 2      1 -0.010926106 Petal.Width mean(+1) versicolor -0.02006005     6.330887     6.325011  6.327949 0.1615387 -0.3352636 0.2983942\n#&gt; 3      1 -0.001611747 Petal.Width mean(+1)  virginica  0.02158742     6.997499     7.033528  7.015513 0.1837267 -0.3438075 0.3769131\n#&gt; 4      2 -0.403310043 Petal.Width mean(+1)     setosa -0.11025325     5.013640     4.901389  4.957514 0.2663606 -0.6618164 0.4041951\n#&gt; 5      2 -0.057683806 Petal.Width mean(+1) versicolor -0.02006005     6.330887     6.325011  6.327949 0.1615387 -0.3352636 0.2983942\n#&gt; 6      2  0.101912012 Petal.Width mean(+1)  virginica  0.02158742     6.997499     7.033528  7.015513 0.1837267 -0.3438075 0.3769131\n\nposterior_draws(est, shape = \"DxP\") |&gt; dim()\n#&gt; [1] 500   3\n\n\n9.2.2 rsample\n\nAs before, we can pass arguments to rsample::bootstraps() through inferences(). For example, for stratified resampling:\n\nest &lt;- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"rsample\", R = 100, strata = \"Species\")\nest\n#&gt; \n#&gt;         Term Contrast    Species Estimate  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103 -0.650  0.510\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201 -0.371  0.264\n#&gt;  Petal.Width mean(+1) virginica    0.0216 -0.262  0.322\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, conf.low, conf.high \n#&gt; Type:  response\n\nattr(est, \"inferences\")\n#&gt; # Bootstrap sampling using stratification with apparent sample \n#&gt; # A tibble: 101 √ó 3\n#&gt;    splits           id           estimates       \n#&gt;    &lt;list&gt;           &lt;chr&gt;        &lt;list&gt;          \n#&gt;  1 &lt;split [150/47]&gt; Bootstrap001 &lt;tibble [3 √ó 7]&gt;\n#&gt;  2 &lt;split [150/48]&gt; Bootstrap002 &lt;tibble [3 √ó 7]&gt;\n#&gt;  3 &lt;split [150/57]&gt; Bootstrap003 &lt;tibble [3 √ó 7]&gt;\n#&gt;  4 &lt;split [150/58]&gt; Bootstrap004 &lt;tibble [3 √ó 7]&gt;\n#&gt;  5 &lt;split [150/55]&gt; Bootstrap005 &lt;tibble [3 √ó 7]&gt;\n#&gt;  6 &lt;split [150/53]&gt; Bootstrap006 &lt;tibble [3 √ó 7]&gt;\n#&gt;  7 &lt;split [150/55]&gt; Bootstrap007 &lt;tibble [3 √ó 7]&gt;\n#&gt;  8 &lt;split [150/63]&gt; Bootstrap008 &lt;tibble [3 √ó 7]&gt;\n#&gt;  9 &lt;split [150/54]&gt; Bootstrap009 &lt;tibble [3 √ó 7]&gt;\n#&gt; 10 &lt;split [150/55]&gt; Bootstrap010 &lt;tibble [3 √ó 7]&gt;\n#&gt; # ‚Ñπ 91 more rows\n\nOr we can extract the individual draws with the posterior_draws() function:\n\nposterior_draws(est) |&gt; head()\n#&gt;   drawid       draw        term contrast    Species    estimate predicted_lo predicted_hi predicted   conf.low conf.high\n#&gt; 1      1  0.2677437 Petal.Width mean(+1)     setosa -0.11025325     5.013640     4.901389  4.957514 -0.6497296 0.5100013\n#&gt; 2      1  0.1692484 Petal.Width mean(+1) versicolor -0.02006005     6.330887     6.325011  6.327949 -0.3712161 0.2635059\n#&gt; 3      1  0.1237673 Petal.Width mean(+1)  virginica  0.02158742     6.997499     7.033528  7.015513 -0.2617070 0.3222299\n#&gt; 4      2 -0.9913947 Petal.Width mean(+1)     setosa -0.11025325     5.013640     4.901389  4.957514 -0.6497296 0.5100013\n#&gt; 5      2 -0.4053751 Petal.Width mean(+1) versicolor -0.02006005     6.330887     6.325011  6.327949 -0.3712161 0.2635059\n#&gt; 6      2 -0.1347756 Petal.Width mean(+1)  virginica  0.02158742     6.997499     7.033528  7.015513 -0.2617070 0.3222299\n\nposterior_draws(est, shape = \"PxD\") |&gt; dim()\n#&gt; [1]   3 100\n\n\n9.2.3 Fractional Weighted Bootstrap (aka Bayesian Bootstrap)\nThe fwb package implements fractional weighted bootstrap (aka Bayesian bootstrap):\n\n‚Äúfwb implements the fractional weighted bootstrap (FWB), also known as the Bayesian bootstrap, following the treatment by Xu et al.¬†(2020). The FWB involves generating sets of weights from a uniform Dirichlet distribution to be used in estimating statistics of interest, which yields a posterior distribution that can be interpreted in the same way the traditional (resampling-based) bootstrap distribution can be.‚Äù -Noah Greifer\n\nThe inferences() function makes it easy to apply this inference strategy to marginaleffects objects:\n\navg_comparisons(mod) |&gt; inferences(method = \"fwb\")\n#&gt; \n#&gt;          Term            Contrast Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Length +1                    0.8929     0.0798  0.723  1.040\n#&gt;  Petal.Width  +1                   -0.0362     0.1620 -0.342  0.286\n#&gt;  Species      versicolor - setosa  -1.4629     0.3393 -2.131 -0.831\n#&gt;  Species      virginica - setosa   -1.9842     0.3973 -2.732 -1.231\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Bootstrap & Simulation</span>"
    ]
  },
  {
    "objectID": "articles/bootstrap.html#simulation-based-inference",
    "href": "articles/bootstrap.html#simulation-based-inference",
    "title": "\n9¬† Bootstrap & Simulation\n",
    "section": "\n9.3 Simulation-based inference",
    "text": "9.3 Simulation-based inference\nThis simulation-based strategy to compute confidence intervals was described in Krinsky & Robb (1986) and popularized by King, Tomz, Wittenberg (2000). We proceed in 3 steps:\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model‚Äôs estimated coefficients and variance equal to the model‚Äôs variance-covariance matrix (classical, ‚ÄúHC3‚Äù, or other).\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, or slopes.\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\nHere are a few examples:\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"simulation\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.272 -0.636  0.435\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.338  0.285\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.172 -0.333  0.350\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx \n#&gt; Type:  response\n\nSince simulation based inference generates R estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the posterior_draws() function, and plot their distributions using packages likeggplot2 and ggdist:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"simulation\") |&gt;\n  posterior_draws(\"rvar\") |&gt;\n  ggplot(aes(y = Species, xdist = rvar)) +\n  stat_slabinterval()",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Bootstrap & Simulation</span>"
    ]
  },
  {
    "objectID": "articles/bootstrap.html#multiple-imputation-and-missing-data",
    "href": "articles/bootstrap.html#multiple-imputation-and-missing-data",
    "title": "\n9¬† Bootstrap & Simulation\n",
    "section": "\n9.4 Multiple imputation and missing data",
    "text": "9.4 Multiple imputation and missing data\nThe same workflow and the same inferences() function can be used to estimate models with multiple imputation for missing data.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Bootstrap & Simulation</span>"
    ]
  },
  {
    "objectID": "articles/categorical.html#masspolr-function",
    "href": "articles/categorical.html#masspolr-function",
    "title": "\n10¬† Categorical outcomes\n",
    "section": "\n10.1 MASS::polr function",
    "text": "10.1 MASS::polr function\nConsider a simple ordered logit model in which we predict the number of gears of a car based its miles per gallon and horsepower:\n\nlibrary(MASS)\nmod &lt;- polr(factor(gear) ~ mpg + hp, data = mtcars, Hess = TRUE)\n\nNow, consider a car with 25 miles per gallon and 110 horsepower. The expected predicted probability for each outcome level (gear) for this car is:\n\npredictions(mod, newdata = datagrid(mpg = 25, hp = 110))\n#&gt; \n#&gt;  Group mpg  hp Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;      3  25 110    0.203     0.0959 2.12   0.0339  4.9 0.0155  0.391\n#&gt;      4  25 110    0.578     0.1229 4.70   &lt;0.001 18.6 0.3373  0.819\n#&gt;      5  25 110    0.218     0.1007 2.17   0.0302  5.1 0.0209  0.416\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp \n#&gt; Type:  probs\n\nSince the gear is categorical, we make one prediction for each level of the outcome.\nNow consider the marginal effects (aka slopes or partial derivatives) for the same car:\n\nslopes(mod, variables = \"mpg\", newdata = datagrid(mpg = 25, hp = 110))\n#&gt; \n#&gt;  Group Term mpg  hp Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;      3  mpg  25 110 -0.06041     0.0169 -3.5813   &lt;0.001 11.5 -0.09347 -0.0273\n#&gt;      4  mpg  25 110 -0.00321     0.0335 -0.0958   0.9237  0.1 -0.06896  0.0625\n#&gt;      5  mpg  25 110  0.06362     0.0301  2.1132   0.0346  4.9  0.00461  0.1226\n#&gt; \n#&gt; Columns: rowid, term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, predicted_lo, predicted_hi, predicted, gear \n#&gt; Type:  probs\n\nAgain, marginaleffects produces one estimate of the slope for each outcome level. For a small step size \\(\\varepsilon\\), the printed quantities are estimated as:\n\\[\\frac{P(gear=3|mpg=25+\\varepsilon, hp=110)-P(gear=3|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\] \\[\\frac{P(gear=4|mpg=25+\\varepsilon, hp=110)-P(gear=4|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\] \\[\\frac{P(gear=5|mpg=25+\\varepsilon, hp=110)-P(gear=5|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\]\nWhen we call avg_slopes(), marginaleffects will repeat the same computation for every row of the original dataset, and then report the average slope for each level of the outcome:\n\navg_slopes(mod)\n#&gt; \n#&gt;  Group Term Estimate Std. Error     z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;      3  hp  -0.00377   0.001514 -2.49  0.01284  6.3 -0.006735 -0.00080\n#&gt;      3  mpg -0.07014   0.015485 -4.53  &lt; 0.001 17.4 -0.100490 -0.03979\n#&gt;      4  hp   0.00201   0.000957  2.10  0.03553  4.8  0.000136  0.00389\n#&gt;      4  mpg  0.03747   0.013861  2.70  0.00687  7.2  0.010303  0.06464\n#&gt;      5  hp   0.00175   0.000833  2.11  0.03519  4.8  0.000122  0.00339\n#&gt;      5  mpg  0.03267   0.009572  3.41  &lt; 0.001 10.6  0.013907  0.05143\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Categorical outcomes</span>"
    ]
  },
  {
    "objectID": "articles/categorical.html#nnet-package",
    "href": "articles/categorical.html#nnet-package",
    "title": "\n10¬† Categorical outcomes\n",
    "section": "\n10.2 nnet package",
    "text": "10.2 nnet package\nThe multinom function of the nnet package allows users to fit log-linear models via neural networks. The data used for this function is a data frame with one observation per row, and the response variable is coded a factor. All the marginaleffects package function work seamlessly with this model. For example, we can estimate a model and compute average marginal effects as follows:\n\nlibrary(nnet)\n\nhead(mtcars)\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nmod &lt;- multinom(factor(gear) ~ hp + mpg, data = mtcars, trace = FALSE)\n\navg_slopes(mod, type = \"probs\")\n#&gt; \n#&gt;  Group Term  Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %    97.5 %\n#&gt;      3  hp  -3.44e-05    0.00225 -0.0153  0.98780  0.0 -0.00444  0.004372\n#&gt;      3  mpg -7.13e-02    0.02645 -2.6959  0.00702  7.2 -0.12316 -0.019467\n#&gt;      4  hp  -4.67e-03    0.00221 -2.1123  0.03466  4.9 -0.00900 -0.000337\n#&gt;      4  mpg  1.59e-02    0.02010  0.7915  0.42865  1.2 -0.02348  0.055298\n#&gt;      5  hp   4.70e-03    0.00130  3.6172  &lt; 0.001 11.7  0.00215  0.007249\n#&gt;      5  mpg  5.54e-02    0.01642  3.3739  &lt; 0.001 10.4  0.02322  0.087590\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n\nNotice that in such models, we get one marginal effect for each term, for each level of the response variable. For this reason, we should use \"group\" in the condition argument (or facet_*() function) when calling one of the plotting functions:\n\nlibrary(ggplot2)\n\nplot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\")\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"probs\") + facet_wrap(~group)\n\n\n\n\nplot_comparisons(\n    mod,\n    variables = list(mpg = c(15, 30)),\n    condition = \"group\",\n    type = \"probs\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Categorical outcomes</span>"
    ]
  },
  {
    "objectID": "articles/categorical.html#mlogit-package",
    "href": "articles/categorical.html#mlogit-package",
    "title": "\n10¬† Categorical outcomes\n",
    "section": "\n10.3 mlogit package",
    "text": "10.3 mlogit package\nThe mlogit package uses data in a slightly different structure, with one row per observation-choice combination. For example, this data on choice of travel mode includes 4 rows per individual, one for each mode of transportation:\n\nlibrary(\"AER\")\nlibrary(\"mlogit\")\nlibrary(\"tidyverse\")\ndata(\"TravelMode\", package = \"AER\")\n\nhead(TravelMode)\n#&gt;   individual  mode choice wait vcost travel gcost income size\n#&gt; 1          1   air     no   69    59    100    70     35    1\n#&gt; 2          1 train     no   34    31    372    71     35    1\n#&gt; 3          1   bus     no   35    25    417    70     35    1\n#&gt; 4          1   car    yes    0    10    180    30     35    1\n#&gt; 5          2   air     no   64    58     68    68     30    2\n#&gt; 6          2 train     no   44    31    354    84     30    2\n\nmod &lt;- mlogit(choice ~ wait + gcost | income + size, TravelMode)\n\navg_slopes(mod, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;  air   income  0.002786    0.00122  2.288  0.02213  5.5  0.000399  0.00517\n#&gt;  air   size   -0.126465    0.02892 -4.374  &lt; 0.001 16.3 -0.183139 -0.06979\n#&gt;  bus   income -0.000372    0.00110 -0.338  0.73548  0.4 -0.002531  0.00179\n#&gt;  bus   size    0.011345    0.02587  0.439  0.66099  0.6 -0.039358  0.06205\n#&gt;  car   income  0.003373    0.00137  2.455  0.01408  6.1  0.000680  0.00607\n#&gt;  car   size    0.045880    0.02476  1.853  0.06385  4.0 -0.002642  0.09440\n#&gt;  train income -0.005787    0.00132 -4.389  &lt; 0.001 16.4 -0.008370 -0.00320\n#&gt;  train size    0.069240    0.02478  2.794  0.00521  7.6  0.020662  0.11782\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that the slopes() function will always return estimates of zero for regressors before the vertical bar in the formula. This is because marginaleffects increments all rows of the prediction dataset in the same way to compute slopes and contrast. Because mlogit data are in ‚Äúlong‚Äù format, this means that alternatives are incremented in the same way, which does not produce alternative-specific changes in the predictors.\nOne strategy to circumvent this problem is to supply a data frame of numeric values to compare, with alternative specific changes. In this example, we test what happens to the probability of selecting each mode of transportation if we only increase the wait time of air travel:\n\naltspec &lt;- data.frame(\n  low = TravelMode$wait,\n  high = ifelse(TravelMode$mode == \"air\", TravelMode$wait + 15, TravelMode$wait)\n)\n\navg_comparisons(mod, variables = list(wait = altspec))\n#&gt; \n#&gt;  Group Term Contrast Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 %  97.5 %\n#&gt;  air   wait   manual  -0.1321    0.01070 -12.35   &lt;0.001 114.0 -0.1531 -0.1111\n#&gt;  bus   wait   manual   0.0251    0.00460   5.45   &lt;0.001  24.2  0.0160  0.0341\n#&gt;  car   wait   manual   0.0701    0.00834   8.41   &lt;0.001  54.5  0.0538  0.0865\n#&gt;  train wait   manual   0.0369    0.00528   6.99   &lt;0.001  38.4  0.0266  0.0473\n#&gt; \n#&gt; Columns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can compute yet more kinds of marginal effects, we can construct customized data frames and feed them to the newdata argument of the slopes() function.\nIf we want to compute the slope of the response function (marginal effects) when each of the predictors is fixed to its global mean, we can do:\n\nnd &lt;- TravelMode |&gt;\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"),\n              function(x) rep(mean(x), 4)))\nnd\n#&gt;       wait    gcost   income     size\n#&gt; 1 34.58929 110.8798 34.54762 1.742857\n#&gt; 2 34.58929 110.8798 34.54762 1.742857\n#&gt; 3 34.58929 110.8798 34.54762 1.742857\n#&gt; 4 34.58929 110.8798 34.54762 1.742857\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error     z Pr(&gt;|z|)   S     2.5 %    97.5 %\n#&gt;  air   income  6.66e-03   2.42e-03  2.75  0.00599 7.4  1.91e-03  1.14e-02\n#&gt;  air   size   -1.69e-01   5.88e-02 -2.88  0.00394 8.0 -2.85e-01 -5.42e-02\n#&gt;  bus   income -1.14e-03   9.44e-04 -1.21  0.22657 2.1 -2.99e-03  7.09e-04\n#&gt;  bus   size    4.67e-02   2.72e-02  1.72  0.08624 3.5 -6.66e-03  1.00e-01\n#&gt;  car   income  6.48e-06   2.02e-05  0.32  0.74894 0.4 -3.32e-05  4.62e-05\n#&gt;  car   size    1.36e-03   8.81e-04  1.54  0.12305 3.0 -3.68e-04  3.08e-03\n#&gt;  train income -5.52e-03   1.91e-03 -2.89  0.00383 8.0 -9.26e-03 -1.78e-03\n#&gt;  train size    1.21e-01   4.45e-02  2.73  0.00634 7.3  3.42e-02  2.08e-01\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nIf we want to compute marginal effects with the gcost and wait fixed at their mean value, conditional on the choice of transportation mode:\n\nnd &lt;- TravelMode |&gt;\n    group_by(mode) |&gt;\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"), mean))\nnd\n#&gt; # A tibble: 4 √ó 5\n#&gt;   mode   wait gcost income  size\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 air    61.0 103.    34.5  1.74\n#&gt; 2 train  35.7 130.    34.5  1.74\n#&gt; 3 bus    41.7 115.    34.5  1.74\n#&gt; 4 car     0    95.4   34.5  1.74\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;  air   income  0.006015    0.00233  2.583  0.00979  6.7  0.001451  0.01058\n#&gt;  air   size   -0.232927    0.05659 -4.116  &lt; 0.001 14.7 -0.343837 -0.12202\n#&gt;  bus   income -0.000713    0.00146 -0.489  0.62487  0.7 -0.003570  0.00214\n#&gt;  bus   size    0.020440    0.03436  0.595  0.55191  0.9 -0.046901  0.08778\n#&gt;  car   income  0.005445    0.00228  2.384  0.01711  5.9  0.000969  0.00992\n#&gt;  car   size    0.067820    0.04124  1.645  0.10003  3.3 -0.012999  0.14864\n#&gt;  train income -0.010747    0.00256 -4.202  &lt; 0.001 15.2 -0.015760 -0.00573\n#&gt;  train size    0.144668    0.04773  3.031  0.00244  8.7  0.051111  0.23822\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe can also explore more complex alternatives. Here, for example, only one alternative is affected by cost reduction:\n\nnd &lt;- datagrid(mode = TravelMode$mode, newdata = TravelMode)\nnd &lt;- lapply(1:4, function(i) mutate(nd, gcost = ifelse(1:4 == i, 30, gcost)))\nnd &lt;- bind_rows(nd)\nnd\n#&gt;    individual choice wait vcost travel gcost income size  mode\n#&gt; 1           1     no   35    48    486    30     35    2   air\n#&gt; 2           1     no   35    48    486   111     35    2 train\n#&gt; 3           1     no   35    48    486   111     35    2   bus\n#&gt; 4           1     no   35    48    486   111     35    2   car\n#&gt; 5           1     no   35    48    486   111     35    2   air\n#&gt; 6           1     no   35    48    486    30     35    2 train\n#&gt; 7           1     no   35    48    486   111     35    2   bus\n#&gt; 8           1     no   35    48    486   111     35    2   car\n#&gt; 9           1     no   35    48    486   111     35    2   air\n#&gt; 10          1     no   35    48    486   111     35    2 train\n#&gt; 11          1     no   35    48    486    30     35    2   bus\n#&gt; 12          1     no   35    48    486   111     35    2   car\n#&gt; 13          1     no   35    48    486   111     35    2   air\n#&gt; 14          1     no   35    48    486   111     35    2 train\n#&gt; 15          1     no   35    48    486   111     35    2   bus\n#&gt; 16          1     no   35    48    486    30     35    2   car\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %    97.5 %\n#&gt;  air   income  8.24e-03   2.46e-03  3.352   &lt;0.001 10.3  0.003422  0.013057\n#&gt;  air   size   -2.12e-01   6.02e-02 -3.526   &lt;0.001 11.2 -0.330512 -0.094367\n#&gt;  bus   income -1.33e-03   1.30e-03 -1.021    0.307  1.7 -0.003877  0.001222\n#&gt;  bus   size    6.06e-02   3.79e-02  1.600    0.110  3.2 -0.013662  0.134911\n#&gt;  car   income  2.66e-05   4.32e-05  0.616    0.538  0.9 -0.000058  0.000111\n#&gt;  car   size    2.38e-03   1.57e-03  1.512    0.131  2.9 -0.000704  0.005459\n#&gt;  train income -6.94e-03   1.86e-03 -3.735   &lt;0.001 12.4 -0.010580 -0.003297\n#&gt;  train size    1.49e-01   4.28e-02  3.488   &lt;0.001 11.0  0.065477  0.233397\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nImportant: The newdata argument for mlogit models must be a ‚Äúbalanced‚Äù data frame, that is, it must have a number of rows that is a multiple of the number of choices.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Categorical outcomes</span>"
    ]
  },
  {
    "objectID": "articles/conformal.html#confidence-vs.-prediction-intervals",
    "href": "articles/conformal.html#confidence-vs.-prediction-intervals",
    "title": "\n11¬† Conformal prediction\n",
    "section": "\n11.1 Confidence vs.¬†prediction intervals",
    "text": "11.1 Confidence vs.¬†prediction intervals\nThe predictions() function from the marginaleffects() package can compute confidence intervals for fitted values in over 80 model classes in R. These intervals quantify the uncertainty about the expected value of the response. A common misunderstanding is that these confidence intervals should be calibrated to cover a certain percentage of unseen data points. This is not the case. In fact, a 95% confidence interval reported by predictions() will typically cover a much smaller share of out-of-sample outcomes.\nConsider this simulation where \\(Y_{train}\\) and \\(Y_{test}\\) are drawn from a normal distribution with mean \\(\\pi\\) and standard deviation 1. We estimate a linear model with an intercept only, and compute a 90% confidence interval for the expected value of the response:\n\nlibrary(marginaleffects)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(nnet)\nlibrary(MASS)\n\nset.seed(1024)\n\nsimulation &lt;- function(...) {\n    Y_train &lt;- rnorm(25, mean = pi)\n    Y_test &lt;- rnorm(25, mean = pi)\n    m &lt;- lm(Y_train ~ 1)\n    p &lt;- predictions(m, conf_level = .90)\n    out &lt;- data.table(\n        `Test set coverage` = mean(Y_test &gt;= p$conf.low & Y_test &lt;= p$conf.high),\n        `True mean coverage` = pi &gt;= p$conf.low[1] & pi &lt;= p$conf.high[1]\n    )\n    return(out)\n}\nresults &lt;- rbindlist(lapply(1:1000, simulation))\n\ncolMeans(results)\n\n Test set coverage True mean coverage \n            0.2498             0.8770 \n\n\nWe see that the confidence interval around predictions covers the true mean of \\(\\pi\\) about 90% of the time, whereas coverage of individual observations in the test set is much lower.\nIf we care about out of sample predictions, that is, if we want our interval to cover a specific share of the actual outcome for unobserved individuals, we must compute ‚Äúprediction intervals‚Äù instead of ‚Äúconfidence intervals.‚Äù How do we do this? Conformal prediction is very flexible and powerful approach.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Conformal prediction</span>"
    ]
  },
  {
    "objectID": "articles/conformal.html#conformal-prediction",
    "href": "articles/conformal.html#conformal-prediction",
    "title": "\n11¬† Conformal prediction\n",
    "section": "\n11.2 Conformal prediction",
    "text": "11.2 Conformal prediction\nIn their excellent tutorial, Angelopoulos and Bates (2022) write that conformal prediction is\n\n‚Äúa user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions.\n\nThese are extraordinary claims which deserve to be underlined: In principle, conformal prediction should offer well-calibrated intervals, regardless of the prediction model we use, and even if that model is misspecified.\nThe main caveats are:\n\nThe conformal prediction algorithms implemented in marginaleffects are designed for exchangeable data.1 They do not offer coverage guarantees in contexts where exchangeability is violated, such as in time series data, when there is spatial dependence between observations, or when there is distribution drift between the training and test data.\nThe conformal prediction algorithms implemented in marginaleffects offer marginal coverage guarantees, that is, they guarantee that a random test point will fall within the interval with a given probability. Below, we show an example where the prediction interval covers the right number of test points overall, but is not well calibrated locally, in different strata of the predictors. Different algorithms have recently been proposed to offer class-conditional coverage guarantees (see Ding et al. (2023) for an example).\nThe width of the conformal prediction interval will typically depend on the quality of the prediction model and of the score function.\nThe score functions implemented in marginaleffects simply take the residual‚Äîor difference between the observed outcome and predicted value. This means that the type argument must ensure that observations and predictions are on commensurable scales (usually type=\"response\" or type=\"prob\").",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Conformal prediction</span>"
    ]
  },
  {
    "objectID": "articles/conformal.html#data-and-models-linear-logit-multinomial-logit",
    "href": "articles/conformal.html#data-and-models-linear-logit-multinomial-logit",
    "title": "\n11¬† Conformal prediction\n",
    "section": "\n11.3 Data and models: Linear, Logit, Multinomial Logit",
    "text": "11.3 Data and models: Linear, Logit, Multinomial Logit\nDownload data, split it into training and testing sets, and estimate a few different models:\n\n# download data\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/military.csv\")\n\n# create a binary outcome variable\ndat &lt;- transform(dat, officer = as.numeric(grepl(\"officer\", grade)))\n\n# train/test split\nidx &lt;- sample(seq_len(nrow(dat)), 60000)\ntest &lt;- dat[idx[1:10000], ]\ntrain &lt;- dat[idx[10001:length(idx)], ]\n\n# linear regression\nm_lm &lt;- lm(rank ~ gender * race, data = train)\np_lm &lt;- predictions(m_lm, newdata = train)\n\n# logit regression\nm_glm &lt;- glm(officer ~ gender * race, data = train, family = binomial)\np_glm &lt;- predictions(m_glm, newdata = train)\n\n# multinomial logit regression\nm_mult &lt;- multinom(branch ~ gender * race, data = train, trace = FALSE)\np_mult &lt;- predictions(m_mult, newdata = train)\n\nFor LM and GLM models, predictions() returns a data frame with one prediction for each row of the original data. This data frame includes confidence intervals:\n\np_glm\n\n\n Estimate Pr(&gt;|z|)     S  2.5 % 97.5 %\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1006   &lt;0.001 668.6 0.0885  0.114\n--- 49990 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.2080   &lt;0.001 837.6 0.1956  0.221\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer \nType:  invlink(link) \n\n\nFor multinomial models, predictions() returns a data frame with one prediction for each row and for each outcome level. We can see the predicted probabilities of each outcome level for the first observation in the original data:\n\np_mult |&gt; subset(rowid == 1)\n\n\n        Group Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n air force       0.181    0.00479 37.8   &lt;0.001   Inf  0.171   0.190\n army            0.473    0.00621 76.2   &lt;0.001   Inf  0.461   0.485\n marine corps    0.101    0.00376 27.0   &lt;0.001 530.9  0.094   0.109\n navy            0.245    0.00535 45.7   &lt;0.001   Inf  0.234   0.255\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Conformal prediction</span>"
    ]
  },
  {
    "objectID": "articles/conformal.html#conformal-predictions-with-inferences",
    "href": "articles/conformal.html#conformal-predictions-with-inferences",
    "title": "\n11¬† Conformal prediction\n",
    "section": "\n11.4 Conformal predictions with inferences()\n",
    "text": "11.4 Conformal predictions with inferences()\n\nIn the ‚ÄúBootstrap and Simultation‚Äù vignette, we saw that the inferences() function can be used to compute confidence intervals for any marginaleffects package estimates. The workflow is simple:\n\nGenerate estimates with predictions().\nPass the resulting object to inferences(), along with arguments to specify how to perform inference to obtain uncertainty estimates.\n\ninferences() supports two strategies for conformal prediction: split or CV+ Ding et al. (2023). The former is faster but less efficient. In the rest of this vignette, we illustrate how to use this same workflow to compute conformal prediction intervals.\n\n11.4.1 Cross-validation +\nThe p_lm, p_glm, and p_mult objects are predictions() objects. They contain the point predictions and confidence intervals for each observation in the training set. Now, we use the inferences() function to compute predictions and prediction intervals for every observation in the test set:\n\np &lt;- predictions(m_lm, conf_level = .9) |&gt; \n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\np\n\n\n Estimate Std. Error   z Pr(&gt;|z|)   S 5.0 % 95.0 % Pred. 5.0 % Pred. 95.0 %\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     5.86     0.0283 207   &lt;0.001 Inf  5.82   5.91        3.00         8.73\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n--- 9990 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer, pred.low, pred.high \nType:  response \n\n\nThe prediction interval is expected to cover the (known) true value about 90% of the time:\n\nmean(p$rank &lt;= p$pred.high & p$rank &gt;= p$pred.low)\n\n[1] 0.9082\n\n\nThe coverage also seems adequate (about 80%) for the logit model:\n\np &lt;- predictions(m_glm, conf_level = .8) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\nmean(p$officer &lt;= p$pred.high & p$officer &gt;= p$pred.low)\n\n[1] 0.7998\n\n\nWhen the outcome is categorical, we use conformal_score=\"softmax\". With this argument, inferences() generates ‚Äúconformal prediction sets,‚Äù that is, sets of possible outcome classes with coverage guarantees. inferences() returns a list column of sets for each observation. On average, those sets should cover the true value about 70% of the time:\n\np &lt;- predictions(m_mult, conf_level = .7) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_score = \"softmax\",\n        conformal_test = test)\nhead(p)\n\n\n    branch                        Pred Set\n army                 air force, army     \n navy      air force, army     , navy     \n navy                 air force, army     \n army                           army, navy\n air force            air force, army     \n army                           army, navy\n\nColumns: rowid, branch, pred.set \n\n\nFor example, for the first observation in the dataset, the conformal prediction is {air force, army} and the true value is army. The conformal prediction set thus covers the true value. The coverage rate is:\n\nmean(sapply(seq_len(nrow(p)), \\(i) p$branch[i] %in% p$pred.set[[i]]))\n\n[1] 0.6928\n\n\n\n11.4.2 Split conformal prediction\nFor split conformal prediction, we must first split the training set into a training and a calibration set (see Angelopoulos and Bates (2022)). Then, we pass the calibration set to the inferences() function:\n\ncalibration &lt;- train[1:1000,]\ntrain &lt;- train[1001:nrow(train),]\np &lt;- predictions(m_lm, conf_level = .9) |&gt;\n    inferences(\n        method = \"conformal_split\",\n        conformal_calibration = calibration,\n        conformal_test = test)\nmean(p$rank &lt;= p$pred.high & p$rank &gt;= p$pred.low)\n\n[1] 0.9112",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Conformal prediction</span>"
    ]
  },
  {
    "objectID": "articles/conformal.html#misspecification",
    "href": "articles/conformal.html#misspecification",
    "title": "\n11¬† Conformal prediction\n",
    "section": "\n11.5 Misspecification",
    "text": "11.5 Misspecification\n\n11.5.1 Polynomials\nAs noted above, the conformal prediction interval should be valid even if the model is misspecified. To illustrate this, we generate data from a linear model with polynomials, but estimate a linear model without polynomials. Then, we plot the results and compute the coverage of the prediction interval:\n\nN &lt;- 1000\nX &lt;- rnorm(N * 2)\ndat &lt;- data.frame(\n    X = X,\n    Y = X + X^2 + X^3 + rnorm(N * 2))\ntrain &lt;- dat[1:N,]\ntest &lt;- dat[(N + 1):nrow(dat),]\n\nm &lt;- lm(Y ~ X, data = train)\np &lt;- predictions(m) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\n\nmean(p$Y &lt;= p$pred.high & p$Y &gt;= p$pred.low)\n\n[1] 0.953\n\nggplot(p, aes(X, Y)) +\n    geom_point(alpha = .1) +\n    geom_ribbon(aes(X, ymin = pred.low, ymax = pred.high), alpha = .2, fill = \"#F0E442\") +\n    geom_ribbon(aes(X, ymin = conf.low, ymax = conf.high), alpha = .4, fill = \"#D55E00\") +\n    theme_bw() +\n    labs(\n        title = \"Confidence and prediction intervals for a misspecified linear model\",\n        subtitle = sprintf(\n            \"Confidence coverage (orange): %.2f%%; Prediction coverage (yellow): %.2f%%.\",\n            mean(p$Y &lt;= p$conf.high & p$Y &gt;= p$conf.low),\n            mean(p$Y &lt;= p$pred.high & p$Y &gt;= p$pred.low)))\n\n\n\n\nThis example is interesting, because it shows that the prediction interval has adquate marginal coverage. However, the intervals are not necessarily well calibrated ‚Äúlocally‚Äù, in different strata of \\(X\\). In the figure above, our model is misspecified, so we make more mistakes in the tails, where predictions are bad. In contrast, the interval catches more observations in the middle of the distribution, which ensures that the overall error rate is adequate.\n\n11.5.2 Poisson vs Negative Binomial\nHere is a second example of model misspecification. We generate data from a negative binomial model, but estimate a Poisson model. Nevertheless, the conformal prediction interval has good coverage:\n\nn &lt;- 10000\nX &lt;- rnorm(n)\neta &lt;- -1 + 2*X\nmu &lt;- exp(eta)\nY &lt;- rnegbin(n, mu = mu, theta = 1)\ndat &lt;- data.frame(X = X, Y = Y)\ntrain &lt;- dat[1:5000,]\ntest &lt;- dat[5001:nrow(dat),]\n\nmod &lt;- glm(Y ~ X, data = train, family = poisson)\n\np &lt;- predictions(mod, conf_level = .9) |&gt;\n    inferences(\n        method = \"conformal_cv+\",\n        R = 10,\n        conformal_test = test)\n\nmean(p$Y &gt;= p$pred.low & p$Y &lt;= p$pred.high)\n\n[1] 0.8968\n\n\n\n\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2022. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification,‚Äù no. arXiv:2107.07511 (September). https://doi.org/10.48550/arXiv.2107.07511.\n\n\nDing, Tiffany, Anastasios N. Angelopoulos, Stephen Bates, Michael I. Jordan, and Ryan J. Tibshirani. 2023. ‚ÄúClass-Conditional Conformal Prediction with Many Classes,‚Äù no. arXiv:2306.09335 (June). https://doi.org/10.48550/arXiv.2306.09335.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Conformal prediction</span>"
    ]
  },
  {
    "objectID": "articles/elasticity.html",
    "href": "articles/elasticity.html",
    "title": "\n12¬† Elasticity\n",
    "section": "",
    "text": "In some contexts, it is useful to interpret the results of a regression model in terms of elasticity or semi-elasticity. One strategy to achieve that is to estimate a log-log or a semilog model, where the left and/or right-hand side variables are logged. Another approach is to note that \\(\\frac{\\partial ln(x)}{\\partial x}=\\frac{1}{x}\\), and to post-process the marginal effects to transform them into elasticities or semi-elasticities.\nFor example, say we estimate a linear model of this form:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\]\nLet \\(\\hat{y}\\) be the adjusted prediction made by the model for some combination of covariates \\(x_1\\) and \\(x_2\\). The slope with respect to \\(x_1\\) (or ‚Äúmarginal effect‚Äù) is:\n\\[\\frac{\\partial \\hat{y}}{\\partial x_1}\\]\nWe can estimate the ‚Äúeyex‚Äù, ‚Äúeydx‚Äù, and ‚Äúdyex‚Äù (semi-)elasticities with respect to \\(x_1\\) as follows:\n\\[\n\\eta_1=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{x_1}{\\hat{y}}\\\\\n\\eta_2=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{1}{\\hat{y}} \\\\\n\\eta_3=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot x_1,\n\\]\nwith interpretations roughly as follows:\n\nA percentage point increase in \\(x_1\\) is associated to a \\(\\eta_1\\) percentage points increase in \\(y\\).\nA unit increase in \\(x_1\\) is associated to a \\(\\eta_2\\) percentage points increase in \\(y\\).\nA percentage point increase in \\(x_1\\) is associated to a \\(\\eta_3\\) units increase in \\(y\\).\n\nFor further intuition, consider the ratio of change in \\(y\\) to change in \\(x\\): \\(\\frac{\\Delta y}{\\Delta x}\\). We can turn this ratio into a ratio between relative changes by dividing both the numerator and the denominator: \\(\\frac{\\frac{\\Delta y}{y}}{\\frac{\\Delta x}{x}}\\). This is of course linked to the expression for the \\(\\eta_1\\) elasticity above.\nWith the marginaleffects package, these quantities are easy to compute:\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\navg_slopes(mod)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp  -0.0318    0.00903 -3.52   &lt;0.001 11.2 -0.0495 -0.0141\n#&gt;    wt  -3.8778    0.63276 -6.13   &lt;0.001 30.1 -5.1180 -2.6376\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"eyex\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;    hp    eY/eX   -0.285     0.0855 -3.34   &lt;0.001 10.2 -0.453 -0.118\n#&gt;    wt    eY/eX   -0.746     0.1418 -5.26   &lt;0.001 22.7 -1.024 -0.468\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"eydx\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 %    97.5 %\n#&gt;    hp    eY/dX -0.00173   0.000502 -3.46   &lt;0.001 10.8 -0.00272 -0.000751\n#&gt;    wt    eY/dX -0.21165   0.037851 -5.59   &lt;0.001 25.4 -0.28583 -0.137461\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"dyex\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;    hp    dY/eX    -4.66       1.32 -3.52   &lt;0.001 11.2  -7.26  -2.06\n#&gt;    wt    dY/eX   -12.48       2.04 -6.13   &lt;0.001 30.1 -16.47  -8.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Elasticity</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#predictions",
    "href": "articles/equivalence.html#predictions",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.1 Predictions",
    "text": "13.1 Predictions\nConsider a single prediction, where all predictors are held at their median or mode:\n\np &lt;- predictions(mod, newdata = \"median\")\np\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp gear\n#&gt;      19.7          1 19.6   &lt;0.001 281.3  17.7   21.6 123    3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear \n#&gt; Type:  response\n\nNow we specify an equivalence interval (or ‚Äúregion‚Äù) for predictions between 17 and 18:\n\nhypotheses(p, equivalence = c(17, 18))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)  hp gear\n#&gt;      19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404     0.951 123    3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n\nThe results allow us to draw three conclusions:\n\nThe p value for the non-inferiority test is 0.0040. This suggests that we can reject the null hypothesis that the parameter is below 17.\nThe p value for the non-superiority test is 0.9508. This suggests that we cannot reject the null hypothesis that the parameter (19.6589) is above 18.\nThe p value for the equivalence test is 0.9508. This suggests that we cannot reject the hypothesis that the parameter falls outside the equivalence interval.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#model-coefficients",
    "href": "articles/equivalence.html#model-coefficients",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.2 Model coefficients",
    "text": "13.2 Model coefficients\nThe hypotheses() function also allows users to conduct equivalence, non-inferiority, and non-superiority tests for model coefficients, and for arbitrary functions of model coefficients.\nOur estimate of the 4th coefficient in the model is:\n\ncoef(mod)[4]\n#&gt; factor(gear)5 \n#&gt;      6.574763\n\nWe can test if this parameter is likely to fall in the [5,7] interval by:\n\nhypotheses(mod, equivalence = c(5, 7))[4, ]\n#&gt; \n#&gt;           Term Estimate Std. Error z Pr(&gt;|z|)    S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  factor(gear)5     6.57       1.64 4   &lt;0.001 14.0  3.36   9.79      0.398      0.169     0.398\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\nThe p value is 0.3979, so we cannot reject the hypothesis that the factor(gear)5 parameter falls outside the [5,7] interval.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#slopes",
    "href": "articles/equivalence.html#slopes",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.3 Slopes",
    "text": "13.3 Slopes\nThe same syntax can be used to conduct tests for all the quantities produced by the marginaleffects package. For example, imagine that, for substantive or theoretical reasons, an average slope between -0.1 and 0.1 is uninteresting. We can conduct an equivalence test to check if this is the case:\n\navg_slopes(mod, variables = \"hp\", equivalence = c(-.1, .1))\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;    hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001    0.00135   0.00135\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n\nThe p value is 0.0013, which suggests that we can reject the hypothesis that the parameter falls outside the region of ‚Äúsubstantive equivalence‚Äù that we have defined by the interval.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#difference-between-comparisons-contrasts",
    "href": "articles/equivalence.html#difference-between-comparisons-contrasts",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.4 Difference between comparisons (contrasts)",
    "text": "13.4 Difference between comparisons (contrasts)\nConsider a model with a multiplicative interaction:\n\nint &lt;- lm(mpg ~ hp * factor(gear), data = mtcars)\n\nThe average contrast for a change of 1 unit in hp differs based on the value of gear:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\")\n#&gt; \n#&gt;  Term Contrast gear Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp mean(+1)    3  -0.0522     0.0146 -3.59   &lt;0.001 11.6 -0.0808 -0.0237\n#&gt;    hp mean(+1)    4  -0.1792     0.0303 -5.92   &lt;0.001 28.2 -0.2385 -0.1199\n#&gt;    hp mean(+1)    5  -0.0583     0.0126 -4.61   &lt;0.001 17.9 -0.0830 -0.0335\n#&gt; \n#&gt; Columns: term, contrast, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nAre these contrasts different from one another? Let‚Äôs look at the pairwise differences between them:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;  3 - 4  0.12695     0.0336  3.781   &lt;0.001 12.6  0.0611  0.1928\n#&gt;  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438\n#&gt;  4 - 5 -0.12092     0.0328 -3.688   &lt;0.001 12.1 -0.1852 -0.0567\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nWe consider that these pairwise comparisons are ‚Äúequivalent to zero‚Äù when they fall in the [-.1, .1] interval:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-.1, .1))\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  3 - 4  0.12695     0.0336  3.781   &lt;0.001 12.6  0.0611  0.1928      0.789     &lt;0.001     0.789\n#&gt;  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438     &lt;0.001     &lt;0.001    &lt;0.001\n#&gt;  4 - 5 -0.12092     0.0328 -3.688   &lt;0.001 12.1 -0.1852 -0.0567     &lt;0.001      0.738     0.738\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n\nThe p (Equiv) column shows that the difference between the average contrasts when gear is 3 and gear is 5 can be said to be equivalent to the specified interval. However, there are good reasons to think that the other two pairwise comparisons may fall outside the interval.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#marginal-means-and-emmeans",
    "href": "articles/equivalence.html#marginal-means-and-emmeans",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.5 Marginal means and emmeans\n",
    "text": "13.5 Marginal means and emmeans\n\nThis example shows the equivalence between results produced by the emmeans package and the marginal_means() function:\n\nlibrary(emmeans)\n\nmod &lt;- lm(log(conc) ~ source + factor(percent), data = pigs)\n\n## {emmeans}\nemmeans(mod, specs = \"source\") |&gt;\n    pairs() |&gt;\n    test(df = Inf,\n         null = 0,\n         delta = log(1.25),\n         side = \"equivalence\",\n         adjust = \"none\")\n#&gt;  contrast    estimate     SE  df z.ratio p.value\n#&gt;  fish - soy    -0.273 0.0529 Inf   0.937  0.8257\n#&gt;  fish - skim   -0.402 0.0542 Inf   3.308  0.9995\n#&gt;  soy - skim    -0.130 0.0530 Inf  -1.765  0.0388\n#&gt; \n#&gt; Results are averaged over the levels of: percent \n#&gt; Degrees-of-freedom method: user-specified \n#&gt; Results are given on the log (not the response) scale. \n#&gt; Statistics are tests of equivalence with a threshold of 0.22314 \n#&gt; P values are left-tailed\n\n## {marginaleffects}\nmarginal_means(\n    mod,\n    variables = \"source\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-log(1.25), log(1.25)))\n#&gt; \n#&gt;         Term   Mean Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  fish - soy  -0.273     0.0529 -5.15   &lt;0.001 21.9 -0.377 -0.1690     &lt;0.001     0.8257    0.8257\n#&gt;  fish - skim -0.402     0.0542 -7.43   &lt;0.001 43.0 -0.508 -0.2961     &lt;0.001     0.9995    0.9995\n#&gt;  soy - skim  -0.130     0.0530 -2.44   0.0146  6.1 -0.233 -0.0255     &lt;0.001     0.0388    0.0388\n#&gt; \n#&gt; Results averaged over levels of: percent, source \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, p.value.equiv, p.value.noninf, p.value.nonsup, statistic.noninf, statistic.nonsup \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/equivalence.html#t-test",
    "href": "articles/equivalence.html#t-test",
    "title": "\n13¬† Equivalence Tests\n",
    "section": "\n13.6 t-test",
    "text": "13.6 t-test\nNow we show that the results produced by hypotheses() are identical to the results produced by the equivalence package in the case of a simple t-test:\n\nlibrary(equivalence)\n\nset.seed(1024)\n\n## simulate data data\nN &lt;- 20\ndat &lt;- data.frame(\n    y = rnorm(N),\n    x = sample(c(rep(0, N / 2), rep(1, N / 2)), N))\n\n## fit model\nmod &lt;- lm(y ~ x, data = dat)\n\n## test with the {equivalence} package\ne &lt;- tost(\n    x = dat$y[dat$x == 0],\n    y = dat$y[dat$x == 1],\n    epsilon = 10)\ne\n#&gt; \n#&gt;  Welch Two Sample TOST\n#&gt; \n#&gt; data:  dat$y[dat$x == 0] and dat$y[dat$x == 1]\n#&gt; df = 17.607\n#&gt; sample estimates:\n#&gt;  mean of x  mean of y \n#&gt; -0.3788551 -0.2724594 \n#&gt; \n#&gt; Epsilon: 10 \n#&gt; 95 percent two one-sided confidence interval (TOST interval):\n#&gt;  -1.058539  0.845747\n#&gt; Null hypothesis of statistical difference is: rejected \n#&gt; TOST p-value: 4.248528e-13\n\n## test with {marginaleffects} package\nh &lt;- hypotheses(mod, equivalence = c(-10, 10), df = e$parameter)[2, ]\nh\n#&gt; \n#&gt;  Term Estimate Std. Error     t Pr(&gt;|t|)   S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)   Df\n#&gt;     x    0.106      0.548 0.194    0.848 0.2 -1.05   1.26     &lt;0.001     &lt;0.001    &lt;0.001 17.6\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, df, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\n# identical p values\nh$p.value.equiv |&gt; as.vector()\n#&gt; [1] 4.248528e-13\n\ne$tost.p.value |&gt; as.vector()\n#&gt; [1] 4.248528e-13",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Equivalence Tests</span>"
    ]
  },
  {
    "objectID": "articles/experiments.html#x2-experiments",
    "href": "articles/experiments.html#x2-experiments",
    "title": "\n14¬† Experiments\n",
    "section": "\n14.1 2x2 Experiments",
    "text": "14.1 2x2 Experiments\nA 2√ó2 factorial design is a type of experimental design that allows researchers to understand the effects of two independent variables (each with two levels) on a single dependent variable. The design is popular among academic researchers as well as in industry when running A/B tests.\nIn this notebook, we illustrate how to analyze these designs with the marginaleffects package for R. As we will see, marginaleffects includes many convenient functions for analyzing both experimental and observational data, and for plotting our results.\n\n14.1.1 Fitting a Model\nWe will use the mtcars dataset. We‚Äôll analyze fuel efficiency, mpg (miles per gallon), as a function of am (transmission type) and vs (engine shape).\nvs is an indicator variable for if the car has a straight engine (1 = straight engine, 0 = V-shaped). am is an indicator variable for if the car has manual transmission (1 = manual transmission, 0=automatic transmission). There are then four types of cars (1 type for each of the four combinations of binary indicators).\nLet‚Äôs start by creating a model for fuel efficiency. For simplicity, we‚Äôll use linear regression and model the interaction between vs and am.\n\nlibrary(tidyverse)\nlibrary(marginaleffects)\nlibrary(modelsummary)\n\n## See ?mtcars for variable definitions\nfit &lt;- lm(mpg ~ vs + am + vs:am, data=mtcars) # equivalent to ~ vs*am\n\nWe can plot the predictions from the model using the plot_predictions() function. From the plot below, we can see a few things:\n\nStraight engines (vs=1) are estimated to have better expected fuel efficiency than V-shaped engines (vs=0).\nManual transmissions (am=1) are estimated to have better fuel efficiency for both V-shaped and straight engines.\nFor straight engines, the effect of manual transmissions on fuel efficiency seems to increase.\n\n\nplot_predictions(fit, by = c(\"vs\", \"am\"))\n\n\n\n\n\n14.1.2 Evaluating Effects From The Model Summary\nSince this model is fairly simple the estimated differences between any of the four possible combinations of vs and am can be read from the regression table, which we create using the modelsummary package:\n\nmodelsummary(fit, gof_map = c(\"r.squared\", \"nobs\"))\n\n\n\n\n¬†(1)\n\n\n\n(Intercept)\n15.050\n\n\n\n(1.002)\n\n\nvs\n5.693\n\n\n\n(1.651)\n\n\nam\n4.700\n\n\n\n(1.736)\n\n\nvs √ó am\n2.929\n\n\n\n(2.541)\n\n\nR2\n0.700\n\n\nNum.Obs.\n32\n\n\n\n\n\nWe can express the same results in the form of a linear equation:\n\\[ \\mbox{mpg} = 15.050 + 5.693 \\cdot \\mbox{vs} + 4.700 \\cdot \\mbox{am} + 2.929 \\cdot \\mbox{vs} \\cdot \\mbox{am}.\\]\nWith a little arithmetic, we can compute estimated differences in fuel efficiency between different groups:\n\n4.700 mpg between am=1 and am=0, when vs=0.\n5.693 mpg between vs=1 and vs=0, when am=0.\n7.629 mpg between am=1 and am=0, when vs=1.\n8.621 mpg between vs=1 and vs=0, when am=1.\n13.322 mpg between a car with am=1 and vs=1, and a car with am=0 and vs=0.\n\nReading off these differences from the model summary is relatively straightforward in very simple cases like this one. However, it becomes more difficult as more variables are added to the model, not to mention obtaining estimated standard errors becomes nightmarish. To make the process easier, we can leverage the avg_comparisons() function from the marginaleffects package to compute the appropriate quantities and standard errors.\n\n14.1.3 Using avg_comparisons() To Estimate All Differences\nThe grey rectangle in the graph below is the estimated fuel efficiency when vs=0 and am=0, that is, for an automatic transmission car with V-shaped engine.\n\n\n\n\n\nLet‚Äôs use avg_comparisons() to get the difference between straight engines and V-shaped engines when the car has automatic transmission. In this call, the variables argument indicates that we want to estimate the effect of a change of 1 unit in the vs variable. The newdata=datagrid(am=0) determines the values of the covariates at which we want to evaluate the contrast.\n\navg_comparisons(fit,\n  variables = \"vs\",\n  newdata = datagrid(am = 0))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    vs    1 - 0     5.69       1.65 3.45   &lt;0.001 10.8  2.46   8.93\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nAs expected, the results produced by avg_comparisons() are exactly the same as those which we read from the model summary table. The contrast that we just computed corresponds to the change illustrasted by the arrow in this plot:\n\n\n\n\n\nThe next difference that we compute is between manual transmissions and automatic transmissions when the car has a V-shaped engine. Again, the call to avg_comparisons() is shown below, and the corresponding contrast is indicated in the plot below using an arrow.\n\navg_comparisons(fit,\n  variables = \"am\",\n  newdata = datagrid(vs = 0))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;    am    1 - 0      4.7       1.74 2.71  0.00678 7.2   1.3    8.1\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\n\n\n\n\n\nThe third difference we estimated was between manual transmissions and automatic transmissions when the car has a straight engine. The model call and contrast are:\n\navg_comparisons(fit,\n  variables = \"am\",\n  newdata = datagrid(vs = 1))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    am    1 - 0     7.63       1.86 4.11   &lt;0.001 14.6  3.99   11.3\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\n\n\n\n\n\nThe last difference and contrast between manual transmissions with straight engines and automatic transmissions with V-shaped engines. We call this a ‚Äúcross-contrast‚Äù because we are measuring the difference between two groups that differ on two explanatory variables at the same time. To compute this contrast, we use the cross argument of avg_comparisons:\n\navg_comparisons(fit,\n  variables = c(\"am\", \"vs\"),\n  cross = TRUE)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % C: am C: vs\n#&gt;      13.3       1.65 8.07   &lt;0.001 50.3  10.1   16.6 1 - 0 1 - 0\n#&gt; \n#&gt; Columns: term, contrast_am, contrast_vs, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n\n\n\n\n\n\n14.1.4 Conclusion\nThe 2x2 design is a very popular design, and when using a linear model, the estimated differences between groups can be directly read off from the model summary, if not with a little arithmetic. However, when using models with a non-identity link function, or when seeking to obtain the standard errors for estimated differences, things become considerably more difficult. This vignette showed how to use avg_comparisons() to specify contrasts of interests and obtain standard errors for those differences. The approach used applies to all generalized linear models and effects can be further stratified using the by argument (although this is not shown in this vignette.)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "articles/experiments.html#regression-adjustment",
    "href": "articles/experiments.html#regression-adjustment",
    "title": "\n14¬† Experiments\n",
    "section": "\n14.2 Regression adjustment",
    "text": "14.2 Regression adjustment\nMany analysts who conduct and analyze experiments wish to use regression adjustment with a linear regression model to improve the precision of their estimate of the treatment effect. Unfortunately, regression adjustment can introduce small-sample bias and other undesirable properties (Freedman 2008). Lin (2013) proposes a simple strategy to fix these problems in sufficiently large samples:\n\nCenter all predictors by subtracting each of their means.\nEstimate a linear model in which the treatment is interacted with each of the covariates.\n\nThe estimatr package includes a convenient function to implement this strategy:\n\nlibrary(estimatr)\nlibrary(marginaleffects)\nlalonde &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MatchIt/lalonde.csv\")\n\nmod &lt;- lm_lin(\n    re78 ~ treat,\n    covariates = ~ age + educ + race,\n    data = lalonde,\n    se_type = \"HC3\")\nsummary(mod)\n#&gt; \n#&gt; Call:\n#&gt; lm_lin(formula = re78 ~ treat, covariates = ~age + educ + race, \n#&gt;     data = lalonde, se_type = \"HC3\")\n#&gt; \n#&gt; Standard error type:  HC3 \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper  DF\n#&gt; (Intercept)         6488.05     356.71 18.1885 2.809e-59  5787.50   7188.6 604\n#&gt; treat                489.73     878.52  0.5574 5.774e-01 -1235.59   2215.0 604\n#&gt; age_c                 85.88      35.42  2.4248 1.561e-02    16.32    155.4 604\n#&gt; educ_c               464.04     131.51  3.5286 4.495e-04   205.77    722.3 604\n#&gt; racehispan_c        2775.47    1155.40  2.4022 1.660e-02   506.38   5044.6 604\n#&gt; racewhite_c         2291.67     793.30  2.8888 4.006e-03   733.71   3849.6 604\n#&gt; treat:age_c           17.23      76.37  0.2256 8.216e-01  -132.75    167.2 604\n#&gt; treat:educ_c         226.71     308.43  0.7350 4.626e-01  -379.02    832.4 604\n#&gt; treat:racehispan_c -1057.84    2652.42 -0.3988 6.902e-01 -6266.92   4151.2 604\n#&gt; treat:racewhite_c  -1205.68    1805.21 -0.6679 5.045e-01 -4750.92   2339.6 604\n#&gt; \n#&gt; Multiple R-squared:  0.05722 ,   Adjusted R-squared:  0.04317 \n#&gt; F-statistic: 4.238 on 9 and 604 DF,  p-value: 2.424e-05\n\nWe can obtain the same results by fitting a model with the standard lm function and using the comparisons() function:\n\nmod &lt;- lm(re78 ~ treat * (age + educ + race), data = lalonde)\navg_comparisons(\n    mod,\n    variables = \"treat\",\n    vcov = \"HC3\")\n#&gt; \n#&gt;   Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  treat    1 - 0      490        879 0.557    0.577 0.8 -1232   2212\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that the treat coefficient and associate standard error in the lm_lin regression are exactly the same as the estimates produced by the comparisons() function.\n\n14.2.1 References\n\nFreedman, David A. ‚ÄúOn Regression Adjustments to Experimental Data.‚Äù Advances in Applied Mathematics 40, no. 2 (February 2008): 180‚Äì93.\nLin, Winston. ‚ÄúAgnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman‚Äôs Critique.‚Äù Annals of Applied Statistics 7, no. 1 (March 2013): 295‚Äì318. https://doi.org/10.1214/12-AOAS583.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "articles/gam.html#estimate-a-generalized-additive-model",
    "href": "articles/gam.html#estimate-a-generalized-additive-model",
    "title": "\n15¬† GAM\n",
    "section": "\n15.1 Estimate a Generalized Additive Model",
    "text": "15.1 Estimate a Generalized Additive Model\nWe will estimate a GAM model using the mgcv package and the simdat dataset distributed with the itsadug package:\n\nlibrary(marginaleffects)\nlibrary(itsadug)\nlibrary(mgcv)\n\nsimdat$Subject &lt;- as.factor(simdat$Subject)\n\ndim(simdat)\n#&gt; [1] 75600     6\nhead(simdat)\n#&gt;    Group      Time Trial Condition Subject         Y\n#&gt; 1 Adults   0.00000   -10        -1     a01 0.7554469\n#&gt; 2 Adults  20.20202   -10        -1     a01 2.7834759\n#&gt; 3 Adults  40.40404   -10        -1     a01 1.9696963\n#&gt; 4 Adults  60.60606   -10        -1     a01 0.6814298\n#&gt; 5 Adults  80.80808   -10        -1     a01 1.6939195\n#&gt; 6 Adults 101.01010   -10        -1     a01 2.3651969\n\nFit a model with a random effect and group-time smooths:\n\nmodel &lt;- bam(Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\"),\n             data = simdat)\n\nsummary(model)\n#&gt; \n#&gt; Family: gaussian \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\")\n#&gt; \n#&gt; Parametric coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)   2.0574     0.6903   2.980  0.00288 **\n#&gt; GroupAdults   3.1265     0.9763   3.202  0.00136 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Approximate significance of smooth terms:\n#&gt;                         edf Ref.df    F p-value    \n#&gt; s(Time):GroupChildren  8.26  8.850 3649  &lt;2e-16 ***\n#&gt; s(Time):GroupAdults    8.66  8.966 6730  &lt;2e-16 ***\n#&gt; s(Subject)            33.94 34.000  569  &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; R-sq.(adj) =  0.609   Deviance explained =   61%\n#&gt; fREML = 2.3795e+05  Scale est. = 31.601    n = 75600",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>GAM</span>"
    ]
  },
  {
    "objectID": "articles/gam.html#adjusted-predictions-predictions-and-plot_predictions",
    "href": "articles/gam.html#adjusted-predictions-predictions-and-plot_predictions",
    "title": "\n15¬† GAM\n",
    "section": "\n15.2 Adjusted Predictions: predictions() and plot_predictions()\n",
    "text": "15.2 Adjusted Predictions: predictions() and plot_predictions()\n\nCompute adjusted predictions for each observed combination of regressor in the dataset used to fit the model. This gives us a dataset with the same number of rows as the original data, but new columns with predicted values and uncertainty estimates:\n\npred &lt;- predictions(model)\ndim(pred)\n#&gt; [1] 75600    12\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    -1.874      0.199 -9.41   &lt;0.001 67.4 -2.2643 -1.4834\n#&gt;    -1.346      0.182 -7.41   &lt;0.001 42.8 -1.7025 -0.9901\n#&gt;    -0.819      0.167 -4.90   &lt;0.001 20.0 -1.1467 -0.4916\n#&gt;    -0.293      0.156 -1.88   0.0605  4.0 -0.5988  0.0129\n#&gt;     0.231      0.149  1.55   0.1204  3.1 -0.0606  0.5232\n#&gt;     0.753      0.146  5.17   &lt;0.001 22.0  0.4675  1.0379\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject \n#&gt; Type:  response\n\nWe can easily plot adjusted predictions for different values of a regressor using the plot_predictions() function:\n\nplot_predictions(model, condition = \"Time\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>GAM</span>"
    ]
  },
  {
    "objectID": "articles/gam.html#marginal-effects-slopes-and-plot_slopes",
    "href": "articles/gam.html#marginal-effects-slopes-and-plot_slopes",
    "title": "\n15¬† GAM\n",
    "section": "\n15.3 Marginal Effects: slopes() and plot_slopes()\n",
    "text": "15.3 Marginal Effects: slopes() and plot_slopes()\n\nMarginal effects are slopes of the prediction equation. They are an observation-level quantity. The slopes() function produces a dataset with the same number of rows as the original data, but with new columns for the slop and uncertainty estimates:\n\nmfx &lt;- slopes(model, variables = \"Time\")\nhead(mfx)\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  Time   0.0261    0.00137 19.1   &lt;0.001 267.8 0.0234 0.0288\n#&gt;  Time   0.0261    0.00136 19.2   &lt;0.001 270.4 0.0234 0.0288\n#&gt;  Time   0.0261    0.00133 19.5   &lt;0.001 280.0 0.0235 0.0287\n#&gt;  Time   0.0260    0.00128 20.3   &lt;0.001 301.4 0.0235 0.0285\n#&gt;  Time   0.0259    0.00120 21.6   &lt;0.001 340.0 0.0235 0.0282\n#&gt;  Time   0.0257    0.00109 23.5   &lt;0.001 404.4 0.0236 0.0279\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Y, Group, Time, Subject \n#&gt; Type:  response\n\nWe can plot marginal effects for different values of a regressor using the plot_slopes() function. This next plot shows the slope of the prediction equation, that is, the slope of the previous plot, at every value of the Time variable.\n\nplot_slopes(model, variables = \"Time\", condition = \"Time\")\n\n\n\n\nThe marginal effects in this plot can be interpreted as measuring the change in Y that is associated with a small increase in Time, for different baseline values of Time.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>GAM</span>"
    ]
  },
  {
    "objectID": "articles/gam.html#excluding-terms",
    "href": "articles/gam.html#excluding-terms",
    "title": "\n15¬† GAM\n",
    "section": "\n15.4 Excluding terms",
    "text": "15.4 Excluding terms\nThe predict() method of the mgcv package allows users to ‚Äúexclude‚Äù some smoothing terms, using the exclude argument. You can pass the same argument to any function in the marginaleffects package:\n\npredictions(model, newdata = \"mean\", exclude = \"s(Subject)\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  Group Time Subject\n#&gt;      11.7      0.695 16.9   &lt;0.001 210.8  10.4   13.1 Adults 1000     a01\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject \n#&gt; Type:  response\n\nSee the documentation in ?mgcv:::predict.bam for details.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>GAM</span>"
    ]
  },
  {
    "objectID": "articles/gcomputation.html#what-is-the-parametric-g-formula",
    "href": "articles/gcomputation.html#what-is-the-parametric-g-formula",
    "title": "\n16¬† G-Computation\n",
    "section": "\n16.1 What is the parametric g-formula?",
    "text": "16.1 What is the parametric g-formula?\nThe parametric g-formula is a method of standardization which can be used to address confounding problems in causal inference with observational data. It relies on the same identification assumptions as Inverse Probability Weighting (IPW), but uses different modeling assumptions. Whereas IPW models the treatment equation, standardization models the mean outcome equation. As Hern√°n and Robins note:\n\n‚ÄúBoth IP weighting and standardization are estimators of the g-formula, a general method for causal inference first described in 1986. ‚Ä¶ We say that standardization is a‚Äùplug-in g-formula estimator‚Äù because it simply replaces the conditional mean outcome in the g-formula by its estimates. When, like in Chapter 13, those estimates come from parametric models, we refer to the method as the parametric g-formula.‚Äù",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>G-Computation</span>"
    ]
  },
  {
    "objectID": "articles/gcomputation.html#how-does-it-work",
    "href": "articles/gcomputation.html#how-does-it-work",
    "title": "\n16¬† G-Computation\n",
    "section": "\n16.2 How does it work?",
    "text": "16.2 How does it work?\nImagine a causal model like this:\n\n\n\n\n\nWe want to estimate the effect of a binary treatment \\(X\\) on outcome \\(Y\\), but there is a confounding variable \\(W\\). We can use standardization with the parametric g-formula to handle this. Roughly speaking, the procedure is as follows:\n\nUse the observed data to fit a regression model with \\(Y\\) as outcome, \\(X\\) as treatment, and \\(W\\) as control variable (with perhaps some polynomials and/or interactions if there are multiple control variables).\nCreate a new dataset exactly identical to the original data, but where \\(X=1\\) in every row.\nCreate a new dataset exactly identical to the original data, but where \\(X=0\\) in every row.\nUse the model from Step 1 to compute adjusted predictions in the two counterfactual datasets from Steps 2 and 3.\nThe quantity of interest is the difference between the means of adjusted predictions in the two counterfactual datasets.\n\nThis is equivalent to computing an ‚ÄúAverage Contrast‚Äù, in which the value of \\(X\\) moves from 0 to 1. Thanks to this equivalence, we can apply the parametric g-formula method using a single line of code in marginaleffects, and obtain delta method standard errors automatically.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>G-Computation</span>"
    ]
  },
  {
    "objectID": "articles/gcomputation.html#example-with-real-world-data",
    "href": "articles/gcomputation.html#example-with-real-world-data",
    "title": "\n16¬† G-Computation\n",
    "section": "\n16.3 Example with real-world data",
    "text": "16.3 Example with real-world data\nLet‚Äôs illustrate this method by replicating an example from Chapter 13 of Hern√°n and Robins. The data come from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). The outcome is wt82_71, a measure of weight gain. The treatment is qsmk, a binary measure of smoking cessation. There are many confounders.\nStep 1 is to fit a regression model of the outcome on the treatment and control variables:\n\nlibrary(boot)\nlibrary(marginaleffects)\n\nf &lt;- wt82_71 ~ qsmk + sex + race + age + I(age * age) + factor(education) +\n     smokeintensity + I(smokeintensity * smokeintensity) + smokeyrs +\n     I(smokeyrs * smokeyrs) + factor(exercise) + factor(active) + wt71 +\n     I(wt71 * wt71) + I(qsmk * smokeintensity)\n\nurl &lt;- \"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/nhefs.csv\"\nnhefs &lt;- read.csv(url)\nnhefs &lt;- na.omit(nhefs[, all.vars(f)])\n\nfit &lt;- glm(f, data = nhefs)\n\nSteps 2 and 3 require us to replicate the full dataset by setting the qsmk treatment to counterfactual values. We can do this automatically by calling comparisons().\n\n16.3.1 TLDR\nThese simple commands do everything we need to apply the parametric g-formula:\n\navg_comparisons(fit, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nThe rest of the vignette walks through the process in a bit more detail and compares to replication code from Hern√°n and Robins.\n\n16.3.2 Adjusted Predictions\nWe can compute average predictions in the original data, and average predictions in the two counterfactual datasets like this:\n\n## average predicted outcome in the original data\np &lt;- predictions(fit)\nmean(p$estimate)\n\n[1] 2.6383\n\n## average predicted outcome in the two counterfactual datasets\np &lt;- predictions(fit, newdata = datagrid(qsmk = 0:1, grid_type = \"counterfactual\"))\naggregate(estimate ~ qsmk, data = p, FUN = mean)\n\n  qsmk estimate\n1    0 1.756213\n2    1 5.273587\n\n\nIn the R code that accompanies their book, Hern√°n and Robins compute the same quantities manually, as follows:\n\n## create a dataset with 3 copies of each subject\nnhefs$interv &lt;- -1 # 1st copy: equal to original one\n\ninterv0 &lt;- nhefs # 2nd copy: treatment set to 0, outcome to missing\ninterv0$interv &lt;- 0\ninterv0$qsmk &lt;- 0\ninterv0$wt82_71 &lt;- NA\n\ninterv1 &lt;- nhefs # 3rd copy: treatment set to 1, outcome to missing\ninterv1$interv &lt;- 1\ninterv1$qsmk &lt;- 1\ninterv1$wt82_71 &lt;- NA\n\nonesample &lt;- rbind(nhefs, interv0, interv1) # combining datasets\n\n## linear model to estimate mean outcome conditional on treatment and confounders\n## parameters are estimated using original observations only (nhefs)\n## parameter estimates are used to predict mean outcome for observations with \n## treatment set to 0 (interv=0) and to 1 (interv=1)\n\nstd &lt;- glm(f, data = onesample)\nonesample$predicted_meanY &lt;- predict(std, onesample)\n\n## estimate mean outcome in each of the groups interv=0, and interv=1\n## this mean outcome is a weighted average of the mean outcomes in each combination \n## of values of treatment and confounders, that is, the standardized outcome\nmean(onesample[which(onesample$interv == -1), ]$predicted_meanY)\n\n[1] 2.6383\n\nmean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n\n[1] 1.756213\n\nmean(onesample[which(onesample$interv == 1), ]$predicted_meanY)\n\n[1] 5.273587\n\n\nIt may be useful to note that the datagrid() function provided by marginaleffects can create counterfactual datasets automatically. This is equivalent to the onesample dataset:\n\nnd &lt;- datagrid(\n    model = fit,\n    qsmk = c(0, 1),\n    grid_type = \"counterfactual\")\n\n\n16.3.3 Contrast\nNow we want to compute the treatment effect with the parametric g-formula, which is the difference in average predicted outcomes in the two counterfactual datasets. This is equivalent to taking the average contrast with the comparisons() function. There are three important things to note in the command that follows:\n\nThe variables argument is used to indicate that we want to estimate a ‚Äúcontrast‚Äù between adjusted predictions when qsmk is equal to 1 or 0.\n\ncomparisons() automatically produces estimates of uncertainty.\n\n\navg_comparisons(std, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nUnder the hood, comparisons() did exactly what we described in the g-formula steps above:\nWe can obtain the same result by manually computing the quantities, using the replication code from Hern√°n and Robins:\n\nmean(onesample[which(onesample$interv == 1), ]$predicted_meanY) -\nmean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n\n[1] 3.517374\n\n\nAlthough manual computation is simple, it does not provide uncertainty estimates. In contrast, comparisons() has already computed the standard error and confidence interval using the delta method.\nInstead of the delta method, most analysts will rely on bootstrapping. For example, the replication code from Hern√°n and Robins does this:\n\n## function to calculate difference in means\nstandardization &lt;- function(data, indices) {\n    # create a dataset with 3 copies of each subject\n    d &lt;- data[indices, ] # 1st copy: equal to original one`\n    d$interv &lt;- -1\n    d0 &lt;- d # 2nd copy: treatment set to 0, outcome to missing\n    d0$interv &lt;- 0\n    d0$qsmk &lt;- 0\n    d0$wt82_71 &lt;- NA\n    d1 &lt;- d # 3rd copy: treatment set to 1, outcome to missing\n    d1$interv &lt;- 1\n    d1$qsmk &lt;- 1\n    d1$wt82_71 &lt;- NA\n    d.onesample &lt;- rbind(d, d0, d1) # combining datasets\n\n    # linear model to estimate mean outcome conditional on treatment and confounders\n    # parameters are estimated using original observations only (interv= -1)\n    # parameter estimates are used to predict mean outcome for observations with set\n    # treatment (interv=0 and interv=1)\n    fit &lt;- glm(f, data = d.onesample)\n\n    d.onesample$predicted_meanY &lt;- predict(fit, d.onesample)\n\n    # estimate mean outcome in each of the groups interv=-1, interv=0, and interv=1\n    return(mean(d.onesample$predicted_meanY[d.onesample$interv == 1]) -\n           mean(d.onesample$predicted_meanY[d.onesample$interv == 0]))\n}\n\n## bootstrap\nresults &lt;- boot(data = nhefs, statistic = standardization, R = 1000)\n\n## generating confidence intervals\nse &lt;- sd(results$t[, 1])\nmeant0 &lt;- results$t0\nll &lt;- meant0 - qnorm(0.975) * se\nul &lt;- meant0 + qnorm(0.975) * se\n\nbootstrap &lt;- data.frame(\n    \" \" = \"Treatment - No Treatment\",\n    estimate = meant0,\n    std.error = se,\n    conf.low = ll,\n    conf.high = ul,\n    check.names = FALSE)\nbootstrap\n\n                           estimate std.error conf.low conf.high\n1 Treatment - No Treatment 3.517374 0.4720206 2.592231  4.442518\n\n\nThe results are close to those that we obtained with comparisons(), but the confidence interval differs slightly because of the difference between bootstrapping and the delta method.\n\navg_comparisons(fit, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>G-Computation</span>"
    ]
  },
  {
    "objectID": "articles/heterogeneity.html",
    "href": "articles/heterogeneity.html",
    "title": "\n17¬† Heterogeneity\n",
    "section": "",
    "text": "This short vignette illustrates how to use recursive partitioning to explore treatment effect heterogeneity. This exercise inspired by Scholbeck et al.¬†2022 and their concept of ‚ÄúcATE‚Äù.\nAs pointed out in other vignettes, most of the quantities estimated by the marginaleffects package are ‚Äúconditional‚Äù, in the sense that they vary based on the values of all the predictors in our model. For instance, consider a Poisson regression that models the number of hourly bike rentals in Washington, DC:\n\nlibrary(marginaleffects)\nlibrary(partykit)\ndata(bikes, package = \"fmeffects\")\n\nmod &lt;- glm(\n    count ~ season * weekday + weather * temp,\n    data = bikes, family = quasipoisson)\n\nWe can use the comparisons() function to estimate how the predicted outcome changes for a 5 celsius increase in temperature:\n\ncmp &lt;- comparisons(mod, variables = list(temp = 5))\ncmp\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n temp       +5     2.78      0.763 3.64  &lt; 0.001 11.9  1.285   4.28\n temp       +5     1.54      0.600 2.58  0.01002  6.6  0.369   2.72\n temp       +5    14.98      2.438 6.14  &lt; 0.001 30.2 10.203  19.76\n temp       +5    20.73      3.172 6.54  &lt; 0.001 33.9 14.513  26.95\n temp       +5    23.44      3.685 6.36  &lt; 0.001 32.2 16.219  30.66\n--- 717 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n temp       +5    24.03      3.914 6.14  &lt; 0.001 30.2 16.355  31.70\n temp       +5    19.05      3.103 6.14  &lt; 0.001 30.2 12.974  25.14\n temp       +5     2.34      0.733 3.18  0.00145  9.4  0.898   3.77\n temp       +5     1.69      0.576 2.94  0.00328  8.3  0.564   2.82\n temp       +5    15.30      2.526 6.06  &lt; 0.001 29.4 10.347  20.25\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, count, season, weekday, weather, temp \nType:  response \n\n\nThe output printed above includes 727 rows: 1 for each of the rows in the original bikes dataset. Indeed, since the ‚Äúeffect‚Äù of a 5 unit increase depends on the values of covariates, different unit of observation will typically be associated with different contrasts.\nIn such cases, a common strategy is to compute an average difference, as described in the G-Computation vignette:\n\navg_comparisons(mod, variables = list(temp = 5))\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n temp       +5     27.1       3.72 7.29   &lt;0.001 41.5  19.8   34.4\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nAlternatively, one may be interested in exploring heterogeneity in effect sizes in different subsets of the data. A convenient way to achieve this is to use the ctree function of the partykit package. This function allows us to find with reasonably homogenous estimates, and to report useful graphical and textual summaries.\nImagine that we are particularly interested in how the effect of temperature on bike rentals varies based on day of the week and season:\n\ntree &lt;- ctree(\n    estimate ~ weekday + season,\n    data = cmp,\n    control = ctree_control(maxdepth = 2)\n)\n\nNow we can use the plot() function to draw the distributions of estimates for the effect of an increase of 5C on bike rentals, by week day and season:\n\nplot(tree)\n\n\n\n\nTo obtain conditional average estimates for each subspace, we first use the predict() function in order to place each observation in the dataset in its corresponding ‚Äúbucket‚Äù or ‚Äúnode‚Äù. Then, we use the by argument to indicate that comparisons() should compute average estimates for each of the nodes in the tree:\n\ndat &lt;- transform(bikes, nodeid = predict(tree, type = \"node\"))\ncomparisons(mod,\n    variables = list(temp = 5),\n    newdata = dat,\n    by = \"nodeid\")\n\n\n Term Contrast nodeid Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n temp mean(+5)      3     6.01      0.915 6.58   &lt;0.001 34.3  4.22   7.81\n temp mean(+5)      4     2.12      0.491 4.32   &lt;0.001 15.9  1.16   3.08\n temp mean(+5)      6    41.23      5.658 7.29   &lt;0.001 41.5 30.14  52.32\n temp mean(+5)      7    20.06      2.884 6.96   &lt;0.001 38.1 14.41  25.71\n\nColumns: term, contrast, nodeid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nThe four nodeid values correspond to the terminal nodes in this tree:\n\nprint(tree)\n\n\nModel formula:\nestimate ~ weekday + season\n\nFitted party:\n[1] root\n|   [2] weekday in Sun, Sat\n|   |   [3] season in fall, summer, winter: 6.013 (n = 156, err = 297.7)\n|   |   [4] season in spring: 2.119 (n = 54, err = 21.1)\n|   [5] weekday in Mon, Tue, Wed, Thu, Fri\n|   |   [6] season in fall, summer, winter: 41.233 (n = 392, err = 26022.7)\n|   |   [7] season in spring: 20.060 (n = 125, err = 2055.4)\n\nNumber of inner nodes:    3\nNumber of terminal nodes: 4",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Heterogeneity</span>"
    ]
  },
  {
    "objectID": "articles/ipw.html",
    "href": "articles/ipw.html",
    "title": "\n18¬† Inverse Probability Weighting\n",
    "section": "",
    "text": "Inverse Probability Weighting (IPW) is a popular technique to remove confounding in statistical modeling. It essentially involves re-weighting your sample so that it represents the population you‚Äôre interested in. Typically, we begin by estimating the predicted probability that each unit is treated. Then, we use these probabilities as weights in model fitting and in the computation of marginal effects, contrasts, risk differences, ratios, etc.\nTo illustrate, we use the Lalonde data.\n\nlibrary(marginaleffects)\ndata(\"lalonde\", package = \"MatchIt\")\nhead(lalonde)\n\n     treat age educ   race married nodegree re74 re75       re78\nNSW1     1  37   11  black       1        1    0    0  9930.0460\nNSW2     1  22    9 hispan       0        1    0    0  3595.8940\nNSW3     1  30   12  black       0        0    0    0 24909.4500\nNSW4     1  27   11  black       0        1    0    0  7506.1460\nNSW5     1  33    8  black       0        1    0    0   289.7899\nNSW6     1  22    9  black       0        1    0    0  4056.4940\n\n\nTo begin, we use a logistic regression model to estimate the probability that each unit will treated:\n\nm &lt;- glm(treat ~ age + educ + race + re74, data = lalonde, family = binomial)\n\nThen, we call predictions() to extract predicted probabilities. Note that we supply the original lalonde data explicity to the newdata argument. This ensures that all the original columns are carried over to the new dataset: dat. We also create a new column called wts that contains the inverse of the predicted probabilities:\n\ndat &lt;- predictions(m, newdata = lalonde)\ndat$wts &lt;- 1 / dat$estimate\n\nNow, we use linear regression to model the outcome of interest: personal income in 1978 (re78). Note that we use the predictions as weights in the model fitting process.\n\nmod &lt;- lm(re78 ~ treat * (age + educ + race + re74), data = dat, weights = wts)\n\nFinally, we call avg_comparisons() to compute the average treatment effect on the treated. Note that we use the wts argument to specify the weights to be used in the computation, and the newdata argument to specify the subset of observations of interest: treated individuals.\n\navg_comparisons(mod,\n    variables = \"treat\",\n    wts = \"wts\",\n    newdata = subset(dat, treat == 1)\n)\n\n\n  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat    1 - 0     1763       1397 1.26    0.207 2.3  -976   4501\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nBy default, avg_comparisons() uses the Hajek estimator, that is, the weights are normalized to sum to 1 before computation. If a user wants to use the Horvitz-Thompson estimator‚Äîwhere normalization accounts for sample size‚Äîthey can easily define a custom comparison function like this one:\n\nht &lt;- \\(hi, lo, w, newdata) {\n    (sum(hi * w) / nrow(newdata)) - (sum(lo * w) / nrow(newdata))\n}\n\ncomparisons(mod,\n    comparison = ht,\n    variables = \"treat\",\n    wts = \"wts\",\n    newdata = subset(dat, treat == 1)) \n\n\n  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat     1, 0     5245       4158 1.26    0.207 2.3 -2905  13396\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inverse Probability Weighting</span>"
    ]
  },
  {
    "objectID": "articles/logit.html#data",
    "href": "articles/logit.html#data",
    "title": "\n19¬† Logit\n",
    "section": "\n19.1 Data",
    "text": "19.1 Data\nWe focus on subset data from the GUSTO-I study, where patients were randomly assigned to accelerated tissue plasminogen activator (tPA) or streptokinase (SK).\nLoad libraries, data and fit a covariate-adjusted logistic regression model.\n\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(ggplot2)\nlibrary(rms)\n\nload(url(\n\"https://github.com/vincentarelbundock/modelarchive/raw/main/data-raw/gusto.rda\"\n))\n\ngusto &lt;- subset(gusto, tx %in% c(\"tPA\", \"SK\"))\ngusto$tx &lt;- factor(gusto$tx, levels = c(\"tPA\", \"SK\"))\n\nmod &lt;- glm(\n    day30 ~ tx + rcs(age, 4) + Killip + pmin(sysbp, 120) + lsp(pulse, 50) +\n    pmi + miloc + sex, family = \"binomial\",\n    data = gusto)\n\n\n19.1.1 One-Number Summaries\nAs usual, we can produce a one-number summary of the relationship of interest by exponentiating the coefficients, which yields an Odds Ratio (OR):\n\nmodelsummary(mod, exponentiate = TRUE, coef_omit = \"^(?!txSK)\") \n\n\n\n\n¬†(1)\n\n\n\ntxSK\n1.230\n\n\n\n(0.065)\n\n\nNum.Obs.\n30510\n\n\nAIC\n12428.6\n\n\nBIC\n12553.5\n\n\nLog.Lik.\n‚àí6199.317\n\n\nF\n173.216\n\n\nRMSE\n0.24\n\n\n\n\n\nUnlike ORs, adjusted risk differences vary from individual to individual based on the values of the control variables. The comparisons() function can compute adjusted risk differences for every individual. Here, we display only the first 6 of them:\n\ncomparisons(\n    mod,\n    variables = \"tx\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 %  97.5 %\n#&gt;    tx SK - tPA 0.001074   0.000497 2.16  0.03060 5.0 0.000100 0.00205\n#&gt;    tx SK - tPA 0.000857   0.000380 2.26  0.02410 5.4 0.000112 0.00160\n#&gt;    tx SK - tPA 0.001780   0.000779 2.29  0.02229 5.5 0.000253 0.00331\n#&gt;    tx SK - tPA 0.001137   0.000500 2.27  0.02302 5.4 0.000157 0.00212\n#&gt;    tx SK - tPA 0.001366   0.000594 2.30  0.02143 5.5 0.000202 0.00253\n#&gt; --- 30500 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;    tx SK - tPA 0.002429   0.000808 3.00  0.00266  8.6 0.000844 0.00401\n#&gt;    tx SK - tPA 0.012130   0.003900 3.11  0.00187  9.1 0.004486 0.01977\n#&gt;    tx SK - tPA 0.036812   0.010361 3.55  &lt; 0.001 11.4 0.016505 0.05712\n#&gt;    tx SK - tPA 0.022969   0.006975 3.29  &lt; 0.001 10.0 0.009298 0.03664\n#&gt;    tx SK - tPA 0.049707   0.012843 3.87  &lt; 0.001 13.2 0.024535 0.07488\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, day30, tx, age, Killip, sysbp, pulse, pmi, miloc, sex \n#&gt; Type:  response\n\nPopulation-averaged (aka ‚Äúmarginal‚Äù) adjusted risk difference (see this vignette) can be obtained using the avg_*() functions or using the by argument:\n\navg_comparisons(mod, variables = \"tx\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;    tx SK - tPA   0.0111    0.00277 4.01   &lt;0.001 14.0 0.00566 0.0165\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nThe comparisons() function above computed the predicted probability of mortality (day30==1) for each observed row of the data in two counterfactual cases: when tx is ‚ÄúSK‚Äù, and when tx is ‚ÄútPA‚Äù. Then, it computed the differences between these two sets of predictions. Finally, it computed the population-average of risk differences.\nInstead of risk differences, we could compute population-averaged (marginal) adjusted risk ratios:\n\navg_comparisons(\n    mod,\n    variables = \"tx\",\n    comparison = \"lnratioavg\",\n    transform = exp)\n#&gt; \n#&gt;  Term                 Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    tx ln(mean(SK) / mean(tPA))     1.18   &lt;0.001 13.3  1.08   1.28\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nPopulation-averaged (marginal) odds ratios:\n\navg_comparisons(\n    mod,\n    variables = \"tx\",\n    comparison = \"lnoravg\",\n    transform = \"exp\")\n#&gt; \n#&gt;  Term                 Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    tx ln(odds(SK) / odds(tPA))     1.19   &lt;0.001 13.4  1.09    1.3\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\n\n19.1.2 Unit-level Summaries\nInstead of estimating one-number summaries, we can focus on unit-level proportion differences using comparisons(). This function applies the fitted logistic regression model to predict outcome probabilities for each patient, i.e., unit-level.\n\ncmp &lt;- comparisons(mod, variables = \"tx\")\ncmp\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 %  97.5 %\n#&gt;    tx SK - tPA 0.001074   0.000497 2.16  0.03060 5.0 0.000100 0.00205\n#&gt;    tx SK - tPA 0.000857   0.000380 2.26  0.02410 5.4 0.000112 0.00160\n#&gt;    tx SK - tPA 0.001780   0.000779 2.29  0.02229 5.5 0.000253 0.00331\n#&gt;    tx SK - tPA 0.001137   0.000500 2.27  0.02302 5.4 0.000157 0.00212\n#&gt;    tx SK - tPA 0.001366   0.000594 2.30  0.02143 5.5 0.000202 0.00253\n#&gt; --- 30500 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;    tx SK - tPA 0.002429   0.000808 3.00  0.00266  8.6 0.000844 0.00401\n#&gt;    tx SK - tPA 0.012130   0.003900 3.11  0.00187  9.1 0.004486 0.01977\n#&gt;    tx SK - tPA 0.036812   0.010361 3.55  &lt; 0.001 11.4 0.016505 0.05712\n#&gt;    tx SK - tPA 0.022969   0.006975 3.29  &lt; 0.001 10.0 0.009298 0.03664\n#&gt;    tx SK - tPA 0.049707   0.012843 3.87  &lt; 0.001 13.2 0.024535 0.07488\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, day30, tx, age, Killip, sysbp, pulse, pmi, miloc, sex \n#&gt; Type:  response\n\nShow the predicted probability for individual patients under both treatment alternatives.\n\nggplot(cmp, aes(predicted_hi, predicted_lo)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, linetype = 3) +\n  coord_fixed() +\n  labs(x = \"SK\", y = \"tPA\")\n\n\n\n\nWe can present the entire distribution of unit-level proportion differences an a cumulative distribution function:\n\nggplot(cmp, aes(estimate)) + stat_ecdf()\n\n\n\n\nOr the same information as a histogram with the mean and median.\n\nggplot(cmp, aes(estimate)) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = mean(cmp$estimate), color = \"orange\") +\n  geom_vline(xintercept = median(cmp$estimate), color = \"darkgreen\") +\n  labs(x = \"SK - TPA\", title = \"Distribution of unit-level contrasts\")\n\n\n\n\n\n19.1.3 Appendix\ncomparisons() performed the following calculations under the hood:\n\nd  &lt;- gusto\n\nd$tx = \"SK\"\npredicted_hi &lt;- predict(mod, newdata = d, type = \"response\")\n\nd$tx = \"tPA\"\npredicted_lo &lt;- predict(mod, newdata = d, type = \"response\")\n\ncomparison &lt;- predicted_hi - predicted_lo\n\nThe original dataset contains 30510 patients, thus comparisons() generates an output with same amount of rows.\n\nnrow(gusto)\n#&gt; [1] 30510\n\n\nnrow(cmp)\n#&gt; [1] 30510",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Logit</span>"
    ]
  },
  {
    "objectID": "articles/lme4.html",
    "href": "articles/lme4.html",
    "title": "\n20¬† Mixed Effects\n",
    "section": "",
    "text": "This vignette replicates some of the analyses in this excellent blog post by Solomon Kurz: Use emmeans() to include 95% CIs around your lme4-based fitted lines\nLoad libraries and fit two models of chick weights:\n\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\n## unconditional linear growth model\nfit1 &lt;- lmer(\n  weight ~ 1 + Time + (1 + Time | Chick),\n  data = ChickWeight)\n\n## conditional quadratic growth model\nfit2 &lt;- lmer(\n  weight ~ 1 + Time + I(Time^2) + Diet + Time:Diet + I(Time^2):Diet + (1 + Time + I(Time^2) | Chick),\n  data = ChickWeight)\n\n\n20.0.1 Unit-level predictions\nPredict weight of each chick over time:\n\npred1 &lt;- predictions(fit1,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np1 &lt;- ggplot(pred1, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Linear growth model\")\n\npred2 &lt;- predictions(fit2,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np2 &lt;- ggplot(pred2, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Quadratic growth model\")\n\np1 + p2\n\n\n\n\nPredictions for each chick, in the 4 counterfactual worlds with different values for the Diet variable:\n\npred &lt;- predictions(fit2)\n\nggplot(pred, aes(Time, estimate, level = Chick)) +\n    geom_line() +\n    ylab(\"Predicted Weight\") +\n    facet_wrap(~ Diet, labeller = label_both)\n\n\n\n\n\n20.0.2 Population-level predictions\nTo make population-level predictions, we set the Chick variable to NA, and set re.form=NA. This last argument is offered by the lme4::predict function which is used behind the scenes to compute predictions:\n\npred &lt;- predictions(\n    fit2,\n    newdata = datagrid(Chick = NA,\n                       Diet = 1:4,\n                       Time = 0:21),\n    re.form = NA)\n\nggplot(pred, aes(x = Time, y = estimate, ymin = conf.low, ymax = conf.high)) +\n    geom_ribbon(alpha = .1, fill = \"red\") +\n    geom_line() +\n    facet_wrap(~ Diet, labeller = label_both) +\n    labs(title = \"Population-level trajectories\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Mixed Effects</span>"
    ]
  },
  {
    "objectID": "articles/matching.html#matching",
    "href": "articles/matching.html#matching",
    "title": "\n21¬† Matching\n",
    "section": "\n21.1 Matching",
    "text": "21.1 Matching\nThe first step is to pre-process the dataset to achieve better covariate balance. To do this, we use the MatchIt::matchit() function and a 1-to-1 nearest neighbor matching with replacement on the Mahaloanobis distance. This function supports many other matching methods, see ?matchit.\n\ndat &lt;- matchit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75, \n    data = lalonde, distance = \"mahalanobis\",\n    replace = TRUE)\ndat &lt;- match.data(dat)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "articles/matching.html#fitting",
    "href": "articles/matching.html#fitting",
    "title": "\n21¬† Matching\n",
    "section": "\n21.2 Fitting",
    "text": "21.2 Fitting\nNow, we estimate a linear regression model with interactions between the treatment and covariates. Note that we use the weights argument to use the weights supplied by our matching method:\n\nfit &lt;- lm(\n    re78 ~ treat * (age + educ + race + married + nodegree),\n    data = dat,\n    weights = weights)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "articles/matching.html#quantity-of-interest",
    "href": "articles/matching.html#quantity-of-interest",
    "title": "\n21¬† Matching\n",
    "section": "\n21.3 Quantity of interest",
    "text": "21.3 Quantity of interest\nFinally, we use the avg_comparisons() function of the marginaleffects package to estimate the ATT and its standard error. In effect, this function applies G-Computation to estimate the quantity of interest. We use the following arguments:\n\n\nvariables=\"treat\" indicates that we are interested in the effect of the treat variable.\n\nnewdata=subset(dat, treat == 1) indicates that we want to estimate the effect for the treated individuals only (i.e., the ATT).\n\nwts=\"weights\" indicates that we want to use the weights supplied by the matching method.\n\n\navg_comparisons(\n    fit,\n    variables = \"treat\",\n    newdata = subset(dat, treat == 1),\n    wts = \"weights\")\n\n\n  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat    1 - 0      661       1009 0.654    0.513 1.0 -1318   2639\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "articles/matching.html#learn-more",
    "href": "articles/matching.html#learn-more",
    "title": "\n21¬† Matching\n",
    "section": "\n21.4 Learn more",
    "text": "21.4 Learn more\nThe MatchIt vignette titled ‚ÄúEstimating Effects After Matching‚Äù describes many more options, including different measures of uncertainty (bootstrap, clustering, etc.), different estimands (ATE, etc.), and different strategies for adjustment.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#tidymodels",
    "href": "articles/machine_learning.html#tidymodels",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.1 tidymodels\n",
    "text": "22.1 tidymodels\n\nmarginaleffects also supports the tidymodels machine learning framework. When the underlying engine used by tidymodels to train the model is itself supported as a standalone package by marginaleffects, we can obtain both estimates and their standard errors:\n\nlibrary(tidymodels)\n\npenguins &lt;- modeldata::penguins |&gt; \n  na.omit() |&gt;\n  select(sex, island, species, bill_length_mm)\n\nmod &lt;- linear_reg(mode = \"regression\") |&gt;\n    set_engine(\"lm\") |&gt;\n    fit(bill_length_mm ~ ., data = penguins)\n\navg_comparisons(mod, type = \"numeric\", newdata = penguins)\n\n\n    Term           Contrast Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n island  Dream - Biscoe       -0.489      0.470 -1.04    0.299   1.7 -1.410  0.433\n island  Torgersen - Biscoe    0.103      0.488  0.21    0.833   0.3 -0.853  1.059\n sex     male - female         3.697      0.255 14.51   &lt;0.001 156.0  3.198  4.197\n species Chinstrap - Adelie   10.347      0.422 24.54   &lt;0.001 439.4  9.521 11.174\n species Gentoo - Adelie       8.546      0.410 20.83   &lt;0.001 317.8  7.742  9.350\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  numeric \n\navg_predictions(mod, type = \"numeric\", newdata = penguins, by = \"island\")\n\n\n    island Estimate Std. Error   z Pr(&gt;|z|)   S 2.5 % 97.5 %\n Biscoe        45.2      0.182 248   &lt;0.001 Inf  44.9   45.6\n Dream         44.2      0.210 211   &lt;0.001 Inf  43.8   44.6\n Torgersen     39.0      0.339 115   &lt;0.001 Inf  38.4   39.7\n\nColumns: island, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  numeric \n\n\nWhen the underlying engine that tidymodels uses to fit the model is not supported by marginaleffects as a standalone model, we can also obtain correct results, but no uncertainy estimates. Here is a random forest model:\n\nlibrary(modelsummary)\n\n# pre-processing\npre &lt;- penguins |&gt;\n    recipe(sex ~ ., data = _) |&gt;\n    step_ns(bill_length_mm, deg_free = 4) |&gt;\n    step_dummy(all_nominal_predictors())\n\n# modelling strategies\nmodels &lt;- list(\n  \"Logit\" = logistic_reg(mode = \"classification\", engine = \"glm\"),\n  \"Random Forest\" = rand_forest(mode = \"classification\", engine = \"ranger\"),\n  \"XGBoost\" = boost_tree(mode = \"classification\", engine = \"xgboost\")\n)\n\n# fit to data\nfits &lt;- lapply(models, \\(x) {\n  pre |&gt;\n  workflow(spec = x) |&gt;\n  fit(penguins)\n})\n\n# marginaleffects\ncmp &lt;- lapply(fits, avg_comparisons, newdata = penguins, type = \"prob\")\n\n# summary table\nmodelsummary(\n  cmp,\n  shape = term + contrast + group ~ model,\n  coef_omit = \"sex\",\n  coef_rename = coef_rename)\n\n\n\n\n\n\nLogit\nRandom Forest\n¬†XGBoost\n\n\n\nBill Length Mm\n+1\nfemale\n‚àí0.101\n‚àí0.059\n‚àí0.075\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\nmale\n0.101\n0.059\n0.075\n\n\n\n\n\n(0.004)\n\n\n\n\nIsland\nDream - Biscoe\nfemale\n‚àí0.044\n0.014\n‚àí0.004\n\n\n\n\n\n(0.069)\n\n\n\n\n\n\nmale\n0.044\n‚àí0.014\n0.004\n\n\n\n\n\n(0.069)\n\n\n\n\n\nTorgersen - Biscoe\nfemale\n0.015\n‚àí0.054\n0.008\n\n\n\n\n\n(0.074)\n\n\n\n\n\n\nmale\n‚àí0.015\n0.054\n‚àí0.008\n\n\n\n\n\n(0.074)\n\n\n\n\nSpecies\nChinstrap - Adelie\nfemale\n0.562\n0.153\n0.441\n\n\n\n\n\n(0.036)\n\n\n\n\n\n\nmale\n‚àí0.562\n‚àí0.153\n‚àí0.441\n\n\n\n\n\n(0.036)\n\n\n\n\n\nGentoo - Adelie\nfemale\n0.453\n0.112\n0.361\n\n\n\n\n\n(0.025)\n\n\n\n\n\n\nmale\n‚àí0.453\n‚àí0.112\n‚àí0.361\n\n\n\n\n\n(0.025)\n\n\n\n\nNum.Obs.\n\n\n333\n\n\n\n\nAIC\n\n\n302.2\n\n\n\n\nBIC\n\n\n336.4\n\n\n\n\nLog.Lik.\n\n\n‚àí142.082",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#mlr3",
    "href": "articles/machine_learning.html#mlr3",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.2 mlr3\n",
    "text": "22.2 mlr3\n\nmlr3 is a machine learning framework for R. It makes it possible for users to train a wide range of models, including linear models, random forests, gradient boosting machines, and neural networks.\nIn this example, we use the bikes dataset supplied by the fmeffects package to train a random forest model predicting the number of bikes rented per hour. We then use marginaleffects to interpret the results of the model.\n\ndata(\"bikes\", package = \"fmeffects\")\n\ntask &lt;- as_task_regr(x = bikes, id = \"bikes\", target = \"count\")\nforest &lt;- lrn(\"regr.ranger\")$train(task)\n\nAs described in other vignettes, we can use the avg_comparisons() function to compute the average change in predicted outcome that is associated with a change in each feature:\n\navg_comparisons(forest, newdata = bikes)\n\n\n       Term      Contrast Estimate\n count      +1               0.000\n holiday    False - True    14.001\n humidity   +1             -23.438\n month      +1               3.895\n season     spring - fall  -29.351\n season     summer - fall   -7.390\n season     winter - fall    4.739\n temp       +1               3.603\n weather    misty - clear   -7.934\n weather    rain - clear   -59.467\n weekday    Fri - Sun       69.384\n weekday    Mon - Sun       77.766\n weekday    Sat - Sun       18.296\n weekday    Thu - Sun       85.197\n weekday    Tue - Sun       83.673\n weekday    Wed - Sun       85.878\n windspeed  +1               0.294\n workingday False - True  -188.588\n year       1 - 0           98.440\n\nColumns: term, contrast, estimate \nType:  response \n\n\nThese results are easy to interpret: An increase of 1 degree Celsius in the temperature is associated with an increase of 3.603 bikes rented per hour.\nWe could obtain the same result manually as follows:\n\nlo &lt;- transform(bikes, temp = temp - 0.5)\nhi &lt;- transform(bikes, temp = temp + 0.5)\nmean(predict(forest, newdata = hi) - predict(forest, newdata = lo))\n\n[1] 3.603054",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#simultaneous-changes",
    "href": "articles/machine_learning.html#simultaneous-changes",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.3 Simultaneous changes",
    "text": "22.3 Simultaneous changes\nWith marginaleffects::avg_comparisons(), we can also compute the average effect of a simultaneous change in multiple predictors, using the variables and cross arguments. In this example, we see what happens (on average) to the predicted outcome when the temp, season, and weather predictors all change together:\n\navg_comparisons(\n    forest,\n    variables = c(\"temp\", \"season\", \"weather\"),\n    cross = TRUE,\n    newdata = bikes)\n\n\n Estimate     C: season C: temp    C: weather\n  -32.469 spring - fall      +1 misty - clear\n  -76.390 spring - fall      +1 rain - clear \n  -11.574 summer - fall      +1 misty - clear\n  -60.974 summer - fall      +1 rain - clear \n    0.334 winter - fall      +1 misty - clear\n  -53.340 winter - fall      +1 rain - clear \n\nColumns: term, contrast_season, contrast_temp, contrast_weather, estimate \nType:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#fmeffects-forward-vs.-centered-effects",
    "href": "articles/machine_learning.html#fmeffects-forward-vs.-centered-effects",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.4 fmeffects: Forward vs.¬†centered effects",
    "text": "22.4 fmeffects: Forward vs.¬†centered effects\nAs the code in the mlr3 section makes clear, the avg_comparisons() computes the effect of a ‚Äúcentered‚Äù change on the outcome. If we want to compute a ‚ÄúForward Marginal Effect‚Äù instead, we can call:\n\navg_comparisons(\n    forest,\n    variables = list(\"temp\" = \\(x) data.frame(x, x + 1)),\n    newdata = bikes)\n\n\n Term Contrast Estimate\n temp   custom     2.39\n\nColumns: term, contrast, estimate \nType:  response \n\n\nThis is equivalent to using the fmeffects package:\n\nfmeffects::fme(\n    model = forest,\n    data = bikes,\n    target = \"count\",\n    feature = \"temp\",\n    step.size = 1)$ame \n\n[1] 2.386648",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#partial-dependence-plots",
    "href": "articles/machine_learning.html#partial-dependence-plots",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.5 Partial Dependence Plots",
    "text": "22.5 Partial Dependence Plots\n\n# https://stackoverflow.com/questions/67634344/r-partial-dependence-plots-from-workflow\nlibrary(\"tidymodels\")\nlibrary(\"marginaleffects\")\ndata(ames, package = \"modeldata\")\n\ndat &lt;- transform(ames,\n    Sale_Price = log10(Sale_Price),\n    Gr_Liv_Area = as.numeric(Gr_Liv_Area))\n\nm &lt;- dat |&gt; \n    recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Bldg_Type, data = _) |&gt;\n    workflow(spec = rand_forest(mode = \"regression\", trees = 1000, engine = \"ranger\")) |&gt;\n    fit(data = dat)\n\n# Percentiles of the x-axis variable\npctiles &lt;- quantile(dat$Gr_Liv_Area, probs = seq(0, 1, length.out = 101))\n\n# Select 1000 profiles at random, otherwise this is very memory-intensive\nprofiles &lt;- dat[sample(nrow(dat), 1000), ]\n\n# Use datagridcf() to replicate the full dataset 101 times. Each time, we\n# replace the value of `Gr_Liv_Area` by one of the percentiles, but keep the\n# other profile features as observed.\nnd &lt;- datagridcf(Gr_Liv_Area = pctiles, newdata = profiles)\n\n# Partial dependence plot\nplot_predictions(m,\n  newdata = nd,\n  by = c(\"Gr_Liv_Area\", \"Bldg_Type\")) +\n  labs(x = \"Living Area\", y = \"Predicted log10(Sale Price)\", color = \"Building Type\")\n\n\n\n\nWe can replicate this plot using the DALEXtra package:\n\nlibrary(\"DALEXtra\")\npdp_rf &lt;- explain_tidymodels(\n    m,\n    data = dplyr::select(dat, -Sale_Price),\n    y = dat$Sale_Price,\n    label = \"random forest\",\n    verbose = FALSE)\npdp_rf &lt;- model_profile(pdp_rf,\n    N = 1000,\n    variables = \"Gr_Liv_Area\",\n    groups = \"Bldg_Type\")\nplot(pdp_rf)\n\n\n\n\nNote that marginaleffects and DALEXtra plots are not exactly identical because the randomly sampled profiles are not the same. You can try the same procedure without sampling ‚Äî or equivalently with N=2930 ‚Äî to see a perfect equivalence.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/machine_learning.html#other-plots",
    "href": "articles/machine_learning.html#other-plots",
    "title": "\n22¬† Machine Learning\n",
    "section": "\n22.6 Other Plots",
    "text": "22.6 Other Plots\nWe can plot the results using the standard marginaleffects helpers. For example, to plot predictions, we can do:\n\nlibrary(mlr3verse)\ndata(\"bikes\", package = \"fmeffects\")\ntask &lt;- as_task_regr(x = bikes, id = \"bikes\", target = \"count\")\nforest &lt;- lrn(\"regr.ranger\")$train(task)\n\nplot_predictions(forest, condition = \"temp\", newdata = bikes)\n\n\n\n\nAs documented in ?plot_predictions, using condition=\"temp\" is equivalent to creating an equally-spaced grid of temp values, and holding all other predictors at their means or modes. In other words, it is equivalent to:\n\nd &lt;- datagrid(temp = seq(min(bikes$temp), max(bikes$temp), length.out = 100), newdata = bikes)\np &lt;- predict(forest, newdata = d)\nplot(d$temp, p, type = \"l\")\n\nAlternatively, we could plot ‚Äúmarginal‚Äù predictions, where replicate the full dataset once for every value of temp, and then average the predicted values over each value of the x-axis:\n\nplot_predictions(forest, by = \"temp\", newdata = bikes)\n\n\n\n\nOf course, we can customize the plot using all the standard ggplot2 functions:\n\nplot_predictions(forest, by = \"temp\", newdata = d) +\n    geom_point(data = bikes, aes(x = temp, y = count), alpha = 0.1) +\n    geom_smooth(data = bikes, aes(x = temp, y = count), se = FALSE, color = \"orange\") +\n    labs(x = \"Temperature (Celsius)\", y = \"Predicted number of bikes rented per hour\",\n         title = \"Black: random forest predictions. Orange: LOESS smoother.\") +\n    theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "articles/multiple_imputation.html#mice",
    "href": "articles/multiple_imputation.html#mice",
    "title": "\n23¬† Missing Data\n",
    "section": "\n23.1 mice\n",
    "text": "23.1 mice\n\nFirst, we impute the dataset using the mice package:\n\nlibrary(mice)\n\ndat_mice &lt;- mice(dat, m = 20, printFlag = FALSE, .Random.seed = 1024)\n\nThen, we use the standard mice syntax to produce an object of class mira with all the models:\n\nmod_mice &lt;- with(dat_mice, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\n\nFinally, we feed the mira object to a marginaleffects function:\n\nmfx_mice &lt;- avg_slopes(mod_mice, by = \"Species\")\nmfx_mice\n#&gt; \n#&gt;          Term                        Contrast    Species Estimate Std. Error      t Pr(&gt;|t|)     S   2.5 % 97.5 %    Df\n#&gt;  Sepal.Length mean(dY/dX)                     setosa       0.0684     0.0560  1.222  0.22414   2.2 -0.0424  0.179 120.0\n#&gt;  Sepal.Length mean(dY/dX)                     versicolor   0.0540     0.0558  0.968  0.33550   1.6 -0.0567  0.165  93.6\n#&gt;  Sepal.Length mean(dY/dX)                     virginica    0.0582     0.0512  1.137  0.25818   2.0 -0.0434  0.160 101.2\n#&gt;  Sepal.Width  mean(dY/dX)                     setosa       0.1890     0.0836  2.260  0.02436   5.4  0.0246  0.353 400.5\n#&gt;  Sepal.Width  mean(dY/dX)                     versicolor   0.2092     0.0772  2.710  0.00807   7.0  0.0558  0.363  89.0\n#&gt;  Sepal.Width  mean(dY/dX)                     virginica    0.2242     0.1041  2.155  0.03506   4.8  0.0162  0.432  61.8\n#&gt;  Species      mean(versicolor) - mean(setosa) setosa       1.1399     0.0977 11.668  &lt; 0.001  68.1  0.9464  1.333 114.8\n#&gt;  Species      mean(versicolor) - mean(setosa) versicolor   1.1399     0.0977 11.668  &lt; 0.001  68.1  0.9464  1.333 114.8\n#&gt;  Species      mean(versicolor) - mean(setosa) virginica    1.1399     0.0977 11.668  &lt; 0.001  68.1  0.9464  1.333 114.8\n#&gt;  Species      mean(virginica) - mean(setosa)  setosa       1.7408     0.1108 15.709  &lt; 0.001 100.7  1.5214  1.960 121.6\n#&gt;  Species      mean(virginica) - mean(setosa)  versicolor   1.7408     0.1108 15.709  &lt; 0.001 100.7  1.5214  1.960 121.6\n#&gt;  Species      mean(virginica) - mean(setosa)  virginica    1.7408     0.1108 15.709  &lt; 0.001 100.7  1.5214  1.960 121.6\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, s.value, predicted_lo, predicted_hi, predicted, df, statistic, p.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "articles/multiple_imputation.html#amelia",
    "href": "articles/multiple_imputation.html#amelia",
    "title": "\n23¬† Missing Data\n",
    "section": "\n23.2 Amelia\n",
    "text": "23.2 Amelia\n\nWith Amelia, the workflow is essentially the same. First, we impute using Amelia:\n\nlibrary(Amelia)\n\ndat_amelia &lt;- amelia(dat, m = 20, noms = \"Species\", p2s = 0)\n\nThen, we use Amelia syntax to produce an object of class amest with all the models:\n\nmod_amelia &lt;- with(dat_amelia, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\n\nFinally, we feed the amest object to a marginaleffects function:\n\nmfx_amelia &lt;- avg_slopes(mod_amelia, by = \"Species\")\nmfx_amelia\n#&gt; \n#&gt;          Term                        Contrast    Species Estimate Std. Error      t Pr(&gt;|t|)    S  2.5 % 97.5 %   Df\n#&gt;  Sepal.Length mean(dY/dX)                     setosa       0.3878     0.0907  4.278  &lt; 0.001 13.3  0.205 0.5705 43.9\n#&gt;  Sepal.Length mean(dY/dX)                     versicolor   0.3231     0.0802  4.029  &lt; 0.001 12.5  0.162 0.4838 56.0\n#&gt;  Sepal.Length mean(dY/dX)                     virginica    0.3467     0.0799  4.340  &lt; 0.001 13.6  0.186 0.5077 44.7\n#&gt;  Sepal.Width  mean(dY/dX)                     setosa      -0.2079     0.1491 -1.394  0.16878  2.6 -0.507 0.0909 55.0\n#&gt;  Sepal.Width  mean(dY/dX)                     versicolor  -0.1157     0.1168 -0.991  0.32647  1.6 -0.350 0.1187 51.8\n#&gt;  Sepal.Width  mean(dY/dX)                     virginica   -0.0452     0.1272 -0.355  0.72323  0.5 -0.298 0.2079 82.1\n#&gt;  Species      mean(versicolor) - mean(setosa) setosa       0.6127     0.1731  3.541  0.00111  9.8  0.262 0.9635 36.7\n#&gt;  Species      mean(versicolor) - mean(setosa) versicolor   0.6127     0.1731  3.541  0.00111  9.8  0.262 0.9635 36.7\n#&gt;  Species      mean(versicolor) - mean(setosa) virginica    0.6127     0.1731  3.541  0.00111  9.8  0.262 0.9635 36.7\n#&gt;  Species      mean(virginica) - mean(setosa)  setosa       1.0364     0.2004  5.171  &lt; 0.001 16.6  0.629 1.4436 34.2\n#&gt;  Species      mean(virginica) - mean(setosa)  versicolor   1.0364     0.2004  5.171  &lt; 0.001 16.6  0.629 1.4436 34.2\n#&gt;  Species      mean(virginica) - mean(setosa)  virginica    1.0364     0.2004  5.171  &lt; 0.001 16.6  0.629 1.4436 34.2\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, s.value, predicted_lo, predicted_hi, predicted, df, statistic, p.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "articles/multiple_imputation.html#other-imputation-packages-missranger-or-lists-of-imputed-data-frames.",
    "href": "articles/multiple_imputation.html#other-imputation-packages-missranger-or-lists-of-imputed-data-frames.",
    "title": "\n23¬† Missing Data\n",
    "section": "\n23.3 Other imputation packages: missRanger, or lists of imputed data frames.",
    "text": "23.3 Other imputation packages: missRanger, or lists of imputed data frames.\nSeveral R packages can impute missing data. Indeed, the Missing Data CRAN View lists at least a dozen alternatives. Since user interfaces change a lot from package to package, marginaleffects supports a single workflow that can be used, with some adaptation, with all imputation packages:\n\nUse an external package to create a list of imputed data frames.\nApply the datalist2mids() function from the miceadds package to convert the list of imputed data frames to a mids object.\nUse the with() function to fit models to create mira object, as illustrated in the mice and Amelia sections above.\nPass the mira object to a marginaleffects function.\n\nConsider the imputation package missRanger, which generates a list of imputed datasets:\n\nlibrary(miceadds)\nlibrary(missRanger)\n\n## convert lists of imputed datasets to `mids` objects\ndat_missRanger &lt;- replicate(20, missRanger(dat, verbose = 0), simplify = FALSE)\nmids_missRanger &lt;- datlist2mids(dat_missRanger)\n\n## fit models\nmod_missRanger &lt;- with(mids_missRanger, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\n\n## `missRanger` slopes\nmfx_missRanger &lt;- avg_slopes(mod_missRanger, by = \"Species\")\nmfx_missRanger\n#&gt; \n#&gt;          Term                        Contrast    Species Estimate Std. Error     t Pr(&gt;|t|)     S    2.5 % 97.5 %      Df\n#&gt;  Sepal.Length mean(dY/dX)                     setosa       0.0586     0.0414  1.42  0.15672   2.7 -0.02249  0.140 2780689\n#&gt;  Sepal.Length mean(dY/dX)                     versicolor   0.0675     0.0392  1.72  0.08514   3.6 -0.00934  0.144  721339\n#&gt;  Sepal.Length mean(dY/dX)                     virginica    0.0643     0.0367  1.75  0.07986   3.6 -0.00766  0.136 1020604\n#&gt;  Sepal.Width  mean(dY/dX)                     setosa       0.2314     0.0690  3.35  &lt; 0.001  10.3  0.09612  0.367 1911621\n#&gt;  Sepal.Width  mean(dY/dX)                     versicolor   0.2186     0.0551  3.97  &lt; 0.001  13.8  0.11066  0.327  246676\n#&gt;  Sepal.Width  mean(dY/dX)                     virginica    0.2089     0.0687  3.04  0.00237   8.7  0.07420  0.344  194885\n#&gt;  Species      mean(versicolor) - mean(setosa) setosa       1.1589     0.0704 16.46  &lt; 0.001 199.8  1.02091  1.297 1115135\n#&gt;  Species      mean(versicolor) - mean(setosa) versicolor   1.1589     0.0704 16.46  &lt; 0.001 199.8  1.02091  1.297 1115135\n#&gt;  Species      mean(versicolor) - mean(setosa) virginica    1.1589     0.0704 16.46  &lt; 0.001 199.8  1.02091  1.297 1115135\n#&gt;  Species      mean(virginica) - mean(setosa)  setosa       1.7781     0.0822 21.64  &lt; 0.001 342.5  1.61703  1.939 1547086\n#&gt;  Species      mean(virginica) - mean(setosa)  versicolor   1.7781     0.0822 21.64  &lt; 0.001 342.5  1.61703  1.939 1547086\n#&gt;  Species      mean(virginica) - mean(setosa)  virginica    1.7781     0.0822 21.64  &lt; 0.001 342.5  1.61703  1.939 1547086\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, s.value, predicted_lo, predicted_hi, predicted, df, statistic, p.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "articles/multiple_imputation.html#comparing-results-with-different-imputation-software",
    "href": "articles/multiple_imputation.html#comparing-results-with-different-imputation-software",
    "title": "\n23¬† Missing Data\n",
    "section": "\n23.4 Comparing results with different imputation software",
    "text": "23.4 Comparing results with different imputation software\nWe can use the modelsummary package to compare the results with listwise deletion to the results using different imputations software:\n\nlibrary(modelsummary)\n\n## listwise deletion slopes\nmod_lwd &lt;- lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species, data = dat)\nmfx_lwd &lt;- avg_slopes(mod_lwd, by = \"Species\")\n\n## regression table\nmodels &lt;- list(\n    \"LWD\" = mfx_lwd,\n    \"mice\" = mfx_mice,\n    \"Amelia\" = mfx_amelia,\n    \"missRanger\" = mfx_missRanger)\nmodelsummary(models, shape = term : contrast + Species ~ model)\n\n\n\n\nSpecies\nLWD\nmice\nAmelia\nmissRanger\n\n\n\nSepal.Length mean(dY/dX)\nsetosa\n0.033\n0.068\n0.388\n0.059\n\n\n\nsetosa\n(0.061)\n(0.056)\n(0.091)\n(0.041)\n\n\n\nversicolor\n0.050\n0.054\n0.323\n0.067\n\n\n\nversicolor\n(0.061)\n(0.056)\n(0.080)\n(0.039)\n\n\n\nvirginica\n0.043\n0.058\n0.347\n0.064\n\n\n\nvirginica\n(0.058)\n(0.051)\n(0.080)\n(0.037)\n\n\nSepal.Width mean(dY/dX)\nsetosa\n0.274\n0.189\n‚àí0.208\n0.231\n\n\n\nsetosa\n(0.091)\n(0.084)\n(0.149)\n(0.069)\n\n\n\nversicolor\n0.255\n0.209\n‚àí0.116\n0.219\n\n\n\nversicolor\n(0.074)\n(0.077)\n(0.117)\n(0.055)\n\n\n\nvirginica\n0.234\n0.224\n‚àí0.045\n0.209\n\n\n\nvirginica\n(0.083)\n(0.104)\n(0.127)\n(0.069)\n\n\nSpecies mean(versicolor) - mean(setosa)\nsetosa\n1.157\n1.140\n0.613\n1.159\n\n\n\nsetosa\n(0.097)\n(0.098)\n(0.173)\n(0.070)\n\n\n\nversicolor\n1.157\n1.140\n0.613\n1.159\n\n\n\nversicolor\n(0.097)\n(0.098)\n(0.173)\n(0.070)\n\n\n\nvirginica\n1.157\n1.140\n0.613\n1.159\n\n\n\nvirginica\n(0.097)\n(0.098)\n(0.173)\n(0.070)\n\n\nSpecies mean(virginica) - mean(setosa)\nsetosa\n1.839\n1.741\n1.036\n1.778\n\n\n\nsetosa\n(0.123)\n(0.111)\n(0.200)\n(0.082)\n\n\n\nversicolor\n1.839\n1.741\n1.036\n1.778\n\n\n\nversicolor\n(0.123)\n(0.111)\n(0.200)\n(0.082)\n\n\n\nvirginica\n1.839\n1.741\n1.036\n1.778\n\n\n\nvirginica\n(0.123)\n(0.111)\n(0.200)\n(0.082)\n\n\nNum.Obs.\n\n60\n150\n150\n150\n\n\nNum.Imp.\n\n\n20\n20\n20\n\n\nR2\n\n0.953\n0.930\n0.853\n0.947\n\n\nR2 Adj.\n\n0.949\n0.928\n0.848\n0.945\n\n\nAIC\n\n‚àí34.0\n\n\n\n\n\nBIC\n\n‚àí19.3\n\n\n\n\n\nLog.Lik.\n\n23.997\n\n\n\n\n\nF\n\n220.780\n\n\n\n\n\nRMSE\n\n0.16",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "articles/multiple_imputation.html#passing-new-data-arguments-newdata-wts-by-etc.",
    "href": "articles/multiple_imputation.html#passing-new-data-arguments-newdata-wts-by-etc.",
    "title": "\n23¬† Missing Data\n",
    "section": "\n23.5 Passing new data arguments: newdata, wts, by, etc.",
    "text": "23.5 Passing new data arguments: newdata, wts, by, etc.\nSometimes we want to pass arguments changing or specifying the data on which we will do our analysis using marginaleffects. This can be for reasons such as wanting to specify the values or weights at which we evaluate e.g.¬†avg_slopes(), or due to the underlying models not robustly preserving all the original data columns (such as fixest objects not saving their data in the fit object making it potentially challenging to retrieve, and even if retrievable it will not include the weights used during fitting as a column as wts expects when given a string).\nIf we are not using multiple imputation, or if we want to just pass a single dataset to the several fitted models after multiple imputation, we can pass a single dataset to the newdata argument. However, if we wish to supply each model in our list resulting after multiple imputation with a /different/ dataset on which to calculate results, we cannot use newdata. Instead, in this case it can be useful to revert to a more manual (but still very easy) approach. Here is an example calculating avg_slopes() using a different set of weights for each of the fixest models which we fit after multiple imputation.\n\nset.seed(1024)\nlibrary(mice)\nlibrary(fixest)\nlibrary(marginaleffects)\n\ndat &lt;- mtcars\n\n## insert missing values\ndat$hp[sample(seq_len(nrow(mtcars)), 10)] &lt;- NA\ndat$mpg[sample(seq_len(nrow(mtcars)), 10)] &lt;- NA\ndat$gear[sample(seq_len(nrow(mtcars)), 10)] &lt;- NA\n\n## multiple imputation\ndat &lt;- mice(dat, m = 5, method = \"sample\", printFlag = FALSE)\ndat &lt;- complete(dat, action = \"all\")\n\n## fit models\nmod &lt;- lapply(dat, \\(x) \n    feglm(am ~ mpg * cyl + hp,\n        weight = ~gear,\n        family = binomial,\n        data = x))\n\n## slopes without weights\nlapply(seq_along(mod), \\(i) \n    avg_slopes(mod[[i]], newdata = dat[[i]])) |&gt;\n    mice::pool()\n#&gt; Class: mipo    m = 5 \n#&gt;   term m     estimate         ubar            b            t dfcom       df      riv    lambda       fmi\n#&gt; 1  cyl 5 -0.134279397 7.096230e-04 2.347149e-03 3.526201e-03    29 2.921624 3.969119 0.7987571 0.8667259\n#&gt; 2   hp 5  0.001649797 5.709333e-07 1.375524e-06 2.221563e-06    29 3.557012 2.891107 0.7430037 0.8213920\n#&gt; 3  mpg 5  0.006083006 1.080610e-04 2.722367e-04 4.347450e-04    29 3.458500 3.023146 0.7514383 0.8284103\n\n## slopes with weights\nlapply(seq_along(mod), \\(i) \n    avg_slopes(mod[[i]], newdata = dat[[i]], wts = \"gear\")) |&gt;\n    mice::pool()\n#&gt; Class: mipo    m = 5 \n#&gt;   term m     estimate         ubar            b            t dfcom       df      riv    lambda       fmi\n#&gt; 1  cyl 5 -0.135839559 7.280346e-04 2.480980e-03 3.705211e-03    29 2.868608 4.089333 0.8035106 0.8704735\n#&gt; 2   hp 5  0.001671181 5.697784e-07 1.424665e-06 2.279376e-06    29 3.474886 3.000461 0.7500288 0.8272414\n#&gt; 3  mpg 5  0.006251264 1.056056e-04 2.705362e-04 4.302490e-04    29 3.422454 3.074113 0.7545478 0.8309834",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "articles/mrp.html#mrp-for-surveys-and-market-research",
    "href": "articles/mrp.html#mrp-for-surveys-and-market-research",
    "title": "\n24¬† MrP\n",
    "section": "\n24.1 MrP for surveys and market research",
    "text": "24.1 MrP for surveys and market research\nImagine that a national supermarket chain plans to introduce a line of meat substitutes in some of its stores. To guide marketing and distribution efforts, the company would like to know the share of the population in each city that is interested in tasting meat substitutes.\nThe company conducts a telephone survey of 1000 randomly selected adults from 40 large American cities. For each survey respondent, we record the city of residence, age, level of education, and whether they are interested in tasting meat substitutes. The variable we focus on is ‚Äúinterest in meat substitutes,‚Äù measured on a scale of 1 to 7 where 7 means ‚Äúvery interested‚Äù and 1 means ‚Äúnot at all interested‚Äù. Our ultimate goal is to estimate the average of this 7 point scale for each city.\nThe (simulated) data that we will use is stored in a R data frame called survey. We can use the nrow() function to confirm the sample size, and the datasummary_df() function from the modelsummary package to display the first few rows of data:\n\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(brms)\n\nnrow(survey)\n#&gt; [1] 1000\n\ndatasummary_df(survey[1:5, ])\n\n\n\nrespondent\ncity\nage\neducation\nmeat_substitute\n\n\n\n0001\nTucson, AZ\n18-54\nHigh school or less\n1.00\n\n\n0002\nMiami, FL\n55+\nHigh school or less\n6.00\n\n\n0003\nAustin, TX\n55+\nPost-secondary\n3.00\n\n\n0004\nAtlanta, GA\n18-54\nPost-secondary\n5.00\n\n\n0005\nMilwaukee, WI\n18-54\nHigh school or less\n5.00\n\n\n\n\n\nThis dataset includes 1000 observations: one per survey respondent. Unfortunately, it is not straightforward to compute precise city-by-city estimates, because although the number of respondents is large overall, the number of respondents within each of the 40 cities is relatively small. Moreover, the company‚Äôs sampling strategy does not guarantee that subsamples are representative of each city‚Äôs population. MrP can help us circumvent these problems in two steps:\n\n\nMultilevel regression (Mr): Estimate a multilevel regression at the individual level, with random intercepts for cities.\n\nPoststratification (P): Adjust the predictions of the model based on the demographic characteristics of each city.\n\nIn the rest of this notebook, we show that the marginaleffects package makes it very easy to apply these steps.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>MrP</span>"
    ]
  },
  {
    "objectID": "articles/mrp.html#mr-for-multilevel-regression",
    "href": "articles/mrp.html#mr-for-multilevel-regression",
    "title": "\n24¬† MrP\n",
    "section": "\n24.2 ‚ÄúMr‚Äù for ‚ÄúMultilevel regression‚Äù",
    "text": "24.2 ‚ÄúMr‚Äù for ‚ÄúMultilevel regression‚Äù\nThe first step of MrP is to use individual-level data to estimate a model that predicts the value of the variable of interest. One of the great benefits of using marginaleffects for MrP, is that this package is agnostic to the type of model used to make predictions. Analysts can use almost any model they like, and the workflow described below would remain the same.\nOne popular choice for MrP is to estimate a multilevel regression model with random intercepts for each of the geographic regions of interest. To do so, analysts could use many different R packages, including lme4, glmmTMB, or brms. In this notebook, we use the brms::brm() function to estimate a bayesian multilevel model, with the age and education variables as fixed components, and random intercepts at the city level:\n\nmod &lt;- brm(meat_substitute ~ age + education + (1 | city), data = survey)\n\nWe can visualize the model estimates using the modelplot() function from the modelsummary package:\n\nmodelplot(mod)\n\n\n\n\nWe are now ready for the second MrP step: poststratification.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>MrP</span>"
    ]
  },
  {
    "objectID": "articles/mrp.html#p-for-poststratification",
    "href": "articles/mrp.html#p-for-poststratification",
    "title": "\n24¬† MrP\n",
    "section": "\n24.3 ‚ÄúP‚Äù for ‚ÄúPoststratification‚Äù",
    "text": "24.3 ‚ÄúP‚Äù for ‚ÄúPoststratification‚Äù\nIn the second MrP step, we use data from the US Census (or a similar source) to create a ‚Äúpoststratification table.‚Äù This table includes one row for each combination of the predictor values in our model. In our model, the age variable has 2 categories (‚Äú18-54‚Äù and ‚Äú54+‚Äù); the education variables has 2 categories (‚ÄúHigh school or less‚Äù and ‚ÄúPost-secondary‚Äù); and the city variable has 40 unique entries. Therefore, the poststratification table must have \\(2 \\times 2 \\times 40 = 160\\) entries.\nCrucially, the poststratification dataset must also include a column with the population share of each combination of predictor values. Consider the table used by our hypothetical supermarket chain. This table includes 160 rows:\n\nnrow(stratification)\n#&gt; [1] 160\n\nAnd here are the entries for the city of Tucson, AZ:\n\ntucson &lt;- subset(stratification, city == \"Tucson, AZ\")\ndatasummary_df(tucson)\n\n\n\ncity\neducation\nage\npopulation_share\n\n\n\nTucson, AZ\nHigh school or less\n18-54\n0.16\n\n\nTucson, AZ\nPost-secondary\n18-54\n0.38\n\n\nTucson, AZ\nHigh school or less\n55+\n0.20\n\n\nTucson, AZ\nPost-secondary\n55+\n0.25\n\n\n\n\n\nAccording to these (simulated) data, the share of Tucson residents who are between 18 and 54 year old and have a High School degree or less is about 16%.\nWe can use the predictions() function from the marginaleffects package to predict the value of the meat_substitute variable for each of the four categories of residents in Tucson:\n\npredictions(mod, newdata = tucson)\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;      1.81  1.48   2.13\n#&gt;      3.03  2.71   3.34\n#&gt;      3.01  2.69   3.33\n#&gt;      4.23  3.90   4.54\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, city, education, age, population_share, meat_substitute \n#&gt; Type:  response\n\nThe MrP estimate for this city is simply the weighted average of predicted values, where weights are the population shares of each category of residents. In this context, we have:\n\\[0.16 \\times 1.81 + 0.38 \\times 3.03 + 0.20 \\times 3.01 + 0.25 \\times 4.23 = 3.13\\]\nInstead of computing estimates by hand for each city, we can use the by and wts arguments of the predictions() function to do everything everywhere all at once:\n\np &lt;- predictions(              # Compute predictions,\n    model = mod,               # using the multilevel regression model `mod`, \n    newdata = stratification,  # for each row of the `stratification` table.\n    by = \"city\",               # Then, take the weighted average of predictions by city,\n    wts = \"population_share\")  # using demographic weights.\np\n#&gt; \n#&gt;               city Estimate 2.5 % 97.5 %\n#&gt;  Washington, DC        4.31  3.99   4.64\n#&gt;  Tucson, AZ            3.13  2.81   3.44\n#&gt;  Seattle, WA           5.06  4.73   5.39\n#&gt;  San Jose, CA          3.91  3.59   4.25\n#&gt;  San Francisco, CA     5.07  4.72   5.40\n#&gt; --- 30 rows omitted. See ?print.marginaleffects --- \n#&gt;  Boston, MA            3.65  3.37   3.93\n#&gt;  Baltimore, MD         2.93  2.56   3.31\n#&gt;  Austin, TX            2.88  2.56   3.19\n#&gt;  Atlanta, GA           4.59  4.27   4.94\n#&gt;  Albuquerque, NM       2.67  2.35   2.99\n#&gt; Columns: city, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nSince we estimated a bayesian model in the ‚ÄúMr‚Äù step, we can now use the posterior_draws() function to extract draws from the posterior distribution of the MrP estimates. This allows us to compute credible intervals for each city, and draw many fancy plots like this one:\n\np |&gt; \n    # extract draws from the posterior distribution\n    posterior_draws() |&gt;\n    # sort cities by interest in meat substitutes\n    arrange(estimate) |&gt;\n    mutate(city = factor(city, levels = rev(unique(city)))) |&gt;\n    # plot the results\n    ggplot(aes(x = draw, y = city)) +\n    geom_density_ridges() +\n    theme_minimal() +\n    theme(panel.grid = element_blank()) +\n    labs(\n        x = \"Average interest in meat substitutes\",\n        y = NULL,\n        title = \"Estimated interest in meat substitutes by city\",\n        subtitle = \"Multilevel Regression and Poststratification\",\n        caption = \"Source: Simulated data\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>MrP</span>"
    ]
  },
  {
    "objectID": "articles/mrp.html#data-simulation",
    "href": "articles/mrp.html#data-simulation",
    "title": "\n24¬† MrP\n",
    "section": "\n24.4 Data simulation",
    "text": "24.4 Data simulation\nAll the data used on this page were simulated using this code:\n\nlibrary(marginaleffects)\nlibrary(countrycode)\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(brms)\nset.seed(1024)\n\ncities &lt;- c(\"New York City, NY\", \"Los Angeles, CA\", \"Chicago, IL\", \"Houston, TX\", \"Phoenix, AZ\", \"Philadelphia, PA\", \"San Antonio, TX\", \"San Diego, CA\", \"Dallas, TX\", \"San Jose, CA\", \"Austin, TX\", \"Jacksonville, FL\", \"Fort Worth, TX\", \"Columbus, OH\", \"San Francisco, CA\", \"Charlotte, NC\", \"Indianapolis, IN\", \"Seattle, WA\", \"Denver, CO\", \"Washington, DC\", \"Boston, MA\", \"Nashville, TN\", \"El Paso, TX\", \"Detroit, MI\", \"Memphis, TN\", \"Portland, OR\", \"Oklahoma City, OK\", \"Las Vegas, NV\", \"Louisville, KY\", \"Baltimore, MD\", \"Milwaukee, WI\", \"Albuquerque, NM\", \"Tucson, AZ\", \"Fresno, CA\", \"Sacramento, CA\", \"Mesa, AZ\", \"Atlanta, GA\", \"Kansas City, MO\", \"Colorado Springs, CO\", \"Miami, FL\")\ncities &lt;- rev(sort(cities))\neducation &lt;- c(\"High school or less\", \"Post-secondary\")\nage &lt;- c(\"18-54\", \"55+\")\nstratification &lt;- expand.grid(\n    city = cities,\n    education = education,\n    age = age) |&gt;\n    mutate(\n        population_share = runif(n(), min = .25, max = .75),\n        population_share = population_share / sum(population_share),\n        .by = \"city\",) |&gt;\n    arrange(city)\nN &lt;- 1000\nsurvey &lt;- data.frame(\n    city = sample(cities, N, replace = TRUE),\n    age = sample(age, N, replace = TRUE),\n    education = sample(education, N, replace = TRUE)\n)\nsurvey &lt;- data.frame(\n    respondent = sprintf(\"%04d\", 1:N),\n    survey)\nM &lt;- model.matrix(~., survey)\nb &lt;- runif(ncol(M))\nsurvey$meat_substitute &lt;- as.numeric(cut(M %*% b, breaks = 7))",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>MrP</span>"
    ]
  },
  {
    "objectID": "articles/svalues.html",
    "href": "articles/svalues.html",
    "title": "\n25¬† S Values\n",
    "section": "",
    "text": "The S value ‚Äî ‚ÄúShannon transform‚Äù or ‚Äúbinary surprisal value‚Äù ‚Äî is a cognitive tool to help analysts make intuitive sense of p values (Rafi and Greenland 2020). It allows us to compare a p value to the outcome of a familiar game of chance.\nConsider this: We toss a coin 4 times to see if we can reject the null hypothesis that the coin toss is fair. If the null is true, the probability of drawing Heads on any single toss is \\(\\frac{1}{2}\\). The probability of observing 4 Heads in a row is \\(\\left (\\frac{1}{2} \\right )^4=\\frac{1}{16}=0.0625\\). This probability characterizes the ‚Äúsurprise‚Äù caused by observing 4 straight heads in a world where the null holds, that is, where the coin toss is fair.\nNow consider a different exercise: We estimate a model and use marginaleffects::hypotheses() to test if two of the estimated coefficients are equal:\n\nlibrary(marginaleffects)\ndat &lt;- transform(mtcars, cyl = factor(cyl))\nmod &lt;- lm(mpg ~ cyl, dat)\nhyp &lt;- hypotheses(mod, \"cyl6 = cyl8\")\nhyp\n#&gt; \n#&gt;         Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  cyl6 = cyl8     4.64       1.49 3.11  0.00186 9.1  1.72   7.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe difference between cyl6 and cyl8 is 4.64, and the associated p value is 0.0018593. Again, the p value can be interpreted as a measure of the surprise caused by the data if the null were true (i.e., if the two coefficients were in fact equal).\nHow many consecutive Heads tosses would be as surprising as this test of equality? To answer this question, we solve for \\(s\\) in \\(p=\\left (\\frac{1}{2} \\right )^s\\). The solution is the negative \\(log_2\\) of p:\n\n-log2(hyp$p.value)\n#&gt; [1] 9.070986\n\nIndeed, the probability of obtaining 9 straight Heads with fair coin tosses is \\(\\left (\\frac{1}{2} \\right )^9=0.0019531\\), which is very close to the p value we observed in the test of coefficient equality (see the S column in the marginaleffects printout above). Comparing our p value to the outcome of such a familiar game of chance gives us a nice intuitive interpretation:\n\nIf the cyl6 and cyl8 coefficients were truly equal, finding an absolute difference greater than 4.64 purely by chance would be as surprising as tossing 9 straight Heads with a fair coin toss.\n\nThe benefits of S values include (Cole, Edwards, and Greenland 2021):\n\nCalibrates the analyst‚Äôs intuitions by reference to a well-known physical process (coin flips).\nAvoids the problematic dichotomization of findings as ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù (Rothman 2021).\nReduces the reliance on arbitrary thresholds of significance like \\(\\alpha=0.05\\).\nGuards against the common misinterpretation of p values as the ‚Äúprobability that the null hypothesis is true‚Äù or as the probability of the alternative hypothesis. This is in part because S is above 1 whenever p&lt;0.5.1\n\nRefers to a more natural scale: ‚ÄúThe difference between a p value of 0.99 and 0.90 in terms of how surprising the observed test statistic is, is not the same as the difference between 0.10 and 0.01.‚Äù2\n\n\n\n\n\n\n\n\n\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2021. ‚ÄúSurprise!‚Äù American Journal of Epidemiology 190 (2): 191‚Äì93.\n\n\nRafi, Zad, and Sander Greenland. 2020. ‚ÄúSemantic and Cognitive Tools to Aid Statistical Science: Replace Confidence and Significance by Compatibility and Surprise.‚Äù BMC Medical Research Methodology 20: 1‚Äì13.\n\n\nRothman, Kenneth J. 2021. ‚ÄúRothman Responds to ‚ÄòSurprise!‚Äô‚Äù American Journal of Epidemiology 190 (2): 194‚Äì95.\n\n\n\n\nThanks to Sander Greenland for this note.‚Ü©Ô∏é\nThanks to Zad Rafi for noting this and for linking to (Rafi and Greenland 2020).‚Ü©Ô∏é",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>S Values</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#emmeans",
    "href": "articles/alternative_software.html#emmeans",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.1 emmeans\n",
    "text": "26.1 emmeans\n\nThe emmeans package is developed by Russell V. Lenth and colleagues. emmeans is a truly incredible piece of software, and a trailblazer in the R ecosystem. It is an extremely powerful package whose functionality overlaps marginaleffects to a significant degree: marginal means, contrasts, and slopes. Even if the two packages can compute many of the same quantities, emmeans and marginaleffects have pretty different philosophies with respect to user interface and computation.\nAn emmeans analysis typically starts by computing ‚Äúmarginal means‚Äù by holding all numeric covariates at their means, and by averaging across a balanced grid of categorical predictors. Then, users can use the contrast() function to estimate the difference between marginal means.\nThe marginaleffects package supplies a marginal_means() function which can replicate most emmeans analyses by computing marginal means. However, the typical analysis is more squarely centered on predicted/fitted values. This is a useful starting point because, in many cases, analysts will find it easy and intuitive to express their scientific queries in terms of changes in predicted values. For example,\n\nHow does the average predicted probability of survival differ between treatment and control group?\nWhat is the difference between the predicted wage of college and high school graduates?\n\nLet‚Äôs say we estimate a linear regression model with two continuous regressors and a multiplicative interaction:\n\\[y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x \\cdot z + \\varepsilon\\]\nIn this model, the effect of \\(x\\) on \\(y\\) will depend on the value of covariate \\(z\\). Let‚Äôs say the user wants to estimate what happens to the predicted value of \\(y\\) when \\(x\\) increases by 1 unit, when \\(z \\in \\{-1, 0, 1\\}\\). To do this, we use the comparisons() function. The variables argument determines the scientific query of interest, and the newdata argument determines the grid of covariate values on which we want to evaluate the query:\n\nmodel &lt;- lm(y ~ x * z, data)\n\ncomparisons(\n  model,\n  variables = list(x = 1), # what is the effect of 1-unit change in x?\n  newdata = datagrid(z = -1:1) # when z is held at values -1, 0, or 1\n)\n\nAs the vignettes show, marginaleffects can also compute contrasts on marginal means. It can also compute various quantities of interest like raw fitted values, slopes (partial derivatives), and contrasts between marginal means. It also offers a flexible mechanism to run (non-)linear hypothesis tests using the delta method, and it offers fully customizable strategy to compute quantities like odds ratios (or completely arbitrary functions of predicted outcome).\nThus, in my (Vincent‚Äôs) biased opinion, the main benefits of marginaleffects over emmeans are:\n\nSupport more model types.\nSimpler, more intuitive, and highly consistent user interface.\nEasier to compute average slopes or unit-level contrasts for whole datasets.\nEasier to compute slopes (aka marginal effects, trends, or partial derivatives) for custom grids and continuous regressors.\nEasier to implement causal inference strategies like the parametric g-formula and regression adjustment in experiments (see vignettes).\nAllows the computation of arbitrary quantities of interest via user-supplied functions and automatic delta method inference.\nCommon plots are easy with the plot_predictions(), plot_comparisons(), and plot_slopes() functions.\n\nTo be fair, many of the marginaleffects advantages listed above come down to subjective preferences over user interface. Readers are thus encouraged to try both packages to see which interface they prefer.\nThe main advantages of emmeans over marginaleffects arise when users are specifically interested in marginal means, where emmeans tends to be much faster and to have a lot of functionality to handle backtransformations. emmeans also has better functionality for effect sizes; notably, the eff_size() function can return effect size estimates that account for uncertainty in both estimated effects and the population SD.\nPlease let me know if you find other features in emmeans so I can add them to this list.\nThe Marginal Means Vignette includes side-by-side comparisons of emmeans and marginaleffects to compute marginal means. The rest of this section compares the syntax for contrasts and marginaleffects.\n\n26.1.1 Contrasts\nAs far as I can tell, emmeans does not provide an easy way to compute unit-level contrasts for every row of the dataset used to fit our model. Therefore, the side-by-side syntax shown below will always include newdata=datagrid() to specify that we want to compute only one contrast: at the mean values of the regressors. In day-to-day practice with slopes(), however, this extra argument would not be necessary.\nFit a model:\n\nlibrary(emmeans)\nlibrary(marginaleffects)\n\nmod &lt;- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)\n\nLink scale, pairwise contrasts:\n\nemm &lt;- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\", adjust = \"none\", df = Inf)\n#&gt;  contrast    estimate      SE  df z.ratio p.value\n#&gt;  cyl6 - cyl4   -0.905    1.63 Inf  -0.555  0.5789\n#&gt;  cyl8 - cyl4  -19.542 4367.17 Inf  -0.004  0.9964\n#&gt;  cyl8 - cyl6  -18.637 4367.16 Inf  -0.004  0.9966\n#&gt; \n#&gt; Degrees-of-freedom method: user-specified \n#&gt; Results are given on the log odds ratio (not the response) scale.\n\ncomparisons(mod,\n            type = \"link\",\n            newdata = \"mean\",\n            variables = list(cyl = \"pairwise\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error        z Pr(&gt;|z|)   S   2.5 %  97.5 %  hp cyl\n#&gt;   cyl    6 - 4   -0.905       1.63 -0.55506    0.579 0.8    -4.1    2.29 147   8\n#&gt;   cyl    8 - 4  -19.542    4367.17 -0.00447    0.996 0.0 -8579.0 8539.95 147   8\n#&gt;   cyl    8 - 6  -18.637    4367.17 -0.00427    0.997 0.0 -8578.1 8540.85 147   8\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, hp, cyl \n#&gt; Type:  link\n\nResponse scale, reference groups:\n\nemm &lt;- emmeans(mod, specs = \"cyl\", regrid = \"response\")\ncontrast(emm, method = \"trt.vs.ctrl1\", adjust = \"none\", df = Inf, ratios = FALSE)\n#&gt;  contrast    estimate    SE  df z.ratio p.value\n#&gt;  cyl6 - cyl4   -0.222 0.394 Inf  -0.564  0.5727\n#&gt;  cyl8 - cyl4   -0.595 0.511 Inf  -1.163  0.2447\n#&gt; \n#&gt; Degrees-of-freedom method: user-specified\n\ncomparisons(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Term Contrast  Estimate Std. Error         z Pr(&gt;|z|)   S     2.5 %   97.5 %  hp cyl\n#&gt;   cyl    6 - 4 -2.22e-01   3.94e-01 -0.564103    0.573 0.8 -9.94e-01 5.50e-01 147   8\n#&gt;   cyl    8 - 4 -5.95e-01   5.11e-01 -1.163332    0.245 2.0 -1.60e+00 4.07e-01 147   8\n#&gt;   hp     +1    -1.56e-10   6.80e-07 -0.000229    1.000 0.0 -1.33e-06 1.33e-06 147   8\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, hp, cyl \n#&gt; Type:  response\n\n\n26.1.2 Contrasts by group\nHere is a slightly more complicated example with contrasts estimated by subgroup in a lme4 mixed effects model. First we estimate a model and compute pairwise contrasts by subgroup using emmeans:\n\nlibrary(dplyr)\nlibrary(lme4)\nlibrary(emmeans)\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/lme4/VerbAgg.csv\")\ndat$woman &lt;- as.numeric(dat$Gender == \"F\")\n\nmod &lt;- glmer(\n    woman ~ btype * resp + situ + (1 + Anger | item),\n    family = binomial,\n    data = dat)\n\nemmeans(mod, specs = \"btype\", by = \"resp\") |&gt;\n    contrast(method = \"revpairwise\", adjust = \"none\")\n#&gt; resp = no:\n#&gt;  contrast      estimate     SE  df z.ratio p.value\n#&gt;  scold - curse  -0.0152 0.1097 Inf  -0.139  0.8898\n#&gt;  shout - curse  -0.2533 0.1022 Inf  -2.479  0.0132\n#&gt;  shout - scold  -0.2381 0.0886 Inf  -2.686  0.0072\n#&gt; \n#&gt; resp = perhaps:\n#&gt;  contrast      estimate     SE  df z.ratio p.value\n#&gt;  scold - curse  -0.2393 0.1178 Inf  -2.031  0.0422\n#&gt;  shout - curse  -0.0834 0.1330 Inf  -0.627  0.5309\n#&gt;  shout - scold   0.1559 0.1358 Inf   1.148  0.2510\n#&gt; \n#&gt; resp = yes:\n#&gt;  contrast      estimate     SE  df z.ratio p.value\n#&gt;  scold - curse   0.0391 0.1292 Inf   0.302  0.7624\n#&gt;  shout - curse   0.5802 0.1784 Inf   3.252  0.0011\n#&gt;  shout - scold   0.5411 0.1888 Inf   2.866  0.0042\n#&gt; \n#&gt; Results are averaged over the levels of: situ \n#&gt; Results are given on the log odds ratio (not the response) scale.\n\nWhat did emmeans do to obtain these results? Roughly speaking:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nIn short, emmeans computes pairwise contrasts between marginal means, which are themselves averages of adjusted predictions. This is different from the default types of contrasts produced by comparisons(), which reports contrasts between adjusted predictions, without averaging across a pre-specified grid of predictors. What does comparisons() do instead?\nLet newdata be a data frame supplied by the user (or the original data frame used to fit the model), then:\n\nCreate a new data frame called newdata2, which is identical to newdata except that the focal variable is incremented by one level.\nCompute contrasts as the difference between adjusted predictions made on the two datasets:\n\npredict(model, newdata = newdata2) - predict(model, newdata = newdata)\n\n\n\nAlthough it is not idiomatic, we can use still use comparisons() to emulate the emmeans results. First, we create a prediction grid with one cell for each combination of categorical predictor in the model:\n\nnd &lt;- datagrid(\n    model = mod,\n    resp = dat$resp,\n    situ = dat$situ,\n    btype = dat$btype)\nnrow(nd)\n#&gt; [1] 18\n\nThis grid has 18 rows, one for each combination of levels for the resp (3), situ (2), and btype (3) variables (3 * 2 * 3 = 18).\nThen we compute pairwise contrasts over this grid:\n\ncmp &lt;- comparisons(mod,\n    variables = list(\"btype\" = \"pairwise\"),\n    newdata = nd,\n    type = \"link\")\nnrow(cmp)\n#&gt; [1] 54\n\nThere are 3 pairwise contrasts, corresponding to the 3 pairwise comparisons possible between the 3 levels of the focal variable btype: scold-curse, shout-scold, shout-curse. The comparisons() function estimates those 3 contrasts for each row of newdata, so we get \\(18 \\times 3 = 54\\) rows.\nFinally, if we wanted contrasts averaged over each subgroup of the resp variable, we can use the avg_comparisons() function with the by argument:\n\navg_comparisons(mod,\n    by = \"resp\",\n    variables = list(\"btype\" = \"pairwise\"),\n    newdata = nd,\n    type = \"link\")\n#&gt; \n#&gt;   Term                  Contrast    resp Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 %   97.5 %\n#&gt;  btype mean(scold) - mean(curse) no       -0.0152     0.1097 -0.139  0.88976 0.2 -0.230  0.19972\n#&gt;  btype mean(scold) - mean(curse) perhaps  -0.2393     0.1178 -2.031  0.04221 4.6 -0.470 -0.00842\n#&gt;  btype mean(scold) - mean(curse) yes       0.0391     0.1292  0.302  0.76239 0.4 -0.214  0.29234\n#&gt;  btype mean(shout) - mean(curse) no       -0.2533     0.1022 -2.479  0.01319 6.2 -0.454 -0.05300\n#&gt;  btype mean(shout) - mean(curse) perhaps  -0.0834     0.1330 -0.627  0.53090 0.9 -0.344  0.17737\n#&gt;  btype mean(shout) - mean(curse) yes       0.5802     0.1784  3.252  0.00115 9.8  0.230  0.92987\n#&gt;  btype mean(shout) - mean(scold) no       -0.2381     0.0886 -2.686  0.00723 7.1 -0.412 -0.06436\n#&gt;  btype mean(shout) - mean(scold) perhaps   0.1559     0.1358  1.148  0.25103 2.0 -0.110  0.42215\n#&gt;  btype mean(shout) - mean(scold) yes       0.5411     0.1888  2.866  0.00416 7.9  0.171  0.91116\n#&gt; \n#&gt; Columns: term, contrast, resp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  link\n\nThese results are identical to those produced by emmeans (except for \\(t\\) vs.¬†\\(z\\)).\n\n26.1.3 Marginal Effects\nAs far as I can tell, emmeans::emtrends makes it easier to compute marginal effects for a few user-specified values than for large grids or for the full original dataset.\nResponse scale, user-specified values:\n\nmod &lt;- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)\n\nemtrends(mod, ~hp, \"hp\", regrid = \"response\", at = list(cyl = 4))\n#&gt;   hp hp.trend    SE  df asymp.LCL asymp.UCL\n#&gt;  147 -0.00786 0.011 Inf   -0.0294    0.0137\n#&gt; \n#&gt; Confidence level used: 0.95\n\nslopes(mod, newdata = datagrid(cyl = 4))\n#&gt; \n#&gt;  Term Contrast cyl Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;   cyl    6 - 4   4 -0.22219      0.394 -0.564    0.573 0.8 -0.9942 0.5498\n#&gt;   cyl    8 - 4   4 -0.59469      0.511 -1.163    0.245 2.0 -1.5966 0.4072\n#&gt;   hp     dY/dX   4 -0.00785      0.011 -0.713    0.476 1.1 -0.0294 0.0137\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cyl, predicted_lo, predicted_hi, predicted, vs, hp \n#&gt; Type:  response\n\nLink scale, user-specified values:\n\nemtrends(mod, ~hp, \"hp\", at = list(cyl = 4))\n#&gt;   hp hp.trend     SE  df asymp.LCL asymp.UCL\n#&gt;  147  -0.0326 0.0339 Inf    -0.099    0.0338\n#&gt; \n#&gt; Confidence level used: 0.95\n\nslopes(mod, type = \"link\", newdata = datagrid(cyl = 4))\n#&gt; \n#&gt;  Term Contrast cyl Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;   cyl    6 - 4   4  -0.9049   1.63e+00 -0.55506    0.579 0.8    -4.100 2.29e+00\n#&gt;   cyl    8 - 4   4 -19.5418   4.37e+03 -0.00447    0.996 0.0 -8579.030 8.54e+03\n#&gt;   hp     dY/dX   4  -0.0326   3.39e-02 -0.96147    0.336 1.6    -0.099 3.38e-02\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cyl, predicted_lo, predicted_hi, predicted, vs, hp \n#&gt; Type:  link\n\n\n26.1.4 More examples\nHere are a few more emmeans vs.¬†marginaleffects comparisons:\n\n## Example of examining a continuous x categorical interaction using emmeans and marginaleffects\n## Authors: Cameron Patrick and Vincent Arel-Bundock\n\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(marginaleffects)\n\n## use the mtcars data, set up am as a factor\ndata(mtcars)\nmc &lt;- mtcars |&gt; mutate(am = factor(am))\n\n## fit a linear model to mpg with wt x am interaction\nm &lt;- lm(mpg ~ wt*am, data = mc)\nsummary(m)\n\n## 1. means for each level of am at mean wt.\nemmeans(m, \"am\")\nmarginal_means(m, variables = \"am\")\npredictions(m, newdata = datagrid(am = 0:1))\n\n## 2. means for each level of am at wt = 2.5, 3, 3.5.\nemmeans(m, c(\"am\", \"wt\"), at = list(wt = c(2.5, 3, 3.5)))\npredictions(m, newdata = datagrid(am = 0:1, wt = c(2.5, 3, 3.5))\n\n## 3. means for wt = 2.5, 3, 3.5, averaged over levels of am (implicitly!).\nemmeans(m, \"wt\", at = list(wt = c(2.5, 3, 3.5)))\n\n## same thing, but the averaging is more explicit, using the `by` argument\npredictions(\n  m,\n  newdata = datagrid(am = 0:1, wt = c(2.5, 3, 3.5)),\n  by = \"wt\")\n\n## 4. graphical version of 2.\nemmip(m, am ~ wt, at = list(wt = c(2.5, 3, 3.5)), CIs = TRUE)\nplot_predictions(m, condition = c(\"wt\", \"am\"))\n\n## 5. compare levels of am at specific values of wt.\n## this is a bit ugly because the emmeans defaults for pairs() are silly.\n## infer = TRUE: enable confidence intervals.\n## adjust = \"none\": begone, Tukey.\n## reverse = TRUE: contrasts as (later level) - (earlier level)\npairs(emmeans(m, \"am\", by = \"wt\", at = list(wt = c(2.5, 3, 3.5))),\n      infer = TRUE, adjust = \"none\", reverse = TRUE)\n\ncomparisons(\n  m,\n  variables = \"am\",\n  newdata = datagrid(wt = c(2.5, 3, 3.5)))\n\n## 6. plot of pairswise comparisons\nplot(pairs(emmeans(m, \"am\", by = \"wt\", at = list(wt = c(2.5, 3, 3.5))),\n      infer = TRUE, adjust = \"none\", reverse = TRUE))\n\n## Since `wt` is numeric, the default is to plot it as a continuous variable on\n## the x-axis.  But not that this is the **exact same info** as in the emmeans plot.\nplot_comparisons(m, variables = \"am\", condition = \"wt\")\n\n## You of course customize everything, set draw=FALSE, and feed the raw data to feed to ggplot2\np &lt;- plot_comparisons(\n  m,\n  variables = \"am\",\n  condition = list(wt = c(2.5, 3, 3.5)),\n  draw = FALSE)\n\nggplot(p, aes(y = wt, x = comparison, xmin = conf.low, xmax = conf.high)) +\n  geom_pointrange()\n\n## 7. slope of wt for each level of am\nemtrends(m, \"am\", \"wt\")\nslopes(m, newdata = datagrid(am = 0:1))",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#margins-and-prediction",
    "href": "articles/alternative_software.html#margins-and-prediction",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.2 margins and prediction\n",
    "text": "26.2 margins and prediction\n\nThe margins and prediction packages for R were designed by Thomas Leeper to emulate the behavior of the margins command from Stata. These packages are trailblazers and strongly influenced the development of marginaleffects. The main benefits of marginaleffects over these packages are:\n\nSupport more model types\nFaster\nMemory efficient\nPlots using ggplot2 instead of Base R\nMore extensive test suite\nActive development\n\nThe syntax of the two packages is very similar.\n\n26.2.1 Average Marginal Effects\n\nlibrary(margins)\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\nmar &lt;- margins(mod)\nsummary(mar)\n#&gt;  factor     AME     SE       z      p   lower   upper\n#&gt;     cyl -0.9416 0.5509 -1.7092 0.0874 -2.0214  0.1382\n#&gt;      hp -0.0180 0.0119 -1.5188 0.1288 -0.0413  0.0052\n#&gt;      wt -3.1670 0.7406 -4.2764 0.0000 -4.6185 -1.7155\n\nmfx &lt;- slopes(mod)\n\n\n26.2.2 Individual-Level Marginal Effects\nMarginal effects in a user-specified data frame:\n\nhead(data.frame(mar))\n#&gt;    mpg cyl disp  hp drat    wt  qsec vs am gear carb   fitted se.fitted   dydx_cyl    dydx_hp   dydx_wt Var_dydx_cyl  Var_dydx_hp Var_dydx_wt X_weights X_at_number\n#&gt; 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 22.82043 0.6876212 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#&gt; 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 22.01285 0.6056817 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#&gt; 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.96040 0.7349593 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#&gt; 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 20.93608 0.5800910 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#&gt; 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 17.16780 0.8322986 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#&gt; 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 20.25036 0.6638322 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n\nhead(mfx)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   cyl   -0.942      0.550 -1.71   0.0871 3.5 -2.02  0.137\n#&gt;   cyl   -0.942      0.550 -1.71   0.0871 3.5 -2.02  0.137\n#&gt;   cyl   -0.942      0.550 -1.71   0.0871 3.5 -2.02  0.137\n#&gt;   cyl   -0.942      0.550 -1.71   0.0871 3.5 -2.02  0.137\n#&gt;   cyl   -0.942      0.551 -1.71   0.0875 3.5 -2.02  0.138\n#&gt;   cyl   -0.942      0.550 -1.71   0.0871 3.5 -2.02  0.137\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, cyl, hp, wt \n#&gt; Type:  response\nnd &lt;- data.frame(cyl = 4, hp = 110, wt = 3)\n\n\n26.2.3 Marginal Effects at the Mean\n\nmar &lt;- margins(mod, data = data.frame(prediction::mean_or_mode(mtcars)), unit_ses = TRUE)\ndata.frame(mar)\n#&gt;        mpg    cyl     disp       hp     drat      wt     qsec     vs      am   gear   carb   fitted se.fitted   dydx_cyl    dydx_hp   dydx_wt Var_dydx_cyl  Var_dydx_hp Var_dydx_wt SE_dydx_cyl SE_dydx_hp SE_dydx_wt X_weights X_at_number\n#&gt; 1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 20.09062 0.4439832 -0.9416168 -0.0180381 -3.166973    0.3035013 0.0001410453     0.54846   0.5509096 0.01187625  0.7405808        NA           1\n\nslopes(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %   97.5 %\n#&gt;   cyl   -0.942     0.5510 -1.71   0.0875  3.5 -2.0216  0.13833\n#&gt;   hp    -0.018     0.0119 -1.52   0.1290  3.0 -0.0413  0.00525\n#&gt;   wt    -3.167     0.7406 -4.28   &lt;0.001 15.7 -4.6185 -1.71549\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, cyl, hp, wt \n#&gt; Type:  response\n\n\n26.2.4 Counterfactual Average Marginal Effects\nThe at argument of the margins package emulates Stata by fixing the values of some variables at user-specified values, and by replicating the full dataset several times for each combination of the supplied values (see the Stata section below). For example, if the dataset includes 32 rows and the user calls at=list(cyl=c(4, 6)), margins will compute 64 unit-level marginal effects estimates:\n\ndat &lt;- mtcars\ndat$cyl &lt;- factor(dat$cyl)\nmod &lt;- lm(mpg ~ cyl * hp + wt, data = mtcars)\n\nmar &lt;- margins(mod, at = list(cyl = c(4, 6, 8)))\nsummary(mar)\n#&gt;  factor    cyl     AME     SE       z      p   lower   upper\n#&gt;     cyl 4.0000  0.0381 0.6000  0.0636 0.9493 -1.1378  1.2141\n#&gt;     cyl 6.0000  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#&gt;     cyl 8.0000  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#&gt;      hp 4.0000 -0.0878 0.0267 -3.2937 0.0010 -0.1400 -0.0355\n#&gt;      hp 6.0000 -0.0499 0.0154 -3.2397 0.0012 -0.0800 -0.0197\n#&gt;      hp 8.0000 -0.0120 0.0108 -1.1065 0.2685 -0.0332  0.0092\n#&gt;      wt 4.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n#&gt;      wt 6.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n#&gt;      wt 8.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n\navg_slopes(\n    mod,\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  Term    Contrast cyl Estimate Std. Error       z Pr(&gt;|z|)    S   2.5 %   97.5 %\n#&gt;   cyl mean(dY/dX)   4   0.0381     0.5999  0.0636   0.9493  0.1 -1.1377  1.21401\n#&gt;   cyl mean(dY/dX)   6   0.0381     0.5998  0.0636   0.9493  0.1 -1.1375  1.21381\n#&gt;   cyl mean(dY/dX)   8   0.0381     0.5999  0.0636   0.9493  0.1 -1.1376  1.21389\n#&gt;   hp  mean(dY/dX)   4  -0.0878     0.0267 -3.2936   &lt;0.001 10.0 -0.1400 -0.03554\n#&gt;   hp  mean(dY/dX)   6  -0.0499     0.0154 -3.2397   0.0012  9.7 -0.0800 -0.01970\n#&gt;   hp  mean(dY/dX)   8  -0.0120     0.0108 -1.1065   0.2685  1.9 -0.0332  0.00923\n#&gt;   wt  mean(dY/dX)   4  -3.1198     0.6613 -4.7175   &lt;0.001 18.7 -4.4160 -1.82362\n#&gt;   wt  mean(dY/dX)   6  -3.1198     0.6613 -4.7174   &lt;0.001 18.7 -4.4160 -1.82362\n#&gt;   wt  mean(dY/dX)   8  -3.1198     0.6613 -4.7174   &lt;0.001 18.7 -4.4160 -1.82362\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\n\n26.2.5 Adjusted Predictions\nThe syntax to compute adjusted predictions using the predictions() package or marginaleffects is very similar:\n\nprediction::prediction(mod) |&gt; head()\n#&gt;    mpg cyl disp  hp drat    wt  qsec vs am gear carb   fitted se.fitted\n#&gt; 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 21.90488 0.6927034\n#&gt; 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 21.10933 0.6266557\n#&gt; 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.64753 0.6652076\n#&gt; 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 20.04859 0.6041400\n#&gt; 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 17.25445 0.7436172\n#&gt; 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 19.53360 0.6436862\n\nmarginaleffects::predictions(mod) |&gt; head()\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      21.9      0.693 31.6   &lt;0.001 726.6  20.5   23.3\n#&gt;      21.1      0.627 33.7   &lt;0.001 823.9  19.9   22.3\n#&gt;      25.6      0.665 38.6   &lt;0.001   Inf  24.3   27.0\n#&gt;      20.0      0.604 33.2   &lt;0.001 799.8  18.9   21.2\n#&gt;      17.3      0.744 23.2   &lt;0.001 393.2  15.8   18.7\n#&gt;      19.5      0.644 30.3   &lt;0.001 669.5  18.3   20.8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp, wt \n#&gt; Type:  response",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#stata",
    "href": "articles/alternative_software.html#stata",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.3 Stata\n",
    "text": "26.3 Stata\n\nStata is a good but expensive software package for statistical analysis. It is published by StataCorp LLC. This section compares Stata‚Äôs margins command to marginaleffects.\nThe results produced by marginaleffects are extensively tested against Stata. See the test suite for a list of the dozens of models where we compared estimates and standard errors.\n\n26.3.1 Average Marginal Effect (AMEs)\nMarginal effects are unit-level quantities. To compute ‚Äúaverage marginal effects‚Äù, we first calculate marginal effects for each observation in a dataset. Then, we take the mean of those unit-level marginal effects.\n\n26.3.1.1 Stata\nBoth Stata‚Äôs margins command and the slopes() function can calculate average marginal effects (AMEs). Here is an example showing how to estimate AMEs in Stata:\nquietly reg mpg cyl hp wt\nmargins, dydx(*)\n\nAverage marginal effects                        Number of obs     =         32\nModel VCE    : OLS\n \nExpression   : Linear prediction, predict()\ndy/dx w.r.t. : cyl hp wt\n \n------------------------------------------------------------------------------\n    |            Delta-method\n    |      dy/dx   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n------------------------------------------------------------------------------\ncyl |  -.9416168   .5509164    -1.71   0.098    -2.070118    .1868842\n hp |  -.0180381   .0118762    -1.52   0.140    -.0423655    .0062893\n wt |  -3.166973   .7405759    -4.28   0.000    -4.683974   -1.649972\n------------------------------------------------------------------------------\n\n26.3.1.2 marginaleffects\nThe same results can be obtained with slopes() and summary() like this:\n\nlibrary(\"marginaleffects\")\nmod &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)\navg_slopes(mod)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %   97.5 %\n#&gt;   cyl   -0.942     0.5506 -1.71   0.0872  3.5 -2.0208  0.13753\n#&gt;   hp    -0.018     0.0119 -1.52   0.1288  3.0 -0.0413  0.00524\n#&gt;   wt    -3.167     0.7406 -4.28   &lt;0.001 15.7 -4.6185 -1.71546\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNote that Stata reports t statistics while marginaleffects reports Z. This produces slightly different p-values because this model has low degrees of freedom: mtcars only has 32 rows\n\n26.3.2 Counterfactual Marginal Effects\nA ‚Äúcounterfactual marginal effect‚Äù is a special quantity obtained by replicating a dataset while fixing some regressor to user-defined values.\nConcretely, Stata computes counterfactual marginal effects in 3 steps:\n\nDuplicate the whole dataset 3 times and sets the values of cyl to the three specified values in each of those subsets.\nCalculate marginal effects for each observation in that large grid.\nTake the average of marginal effects for each value of the variable of interest.\n\n\n26.3.2.1 Stata\nWith the at argument, Stata‚Äôs margins command estimates average counterfactual marginal effects. Here is an example:\nquietly reg mpg i.cyl##c.hp wt\nmargins, dydx(hp) at(cyl = (4 6 8))\n\nAverage marginal effects                        Number of obs     =         32\nModel VCE    : OLS\n\nExpression   : Linear prediction, predict()\ndy/dx w.r.t. : hp\n\n1._at        : cyl             =           4\n\n2._at        : cyl             =           6\n\n3._at        : cyl             =           8\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nhp           |\n         _at |\n          1  |   -.099466   .0348665    -2.85   0.009    -.1712749   -.0276571\n          2  |  -.0213768    .038822    -0.55   0.587    -.1013323    .0585787\n          3  |   -.013441   .0125138    -1.07   0.293    -.0392137    .0123317\n------------------------------------------------------------------------------\n\n\n26.3.2.2 marginaleffects\nYou can estimate average counterfactual marginal effects with slopes() by using the datagridcf() to create a counterfactual dataset in which the full original dataset is replicated for each potential value of the cyl variable. Then, we tell the by argument to average within groups:\n\nmod &lt;- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)\n\navg_slopes(\n    mod,\n    variables = \"hp\",\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  Term    Contrast cyl Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 %  97.5 %\n#&gt;    hp mean(dY/dX)   4  -0.0995     0.0349 -2.853  0.00433 7.9 -0.1678 -0.0311\n#&gt;    hp mean(dY/dX)   6  -0.0214     0.0388 -0.551  0.58187 0.8 -0.0975  0.0547\n#&gt;    hp mean(dY/dX)   8  -0.0134     0.0125 -1.074  0.28278 1.8 -0.0380  0.0111\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\nThis is equivalent to taking the group-wise mean of observation-level marginal effects (without the by argument):\n\nmfx &lt;- slopes(\n    mod,\n    variables = \"hp\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\naggregate(estimate ~ term + cyl, data = mfx, FUN = mean)\n#&gt;   term cyl    estimate\n#&gt; 1   hp   4 -0.09946598\n#&gt; 2   hp   6 -0.02137679\n#&gt; 3   hp   8 -0.01344103\n\n\nNote that following Stata, the standard errors for group-averaged marginal effects are computed by taking the ‚ÄúJacobian at the mean:‚Äù\n\nJ &lt;- attr(mfx, \"jacobian\")\nJ_mean &lt;- aggregate(J, by = list(mfx$cyl), FUN = mean)\nJ_mean &lt;- as.matrix(J_mean[, 2:ncol(J_mean)])\nsqrt(diag(J_mean %*% vcov(mod) %*% t(J_mean)))\n#&gt; [1] 0.03486654 0.03882093 0.01251377\n\n\n26.3.3 Average Counterfactual Adjusted Predictions\n\n26.3.3.1 Stata\nJust like Stata‚Äôs margins command computes average counterfactual marginal effects, it can also estimate average counterfactual adjusted predictions.\nHere is an example:\nquietly reg mpg i.cyl##c.hp wt\nmargins, at(cyl = (4 6 8))\n\nPredictive margins                              Number of obs     =         32\nModel VCE    : OLS\n\nExpression   : Linear prediction, predict()\n\n1._at        : cyl             =           4\n\n2._at        : cyl             =           6\n\n3._at        : cyl             =           8\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   17.44233   2.372914     7.35   0.000     12.55522    22.32944\n          2  |    18.9149   1.291483    14.65   0.000     16.25505    21.57476\n          3  |   18.33318   1.123874    16.31   0.000     16.01852    20.64785\n------------------------------------------------------------------------------\nAgain, this is what Stata does in the background:\n\nIt duplicates the whole dataset 3 times and sets the values of cyl to the three specified values in each of those subsets.\nIt calculates predictions for that large grid.\nIt takes the average prediction for each value of cyl.\n\nIn other words, average counterfactual adjusted predictions as implemented by Stata are a hybrid between predictions at the observed values (the default in marginaleffects::predictions) and predictions at representative values.\n\n26.3.3.2 marginaleffects\nYou can estimate average counterfactual adjusted predictions with predictions() by, first, setting the grid_type argument of datagrid() to \"counterfactual\" and, second, by averaging the predictions using the by argument of summary(), or a manual function like dplyr::summarise().\n\nmod &lt;- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)\n\npredictions(\n    mod,\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  cyl Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    4     17.4       2.37  7.35   &lt;0.001  42.2  12.8   22.1\n#&gt;    6     18.9       1.29 14.65   &lt;0.001 158.9  16.4   21.4\n#&gt;    8     18.3       1.12 16.31   &lt;0.001 196.3  16.1   20.5\n#&gt; \n#&gt; Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    newdata = datagridcf(cyl = c(4, 6, 8))) |&gt;\n    group_by(cyl) |&gt;\n    summarize(AAP = mean(estimate))\n#&gt; # A tibble: 3 √ó 2\n#&gt;   cyl     AAP\n#&gt;   &lt;fct&gt; &lt;dbl&gt;\n#&gt; 1 4      17.4\n#&gt; 2 6      18.9\n#&gt; 3 8      18.3",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#brmsmargins",
    "href": "articles/alternative_software.html#brmsmargins",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.4 brmsmargins\n",
    "text": "26.4 brmsmargins\n\nThe brmsmargins package is developed by Joshua Wiley:\n\nThis package has functions to calculate marginal effects from brms models ( http://paul-buerkner.github.io/brms/ ). A central motivator is to calculate average marginal effects (AMEs) for continuous and discrete predictors in fixed effects only and mixed effects regression models including location scale models.\n\nThe main advantage of brmsmargins over marginaleffects is its ability to compute ‚ÄúMarginal Coefficients‚Äù following the method described in Hedeker et al (2012).\nThe main advantages of marginaleffects over brmsmargins are:\n\nSupport for 60+ model types, rather than just the brms package.\nSimpler user interface (subjective).\nAt the time of writing (2022-05-25) brmsmargins did not support certain brms models such as those with multivariate or multinomial outcomes. It also did not support custom outcome transformations.\n\nThe rest of this section presents side-by-side replications of some of the analyses from the brmsmargins vignettes in order to show highlight parallels and differences in syntax.\n\n26.4.1 Marginal Effects for Fixed Effects Models\n\n26.4.1.1 AMEs for Logistic Regression\nEstimate a logistic regression model with brms:\n\nlibrary(brms)\nlibrary(brmsmargins)\nlibrary(marginaleffects)\nlibrary(data.table)\nlibrary(withr)\nsetDTthreads(5)\nh &lt;- 1e-4\n\nvoid &lt;- capture.output(\n    bayes.logistic &lt;- brm(\n      vs ~ am + mpg, data = mtcars,\n      family = \"bernoulli\", seed = 1234,\n      silent = 2, refresh = 0,\n      backend = \"cmdstanr\",\n      chains = 4L, cores = 4L)\n)\n\nCompute AMEs manually:\n\nd1 &lt;- d2 &lt;- mtcars\nd2$mpg &lt;- d2$mpg + h\np1 &lt;- posterior_epred(bayes.logistic, newdata = d1)\np2 &lt;- posterior_epred(bayes.logistic, newdata = d2)\nm &lt;- (p2 - p1) / h\nquantile(rowMeans(m), c(.5, .025, .975))\n#&gt;        50%       2.5%      97.5% \n#&gt; 0.07010427 0.05418413 0.09092451\n\nCompute AMEs with brmsmargins:\n\nbm &lt;- brmsmargins(\n  bayes.logistic,\n  add = data.frame(mpg = c(0, 0 + h)),\n  contrasts = cbind(\"AME MPG\" = c(-1 / h, 1 / h)),\n  CI = 0.95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#&gt;            M        Mdn         LL         UL PercentROPE PercentMID   CI CIType ROPE  MID   Label\n#&gt; 1 0.07105468 0.07010427 0.05418413 0.09092451          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME MPG\n\nCompute AMEs using marginaleffects:\n\navg_slopes(bayes.logistic) \n#&gt; \n#&gt;  Term Contrast Estimate   2.5 %  97.5 %\n#&gt;   am     1 - 0  -0.2665 -0.4242 -0.0703\n#&gt;   mpg    dY/dX   0.0701  0.0542  0.0909\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\nThe mpg element of the Effect column from marginaleffects matches the the M column of the output from brmsmargins.\n\n26.4.2 Marginal Effects for Mixed Effects Models\nEstimate a mixed effects logistic regression model with brms:\n\nd &lt;- withr::with_seed(\n  seed = 12345, code = {\n    nGroups &lt;- 100\n    nObs &lt;- 20\n    theta.location &lt;- matrix(rnorm(nGroups * 2), nrow = nGroups, ncol = 2)\n    theta.location[, 1] &lt;- theta.location[, 1] - mean(theta.location[, 1])\n    theta.location[, 2] &lt;- theta.location[, 2] - mean(theta.location[, 2])\n    theta.location[, 1] &lt;- theta.location[, 1] / sd(theta.location[, 1])\n    theta.location[, 2] &lt;- theta.location[, 2] / sd(theta.location[, 2])\n    theta.location &lt;- theta.location %*% chol(matrix(c(1.5, -.25, -.25, .5^2), 2))\n    theta.location[, 1] &lt;- theta.location[, 1] - 2.5\n    theta.location[, 2] &lt;- theta.location[, 2] + 1\n    d &lt;- data.table(\n      x = rep(rep(0:1, each = nObs / 2), times = nGroups))\n    d[, ID := rep(seq_len(nGroups), each = nObs)]\n\n    for (i in seq_len(nGroups)) {\n      d[ID == i, y := rbinom(\n        n = nObs,\n        size = 1,\n        prob = plogis(theta.location[i, 1] + theta.location[i, 2] * x))\n        ]\n    }\n    copy(d)\n  })\n\nvoid &lt;- capture.output(\n    mlogit &lt;- brms::brm(\n      y ~ 1 + x + (1 + x | ID), family = \"bernoulli\",\n      data = d, seed = 1234,\n      backend = \"cmdstanr\",\n      silent = 2, refresh = 0,\n      chains = 4L, cores = 4L)\n)\n\n\n26.4.2.1 AME: Including Random Effects\n\nbm &lt;- brmsmargins(\n  mlogit,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  effects = \"includeRE\",\n  CI = .95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#&gt;          M       Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 0.111492 0.1115944 0.08095807 0.1420166          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n\navg_slopes(mlogit)\n#&gt; \n#&gt;  Term Contrast Estimate  2.5 % 97.5 %\n#&gt;     x    1 - 0    0.111 0.0806   0.14\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\n\n26.4.2.2 AME: Fixed Effects Only (Grand Mean)\n\nbm &lt;- brmsmargins(\n  mlogit,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  effects = \"fixedonly\",\n  CI = .95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#&gt;           M       Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 0.1039555 0.1034452 0.06319565 0.1491665          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n\navg_slopes(mlogit, re_formula = NA)\n#&gt; \n#&gt;  Term Contrast Estimate  2.5 % 97.5 %\n#&gt;     x    1 - 0    0.101 0.0623  0.143\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\n\n26.4.3 Marginal Effects for Location Scale Models\n\n26.4.3.1 AMEs for Fixed Effects Location Scale Models\nEstimate a fixed effects location scale model with brms:\n\nd &lt;- withr::with_seed(\n  seed = 12345, code = {\n    nObs &lt;- 1000L\n    d &lt;- data.table(\n      grp = rep(0:1, each = nObs / 2L),\n      x = rnorm(nObs, mean = 0, sd = 0.25))\n    d[, y := rnorm(nObs,\n                   mean = x + grp,\n                   sd = exp(1 + x + grp))]\n    copy(d)\n  })\n\nvoid &lt;- capture.output(\n    ls.fe &lt;- brm(bf(\n      y ~ 1 + x + grp,\n      sigma ~ 1 + x + grp),\n      family = \"gaussian\",\n      data = d, seed = 1234,\n      silent = 2, refresh = 0,\n      backend = \"cmdstanr\",\n      chains = 4L, cores = 4L)\n)\n\n\n26.4.3.2 Fixed effects only\n\nbm &lt;- brmsmargins(\n  ls.fe,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  CI = 0.95, CIType = \"ETI\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#&gt;          M     Mdn        LL      UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 1.626186 1.63215 0.7349262 2.46998          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n\navg_slopes(ls.fe, re_formula = NA)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   grp    1 - 0     1.02 0.355   1.70\n#&gt;   x      dY/dX     1.63 0.735   2.47\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\n\n26.4.3.3 Discrete change and distributional parameter (dpar)\nCompute the contrast between adjusted predictions on the sigma parameter, when grp=0 and grp=1:\n\nbm &lt;- brmsmargins(\n  ls.fe,\n  at = data.frame(grp = c(0, 1)),\n  contrasts = cbind(\"AME grp\" = c(-1, 1)),\n  CI = 0.95, CIType = \"ETI\", dpar = \"sigma\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#&gt;          M     Mdn       LL       UL PercentROPE PercentMID   CI CIType ROPE  MID   Label\n#&gt; 1 4.899239 4.89621 4.423663 5.422412          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME grp\n\nIn marginaleffects we use the comparisons() function and the variables argument:\n\navg_comparisons(\n  ls.fe,\n  variables = list(grp = 0:1),\n  dpar = \"sigma\")\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   grp    1 - 0      4.9  4.42   5.42\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\n\n26.4.3.4 Marginal effect (continuous) on sigma\n\nbm &lt;- brmsmargins(\n  ls.fe,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  CI = 0.95, CIType = \"ETI\", dpar = \"sigma\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#&gt;          M     Mdn       LL       UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 4.458758 4.46162 3.498163 5.443716          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n\navg_slopes(ls.fe, dpar = \"sigma\", re_formula = NA)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   grp    1 - 0     4.90  4.42   5.42\n#&gt;   x      dY/dX     4.46  3.50   5.44\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#effects",
    "href": "articles/alternative_software.html#effects",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.5 effects\n",
    "text": "26.5 effects\n\nThe effects package was created by John Fox and colleagues.\n\n\nmarginaleffects supports 30+ more model types than effects.\n\neffects focuses on the computation of ‚Äúadjusted predictions.‚Äù The plots it produces are roughly equivalent to the ones produced by the plot_predictions() and predictions() functions in marginaleffects.\n\neffects does not appear support marginal effects (slopes), marginal means, or contrasts\n\neffects uses Base graphics whereas marginaleffects uses ggplot2\n\n\neffects includes a lot of very powerful options to customize plots. In contrast, marginaleffects produces objects which can be customized by chaining ggplot2 functions. Users can also call plot_predictions(model, draw=FALSE) to create a prediction grid, and then work the raw data directly to create the plot they need\n\neffects offers several options which are not currently available in marginaleffects, including:\n\nPartial residuals plots\nMany types of ways to plot adjusted predictions: package vignette",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#modelbased",
    "href": "articles/alternative_software.html#modelbased",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.6 modelbased\n",
    "text": "26.6 modelbased\n\nThe modelbased package is developed by the easystats team.\nThis section is incomplete; contributions are welcome.\n\nWrapper around emmeans to compute marginal means and marginal effects.\nPowerful functions to create beautiful plots.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/alternative_software.html#ggeffects",
    "href": "articles/alternative_software.html#ggeffects",
    "title": "\n26¬† Alternative Software\n",
    "section": "\n26.7 ggeffects\n",
    "text": "26.7 ggeffects\n\nThe ggeffects package is developed by Daniel L√ºdecke.\nThis section is incomplete; contributions are welcome.\n\nWrapper around emmeans to compute marginal means.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Alternative Software</span>"
    ]
  },
  {
    "objectID": "articles/extensions.html#support-a-new-model-type",
    "href": "articles/extensions.html#support-a-new-model-type",
    "title": "\n27¬† Extensions\n",
    "section": "\n27.1 Support a new model type",
    "text": "27.1 Support a new model type\nIt is very easy to add support for new models in marginaleffects. All we need is to set a global option and define 4 very simple functions.\nIf you add support for a class of models produced by a CRAN package, please consider submitting your code for inclusion in the package: https://github.com/vincentarelbundock/marginaleffects\nIf you add support for a class of models produced by a package hosted elsewhere than CRAN, you can submit it for inclusion in the unsupported user-submitted library of extensions: Currently\n\n\ncountreg package. Thanks to Olivier Beaumais.\n\ncensreg package. Thanks to Oleg Komashko.\n\nThe rest of this section illustrates how to add support for a very simple lm_manual model.\n\n27.1.1 Fit function\nTo begin, we define a function which fits a model. Normally, this function will be supplied by a modeling package published on CRAN. Here, we create a function called lm_manual(), which estimates a linear regression model using simple linear algebra operates:\n\nlm_manual &lt;- function(f, data, ...) {\n    # design matrix\n    X &lt;- model.matrix(f, data = data)\n    # response matrix\n    Y &lt;- data[[as.character(f[2])]]\n    # coefficients\n    b &lt;- solve(crossprod(X)) %*% crossprod(X, Y)\n    Yhat &lt;- X %*% b\n    # variance-covariance matrix\n    e &lt;- Y - Yhat\n    df &lt;- nrow(X) - ncol(X)\n    s2 &lt;- sum(e^2) / df\n    V &lt;- s2 * solve(crossprod(X))\n    # model object\n    out &lt;- list(\n        d = data,\n        f = f,\n        X = X,\n        Y = Y,\n        V = V,\n        b = b)\n    # class name: lm_manual\n    class(out) &lt;- c(\"lm_manual\", \"list\")\n    return(out)\n}\n\nImportant: The custom fit function must assign a new class name to the object it returns. In the example above, the model is assigned to be of class lm_manual (see the penultimate line of code in the function).\nOur new function replicates the results of lm():\n\nmodel &lt;- lm_manual(mpg ~ hp + drat, data = mtcars)\nmodel$b\n#&gt;                    [,1]\n#&gt; (Intercept) 10.78986122\n#&gt; hp          -0.05178665\n#&gt; drat         4.69815776\n\nmodel_lm &lt;- lm(mpg ~ hp + drat, data = mtcars)\ncoef(model_lm)\n#&gt; (Intercept)          hp        drat \n#&gt; 10.78986122 -0.05178665  4.69815776\n\n\n27.1.2 marginaleffects extension\nTo extend support in marginaleffects, the first step is to tell the package that our new class is supported. We do this by defining a global option:\n\nlibrary(marginaleffects)\n\noptions(\"marginaleffects_model_classes\" = \"lm_manual\")\n\nThen, we define 4 methods:\n\n\nget_coef()\n\nMandatory arguments: model, ...\n\nReturns: named vector of parameters (coefficients).\n\n\n\nset_coef()\n\nMandatory arguments: model, coefs (named vector of coefficients), ...\n\nReturns: A new model object in which the original coefficients were replaced by the new vector.\nExample\n\n\n\nget_vcov()\n\nMandatory arguments: model, ....\nOptional arguments: vcov\n\nReturns: A named square variance-covariance matrix.\n\n\n\nget_predict()\n\nMandatory arguments: model, newdata (data frame), ...\n\nOption arguments: type and other model-specific arguments.\nReturns: A data frame with two columns: a unique rowid and a column of estimate values.\n\n\n\nNote that each of these methods will be named with the suffix .lm_manual to indicate that they should be used whenever marginaleffects needs to process an object of class lm_manual.\n\nget_coef.lm_manual &lt;- function(model, ...) {\n    b &lt;- model$b\n    b &lt;- setNames(as.vector(b), row.names(b))\n    return(b)\n}\n\nset_coef.lm_manual &lt;- function(model, coefs, ...) {\n    out &lt;- model\n    out$b &lt;- coefs\n    return(out)\n}\n\nget_vcov.lm_manual &lt;- function(model, ...) {\n    return(model$V)\n}\n\nget_predict.lm_manual &lt;- function(model, newdata, ...) {\n    newX &lt;- model.matrix(model$f, data = newdata)\n    Yhat &lt;- newX %*% model$b\n    out &lt;- data.frame(\n        rowid = seq_len(nrow(Yhat)),\n        estimate = as.vector(Yhat))\n    return(out)\n}\n\nThe methods we just defined work as expected:\n\nget_coef(model)\n#&gt; (Intercept)          hp        drat \n#&gt; 10.78986122 -0.05178665  4.69815776\n\nget_vcov(model)\n#&gt;             (Intercept)            hp         drat\n#&gt; (Intercept) 25.78356135 -3.054007e-02 -5.836030687\n#&gt; hp          -0.03054007  8.635615e-05  0.004969385\n#&gt; drat        -5.83603069  4.969385e-03  1.419990359\n\nget_predict(model, newdata = head(mtcars))\n#&gt;   rowid estimate\n#&gt; 1     1 23.41614\n#&gt; 2     2 23.41614\n#&gt; 3     3 24.06161\n#&gt; 4     4 19.56366\n#&gt; 5     5 16.52639\n#&gt; 6     6 18.31918\n\nNow we can use the avg_slopes() function:\n\navg_slopes(model, newdata = mtcars, variables = c(\"hp\", \"drat\"))\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 %  97.5 %\n#&gt;  drat   4.6982    1.19172  3.94   &lt;0.001 13.6  2.36  7.0339\n#&gt;  hp    -0.0518    0.00929 -5.57   &lt;0.001 25.2 -0.07 -0.0336\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(model, newdata = mtcars) |&gt; head()\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  mpg cyl disp  hp drat   wt qsec vs am gear carb\n#&gt;      23.4      0.671 34.9   &lt;0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n#&gt;      23.4      0.671 34.9   &lt;0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n#&gt;      24.1      0.720 33.4   &lt;0.001 810.2  22.6   25.5 22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n#&gt;      19.6      0.999 19.6   &lt;0.001 281.4  17.6   21.5 21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\n#&gt;      16.5      0.735 22.5   &lt;0.001 369.1  15.1   18.0 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\n#&gt;      18.3      1.343 13.6   &lt;0.001 138.3  15.7   21.0 18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb \n#&gt; Type:  response\n\nNote that, for custom model, we typically have to supply values for the newdata and variables arguments explicitly.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Extensions</span>"
    ]
  },
  {
    "objectID": "articles/extensions.html#modify-or-extend-supported-models",
    "href": "articles/extensions.html#modify-or-extend-supported-models",
    "title": "\n27¬† Extensions\n",
    "section": "\n27.2 Modify or extend supported models",
    "text": "27.2 Modify or extend supported models\nLet‚Äôs say you want to estimate a model using the mclogit::mblogit function. That package is already supported by marginaleffects, but you want to use a type (scale) of predictions that is not currently supported: a ‚Äúcentered link scale.‚Äù\nTo achieve this, we would need to override the get_predict.mblogit() method. However, it can be unsafe to reassign methods supplied by a package that we loaded with library. To be safe, we assign a new model class to our object (‚Äúcustomclass‚Äù) which will inherit from mblogit. Then, we define a get_predict.customclass method to make our new kinds of predictions.\nLoad libraries, estimate a model:\n\nlibrary(mclogit)\nlibrary(data.table)\n\nmodel &lt;- mblogit(\n    factor(gear) ~ am + mpg,\n    data = mtcars,\n    trace = FALSE)\n\nTell marginaleffects that we are adding support for a new class model models, and assign a new inherited class name to a duplicate of the model object:\n\noptions(\"marginaleffects_model_classes\" = \"customclass\")\n\nmodel_custom &lt;- model\n\nclass(model_custom) &lt;- c(\"customclass\", class(model))\n\nDefine a new get_predict.customclass method. We use the default predict() function to obtain predictions. Since this is a multinomial model, predict() returns a matrix of predictions with one column per level of the response variable.\nOur new get_predict.customclass method takes this matrix of predictions, modifies it, and reshapes it to return a data frame with three columns: rowid, group, and estimate:\n\nget_predict.customclass &lt;- function(model, newdata, ...) {\n    out &lt;- predict(model, newdata = newdata, type = \"link\")\n    out &lt;- cbind(0, out)\n    colnames(out)[1] &lt;- dimnames(model$D)[[1]][[1]]\n    out &lt;- out - rowMeans(out)\n    out &lt;- as.data.frame(out)\n    out$rowid &lt;- seq_len(nrow(out))\n    out &lt;- data.table(out)\n    out &lt;- melt(\n        out,\n        id.vars = \"rowid\",\n        value.name = \"estimate\",\n        variable.name = \"group\")\n}\n\nFinally, we can call any slopes() function and obtain results. Notice that our object of class customclass now produces different results than the default mblogit object:\n\navg_predictions(model)\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;      3    0.469     0.0444 10.56  &lt; 0.001 84.2 0.382  0.556\n#&gt;      4    0.375     0.0670  5.60  &lt; 0.001 25.5 0.244  0.506\n#&gt;      5    0.156     0.0501  3.12  0.00183  9.1 0.058  0.255\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_predictions(model_custom)\n#&gt; \n#&gt;  Group Estimate Std. Error         z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;      3    -1.42       2525 -0.000561    1.000 0.0 -4950   4947\n#&gt;      4     6.36       1779  0.003578    0.997 0.0 -3480   3493\n#&gt;      5    -4.95       3074 -0.001609    0.999 0.0 -6030   6020\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Extensions</span>"
    ]
  },
  {
    "objectID": "articles/links.html",
    "href": "articles/links.html",
    "title": "28¬† Links",
    "section": "",
    "text": "Equivalence Tests Using marginaleffects: Reproducing the Clark and Golder (2006) Example from Rainey (2014) by Carlisle Rainey\nYou are what you ATE: Choosing an effect size measure for binary outcomes by Cameron Patrick\nCausal inference with potential outcomes bootcamp by Solomon Kurz\nMarginal and conditional effects for GLMMs with marginaleffects by Andrew Heiss\nMarginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are by Andrew Heiss\nMatching by Noah Greifer\nDouble propensity score adjustment using g-computation by Noah Greifer\nSubgroup Analysis After Propensity Score Matching Using R by Noah Greifer\nBayesian Model Averaged Marginal Effects by A. Jordan Nafa",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Links</span>"
    ]
  },
  {
    "objectID": "articles/performance.html#what-to-do-when-marginaleffects-is-slow",
    "href": "articles/performance.html#what-to-do-when-marginaleffects-is-slow",
    "title": "\n29¬† Performance\n",
    "section": "\n29.1 What to do when marginaleffects is slow?",
    "text": "29.1 What to do when marginaleffects is slow?\nSome options:\n\nCompute marginal effects and contrasts at the mean (or other representative value) instead of all observed rows of the original dataset: Use the newdata argument and the datagrid() function.\nCompute marginal effects for a subset of variables, paying special attention to exclude factor variables which can be particularly costly to process: Use the variables argument.\nDo not compute standard errors: Use the vcov = FALSE argument.\n\nThis simulation illustrates how computation time varies for a model with 25 regressors and 100,000 observations:\n\nlibrary(marginaleffects)\n\n## simulate data and fit a large model\nN &lt;- 1e5\ndat &lt;- data.frame(matrix(rnorm(N * 26), ncol = 26))\nmod &lt;- lm(X1 ~ ., dat)\n\nresults &lt;- bench::mark(\n    # marginal effects at the mean; no standard error\n    slopes(mod, vcov = FALSE, newdata = \"mean\"),\n    # marginal effects at the mean\n    slopes(mod, newdata = \"mean\"),\n    # 1 variable; no standard error\n    slopes(mod, vcov = FALSE, variables = \"X3\"),\n    # 1 variable\n    slopes(mod, variables = \"X3\"),\n    # 26 variables; no standard error\n    slopes(mod, vcov = FALSE),\n    # 26 variables\n    slopes(mod),\n    iterations = 1, check = FALSE)\n\nresults[, c(1, 3, 5)]\n## &lt;bch:expr&gt;                                  &lt;bch:tm&gt; &lt;bch:byt&gt;\n## slopes(mod, vcov = FALSE, newdata = \"mean\") 230.09ms    1.24GB\n## slopes(mod, newdata = \"mean\")               329.14ms    1.25GB\n## slopes(mod, vcov = FALSE, variables = \"X3\")  198.7ms  496.24MB\n## slopes(mod, variables = \"X3\")                  1.27s    3.29GB\n## slopes(mod, vcov = FALSE)                      5.73s   11.05GB\n## slopes(mod)                                   21.68s   78.02GB\n\nThe benchmarks above were conducted using the development version of marginaleffects on 2023-02-03.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "articles/performance.html#speed-comparison",
    "href": "articles/performance.html#speed-comparison",
    "title": "\n29¬† Performance\n",
    "section": "\n29.2 Speed comparison",
    "text": "29.2 Speed comparison\nThe slopes() function is relatively fast. This simulation was conducted using the development version of the package on 2022-04-04:\n\nlibrary(margins)\n\nN &lt;- 1e3\ndat &lt;- data.frame(\n    y = sample(0:1, N, replace = TRUE),\n    x1 = rnorm(N),\n    x2 = rnorm(N),\n    x3 = rnorm(N),\n    x4 = factor(sample(letters[1:5], N, replace = TRUE)))\nmod &lt;- glm(y ~ x1 + x2 + x3 + x4, data = dat, family = binomial)\n\nmarginaleffects is about the same speed as margins when unit-level standard errors are not computed:\n\nresults &lt;- bench::mark(\n    slopes(mod, vcov = FALSE),\n    margins(mod, unit_ses = FALSE),\n    check = FALSE, relative = TRUE)\nresults[, c(1, 3, 5)]\n\n##   expression                        median mem_alloc\n##   &lt;bch:expr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;\n## 1 slopes(mod, vcov = FALSE)   1         1\n## 2 margins(mod, unit_ses = FALSE)       6.15      4.17\n\nmarginaleffects can be 100x times faster than margins when unit-level standard errors are computed:\n\nresults &lt;- bench::mark(\n    slopes(mod, vcov = TRUE),\n    margins(mod, unit_ses = TRUE),\n    check = FALSE, relative = TRUE, iterations = 1)\nresults[, c(1, 3, 5)]\n\n## &lt;bch:expr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;\n## slopes(mod, vcov = TRUE)          1        1  \n## margins(mod, unit_ses = TRUE)   128.      18.6\n\nModels estimated on larger datasets (&gt; 1000 observations) can be difficult to process using the margins package, because of memory and time constraints. In contrast, marginaleffects can work well on much larger datasets.\nIn some cases, marginaleffects will be considerably slower than packages like emmeans or modmarg. This is because these packages make extensive use of hard-coded analytical derivatives, or reimplement their own fast prediction functions.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "articles/python.html#fitting-a-numpyro-model",
    "href": "articles/python.html#fitting-a-numpyro-model",
    "title": "\n30¬† Python\n",
    "section": "\n30.1 Fitting a NumPyro model",
    "text": "30.1 Fitting a NumPyro model\nTo begin, we load the reticulate package which allows us to interact with the Python interpreter from an R session. Then, we write a NumPyro model and we load it to memory using the source_python() function. The important functions to note in the Python code are:\n\n\nload_df() downloads data on pulmonary fibrosis.\n\nmodel() defines the NumPyro model.\n\nfit_mcmc_model() fits the model using Markov Chain Monte Carlo.\n\npredict_mcmc(): accepts a data frame and returns a matrix of draws from the posterior distribution of adjusted predictions (fitted values).\n\n\nlibrary(reticulate)\nlibrary(marginaleffects)\n\nmodel &lt;- '\n## Model code adapted from the NumPyro documtation under Apache License:\n## https://num.pyro.ai/en/latest/tutorials/bayesian_hierarchical_linear_regression.html\n\nimport pandas as pd\nimport numpy as np\nimport numpyro\nfrom numpyro.infer import SVI, Predictive, MCMC,NUTS, autoguide, TraceMeanField_ELBO\nimport numpyro.distributions as dist\nfrom numpyro.infer.initialization import init_to_median, init_to_uniform,init_to_sample\nfrom jax import random\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\n\ndef load_df():\n    train = pd.read_csv(\"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/osic_pulmonary_fibrosis.csv\")\n    return train\n\n\ndef model(data, predict = False):\n    FVC_obs = data[\"FVC\"].values  if predict == False else None\n    patient_encoder = LabelEncoder()\n    Age_obs = data[\"Age\"].values\n    patient_code = patient_encoder.fit_transform(data[\"Patient\"].values)\n    Œº_Œ± = numpyro.sample(\"Œº_Œ±\", dist.Normal(0.0, 500.0))\n    œÉ_Œ± = numpyro.sample(\"œÉ_Œ±\", dist.HalfNormal(100.0))\n\n    age = numpyro.sample(\"age\", dist.Normal(0.0, 500.0))\n\n    n_patients = len(np.unique(patient_code))\n\n    with numpyro.plate(\"plate_i\", n_patients):\n        Œ± = numpyro.sample(\"Œ±\", dist.Normal(Œº_Œ±, œÉ_Œ±))\n\n    œÉ = numpyro.sample(\"œÉ\", dist.HalfNormal(100.0))\n    FVC_est = Œ±[patient_code] + age * Age_obs\n\n    with numpyro.plate(\"data\", len(patient_code)):\n        numpyro.sample(\"obs\", dist.Normal(FVC_est, œÉ), obs=FVC_obs)\n\n\ndef fit_mcmc_model(train_df, samples = 1000):\n    numpyro.set_host_device_count(4)\n    rng_key = random.PRNGKey(0)\n    mcmc = MCMC(\n        NUTS(model),\n        num_samples=samples,\n        num_warmup=1000,\n        progress_bar=True,\n        num_chains = 4\n        )\n    \n    mcmc.run(rng_key, train_df)\n\n    posterior_draws = mcmc.get_samples()\n\n    with open(\"mcmc_posterior_draws.pickle\", \"wb\") as handle:\n        pickle.dump(posterior_draws, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef predict_mcmc(data):\n\n    with open(\"mcmc_posterior_draws.pickle\", \"rb\") as handle:\n        posterior_draws = pickle.load(handle)\n\n    predictive = Predictive(model = model,posterior_samples=posterior_draws)\n    samples = predictive(random.PRNGKey(1), data, predict = True)\n    y_pred = samples[\"obs\"]\n    # transpose so that each column is a draw and each row is an observation\n    y_pred = np.transpose(np.array(y_pred))\n\n    return y_pred \n'\n\n## save python script to temp file\ntmp &lt;- tempfile()\ncat(model, file = tmp)\n\n## load functions\nsource_python(tmp)\n\n## download data\ndf &lt;- load_df()\n\n## fit model\nfit_mcmc_model(df)",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "articles/python.html#analyzing-the-results-in-marginaleffects",
    "href": "articles/python.html#analyzing-the-results-in-marginaleffects",
    "title": "\n30¬† Python\n",
    "section": "\n30.2 Analyzing the results in marginaleffects\n",
    "text": "30.2 Analyzing the results in marginaleffects\n\nEach of the functions in the marginaleffects package requires that users supply a model object on which the function will operate. When estimating models outside R, we do not have such a model object. We thus begin by creating a ‚Äúfake‚Äù model object: an empty data frame which we define to be of class ‚Äúcustom‚Äù. Then, we set a global option to tell marginaleffects that this ‚Äúcustom‚Äù class is supported.\n\nmod &lt;- data.frame()\nclass(mod) &lt;- \"custom\"\n\noptions(\"marginaleffects_model_classes\" = \"custom\")\n\nNext, we define a get_predict method for our new custom class. This method must accept three arguments: model, newdata, and .... The get_predict method must return a data frame with one row for each of the rows in newdata, two columns (rowid and estimate), and an attribute called posterior_draws() which hosts a matrix of posterior draws with the same number of rows as newdata.\nThe method below uses reticulate to call the predict_mcmc() function that we defined in the Python script above. The predict_mcmc() function accepts a data frame and returns a matrix with the same number of rows.\n\nget_predict.custom &lt;- function(model, newdata, ...) {\n    pred &lt;- predict_mcmc(newdata)\n    out &lt;- data.frame(\n        rowid = seq_len(nrow(newdata)),\n        predicted = apply(pred, 1, stats::median)\n    )\n    attr(out, \"posterior_draws\") &lt;- pred\n    return(out)\n}\n\nNow we can use most of the marginaleffects package functions to analyze our results. Since we use a ‚Äúfake‚Äù model object, marginaleffects cannot retrieve the original data from the model object, and we always need to supply a newdata argument:\n\n## predictions on the original dataset\npredictions(mod, newdata = df) |&gt; head()\n\n## predictions for user-defined predictor values\npredictions(mod, newdata = datagrid(newdata = df, Age = c(60, 70)))\n\npredictions(mod, newdata = datagrid(newdata = df, Age = range))\n\n## average predictions by group\npredictions(mod, newdata = df, by = \"Sex\")\n\n## contrasts (average)\navg_comparisons(mod, variables = \"Age\", newdata = df)\n\navg_comparisons(mod, variables = list(\"Age\" = \"sd\"), newdata = df)\n\n## slope (elasticity)\navg_slopes(mod, variables = \"Age\", slope = \"eyex\", newdata = df)",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#delta-method",
    "href": "articles/uncertainty.html#delta-method",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.1 Delta Method",
    "text": "31.1 Delta Method\nAll the standard errors generated by the slopes(), comparisons(), and hypotheses() functions of this package package are estimated using the delta method. Mathematical treatments of this method can be found in most statistics textbooks and on Wikipedia. Roughly speaking, the delta method allows us to approximate the distribution of a smooth function of an asymptotically normal estimator.\nConcretely, this allows us to generate standard errors around functions of a model‚Äôs coefficient estimates. Predictions, contrasts, marginal effects, and marginal means are functions of the coefficients, so we can use the delta method to estimate standard errors around all of those quantities. Since there are a lot of mathematical treatments available elsewhere, this vignette focuses on the ‚Äúimplementation‚Äù in marginaleffects.\nConsider the case of the marginal_means() function. When a user calls this function, they obtain a vector of marginal means. To estimate standard errors around this vector:\n\nTake the numerical derivative of the marginal means vector with respect to the first coefficient in the model:\n\nCompute marginal means with the original model: \\(f(\\beta)\\)\n\nIncrement the first (and only the first) coefficient held inside the model object by a small amount, and compute marginal means again: \\(f(\\beta+\\varepsilon)\\)\n\nCalculate: \\(\\frac{f(\\beta+\\varepsilon) - f(\\beta)}{\\varepsilon}\\)\n\n\n\nRepeat step 1 for every coefficient in the model to construct a \\(J\\) matrix.\nExtract the variance-covariance matrix of the coefficient estimates: \\(V\\)\n\nStandard errors are the square root of the diagonal of \\(JVJ'\\)\n\n\nScroll down this page to the Numerical Derivatives section to see a detailed explanation, along with code for manual computation.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#standard-errors-and-intervals-for-slopes-and-comparisons",
    "href": "articles/uncertainty.html#standard-errors-and-intervals-for-slopes-and-comparisons",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.2 Standard errors and intervals for slopes() and comparisons()\n",
    "text": "31.2 Standard errors and intervals for slopes() and comparisons()\n\nAll standard errors for the slopes() and comparisons() functions are computed using the delta method, as described above. The confidence intervals are calculated as estimate ¬± qnorm((1 - conf_level) / 2) standard errors (e.g., for 95% confidence intervals, estimate ¬± 1.96 standard errors) and assume that the (transformed) estimates are normally distributed.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#standard-errors-and-intervals-for-marginal_means-and-predictions",
    "href": "articles/uncertainty.html#standard-errors-and-intervals-for-marginal_means-and-predictions",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.3 Standard errors and intervals for marginal_means() and predictions()\n",
    "text": "31.3 Standard errors and intervals for marginal_means() and predictions()\n\nThe marginal_means() and predictions() function can compute the confidence intervals in two ways. If the following conditions hold:\n\nThe user sets: type = \"response\"\n\nThe model class is glm\n\nThe transform argument is NULL\n\n\nthen marginal_means() and predictions() will first compute estimates on the link scale, and then back transform them using the inverse link function supplied by insight::link_inverse(model) function.\nIn all other cases, standard errors are computed using the delta method as described above.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#robust-standard-errors",
    "href": "articles/uncertainty.html#robust-standard-errors",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.4 Robust standard errors",
    "text": "31.4 Robust standard errors\nAll the functions in the marginaleffects package can compute robust standard errors on the fly for any model type supported by the sandwich package. The vcov argument supports string shortcuts like \"HC3\", a one-sided formula to request clustered standard errors, variance-covariance matrices, or functions which return such matrices. Here are a few examples.\nAdjusted predictions with classical or heteroskedasticity-robust standard errors:\n\nlibrary(marginaleffects)\nlibrary(patchwork)\nmod &lt;- lm(mpg ~ hp, data = mtcars)\n\np &lt;- predictions(mod)\nhead(p, 2)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      22.6      0.777 29.1   &lt;0.001 614.7  21.1   24.1\n#&gt;      22.6      0.777 29.1   &lt;0.001 614.7  21.1   24.1\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp \n#&gt; Type:  response\n\np &lt;- predictions(mod, vcov = \"HC3\")\nhead(p, 2)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      22.6      0.863 26.2   &lt;0.001 499.5  20.9   24.3\n#&gt;      22.6      0.863 26.2   &lt;0.001 499.5  20.9   24.3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp \n#&gt; Type:  response\n\nMarginal effects with cluster-robust standard errors:\n\navg_slopes(mod, vcov = ~cyl)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp  -0.0682     0.0187 -3.65   &lt;0.001 11.9 -0.105 -0.0316\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nComparing adjusted predictions with classical and robust standard errors:\n\np1 &lt;- plot_predictions(mod, condition = \"hp\")\np2 &lt;- plot_predictions(mod, condition = \"hp\", vcov = \"HC3\")\np1 + p2",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#simulation-based-inference",
    "href": "articles/uncertainty.html#simulation-based-inference",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.5 Simulation-based inference",
    "text": "31.5 Simulation-based inference\nmarginaleffects offers an experimental inferences() function to conduct simulation-based inference following the strategy proposed by Krinsky & Robb (1986):\n\nDraw iter sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model‚Äôs estimated coefficients and variance equal to the model‚Äôs variance-covariance matrix (classical, ‚ÄúHC3‚Äù, or other).\nUse the iter sets of coefficients to compute iter sets of estimands: predictions, comparisons, or slopes.\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\nHere are a few examples:\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\nlibrary(ggdist)\n\nmod &lt;- glm(vs ~ hp * wt + factor(gear), data = mtcars, family = binomial)\n\nmod |&gt; predictions() |&gt; inferences(method = \"simulation\")\n#&gt; \n#&gt;  Estimate Std. Error    2.5 % 97.5 %\n#&gt;  7.84e-01      0.190 2.95e-01  0.976\n#&gt;  7.84e-01      0.162 3.50e-01  0.965\n#&gt;  8.98e-01      0.144 4.22e-01  0.990\n#&gt;  8.74e-01      0.234 1.67e-01  0.995\n#&gt;  1.31e-02      0.195 7.76e-05  0.768\n#&gt; --- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;  3.83e-01      0.300 1.43e-02  0.955\n#&gt;  1.21e-06      0.125 6.44e-12  0.430\n#&gt;  6.89e-03      0.159 2.69e-05  0.627\n#&gt;  8.07e-11      0.160 2.22e-16  0.778\n#&gt;  7.95e-01      0.164 3.52e-01  0.968\n#&gt; Columns: rowid, estimate, std.error, conf.low, conf.high, vs, hp, wt, gear \n#&gt; Type:  response\n\nmod |&gt; avg_slopes(vcov = ~gear) |&gt; inferences(method = \"simulation\")\n#&gt; \n#&gt;  Term Contrast  Estimate Std. Error   2.5 %  97.5 %\n#&gt;  gear    4 - 3 -3.92e-02    0.05744 -0.0910 0.12941\n#&gt;  gear    5 - 3 -1.93e-01    0.27384 -0.4888 0.33715\n#&gt;  hp      dY/dX -5.02e-03    0.00439 -0.0113 0.00472\n#&gt;  wt      dY/dX -4.06e-05    0.31283 -0.5676 0.72261\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response\n\nSince simulation based inference generates iter estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the posterior_draws() function, and plot their distributions using packages likeggplot2 and ggdist:\n\nmod |&gt;\n  avg_comparisons(variables = \"gear\") |&gt;\n  inferences(method = \"simulation\") |&gt;\n  posterior_draws(\"rvar\") |&gt;\n  ggplot(aes(y = contrast, xdist = rvar)) +\n  stat_slabinterval()",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#bootstrap",
    "href": "articles/uncertainty.html#bootstrap",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.6 Bootstrap",
    "text": "31.6 Bootstrap\nIt is easy to use the bootstrap as an alternative strategy to compute standard errors and confidence intervals. Several R packages can help us achieve this, including the long-established boot package:\n\nlibrary(boot)\nset.seed(123)\n\nbootfun &lt;- function(data, indices, ...) {\n    d &lt;- data[indices, ]\n    mod &lt;- lm(mpg ~ am + hp + factor(cyl), data = d)\n    cmp &lt;- comparisons(mod, newdata = d, vcov = FALSE, variables = \"am\")\n    tidy(cmp)$estimate\n}\n\nb &lt;- boot(data = mtcars, statistic = bootfun, R = 1000)\n\nb\n#&gt; \n#&gt; ORDINARY NONPARAMETRIC BOOTSTRAP\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; boot(data = mtcars, statistic = bootfun, R = 1000)\n#&gt; \n#&gt; \n#&gt; Bootstrap Statistics :\n#&gt;     original     bias    std. error\n#&gt; t1* 4.157856 0.01543426    1.003461\nboot.ci(b, type = \"perc\")\n#&gt; BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#&gt; Based on 1000 bootstrap replicates\n#&gt; \n#&gt; CALL : \n#&gt; boot.ci(boot.out = b, type = \"perc\")\n#&gt; \n#&gt; Intervals : \n#&gt; Level     Percentile     \n#&gt; 95%   ( 2.240,  6.277 )  \n#&gt; Calculations and Intervals on Original Scale\n\nNote that, in the code above, we set vcov=FALSE to avoid computation of delta method standard errors and speed things up.\nCompare to the delta method standard errors:\n\nmod &lt;- lm(mpg ~ am + hp + factor(cyl), data = mtcars)\navg_comparisons(mod, variables = \"am\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    am    1 - 0     4.16       1.26 3.31   &lt;0.001 10.1   1.7   6.62\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#mixed-effects-models-satterthwaite-and-kenward-roger-corrections",
    "href": "articles/uncertainty.html#mixed-effects-models-satterthwaite-and-kenward-roger-corrections",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.7 Mixed effects models: Satterthwaite and Kenward-Roger corrections",
    "text": "31.7 Mixed effects models: Satterthwaite and Kenward-Roger corrections\nFor linear mixed effects models we can apply the Satterthwaite and Kenward-Roger corrections in the same way as above:\n\nlibrary(marginaleffects)\nlibrary(patchwork)\nlibrary(lme4)\n\ndat &lt;- mtcars\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lmer(mpg ~ hp + am + (1 | cyl), data = dat)\n\nMarginal effects at the mean with classical standard errors and z-statistic:\n\nslopes(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    am TRUE - FALSE   4.6661     1.1343  4.11   &lt;0.001 14.6  2.4430  6.8892\n#&gt;    hp dY/dX         -0.0518     0.0115 -4.52   &lt;0.001 17.3 -0.0743 -0.0294\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, am, cyl \n#&gt; Type:  response\n\nMarginal effects at the mean with Kenward-Roger adjusted variance-covariance and degrees of freedom:\n\nslopes(mod,\n                newdata = \"mean\",\n                vcov = \"kenward-roger\")\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     t Pr(&gt;|t|)   S  2.5 %  97.5 %   Df\n#&gt;    am TRUE - FALSE   4.6661     1.2824  3.64   0.0874 3.5 -1.980 11.3121 1.68\n#&gt;    hp dY/dX         -0.0518     0.0152 -3.41   0.0964 3.4 -0.131  0.0269 1.68\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, am, cyl, df \n#&gt; Type:  response\n\nWe can use the same option in any of the package‚Äôs core functions, including:\n\nplot_predictions(mod, condition = \"hp\", vcov = \"satterthwaite\")",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#numerical-derivatives-sensitivity-to-step-size",
    "href": "articles/uncertainty.html#numerical-derivatives-sensitivity-to-step-size",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.8 Numerical derivatives: Sensitivity to step size",
    "text": "31.8 Numerical derivatives: Sensitivity to step size\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin &lt;- ifelse(dat$body_mass_g &gt; median(dat$body_mass_g, na.rm = TRUE), 1, 0)\nmod &lt;- glm(large_penguin ~ bill_length_mm * flipper_length_mm + species, data = dat, family = binomial)\n\nmarginaleffects uses numerical derivatives in two contexts:\n\nEstimate the partial derivatives reported by slopes() function.\n\nCentered finite difference\n\n\\(\\frac{f(x + \\varepsilon_1 / 2) - f(x - \\varepsilon_1 / 2)}{\\varepsilon_1}\\), where we take the derivative with respect to a predictor of interest, and \\(f\\) is the predict() function.\n\n\nEstimate standard errors using the delta method.\n\nForward finite difference\n\n\\(\\frac{g(\\hat{\\beta}) - g(\\hat{\\beta} + \\varepsilon_2)}{\\varepsilon_2}\\), where we take the derivative with respect to a model‚Äôs coefficients, and \\(g\\) is a marginaleffects function which returns some quantity of interest (e.g., slope, marginal means, predictions, etc.)\n\n\n\nNote that the step sizes used in those two contexts can differ. If the variables and coefficients have very different scales, it may make sense to use different values for \\(\\varepsilon_1\\) and \\(\\varepsilon_2\\).\nBy default, \\(\\varepsilon_1\\) is set to 1e-4 times the range of the variable with respect to which we are taking the derivative. By default, \\(\\varepsilon_2\\) is set to the maximum value of 1e-8, or 1e-4 times the smallest absolute coefficient estimate. (These choices are arbitrary, but I have found that in practice, smaller values can produce unstable results.)\n\\(\\varepsilon_1\\) can be controlled by the eps argument of the slopes() function. \\(\\varepsilon_2\\) can be controlled by setting a global option which tells marginaleffects to compute the jacobian using the numDeriv package instead of its own internal functions. This allows more control over the step size, and also gives access to other differentiation methods, such as Richardson‚Äôs. To use numDeriv, we define a list of arguments which will be pushed forward to numDeriv::jacobian:\n\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.68   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"Richardson\"))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.68   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-3)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279      0.568 0.049    0.961 0.1 -1.09   1.14\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-5)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00601 4.64   &lt;0.001 18.1 0.0161 0.0396\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-7)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.68   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\nNotice that the standard errors can vary considerably when using different step sizes. It is good practice for analysts to consider the sensitivity of their results to this setting.\nNow, we illustrate the full process of standard error computation, using raw R code. First, we choose two step sizes:\n\neps1 &lt;- 1e-5 # slope\neps2 &lt;- 1e-7 # delta method\n\ns &lt;- slopes(mod, newdata = head(dat, 3), variables = \"bill_length_mm\", eps = eps1)\nprint(s[, 1:5], digits = 6)\n#&gt; \n#&gt;            Term  Estimate Std. Error       z\n#&gt;  bill_length_mm 0.0179765  0.0086771 2.07172\n#&gt;  bill_length_mm 0.0359630  0.0126120 2.85150\n#&gt;  bill_length_mm 0.0849071  0.0213175 3.98298\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic\n\nWe can get the same estimates manually with these steps:\n\nlinkinv &lt;- mod$family$linkinv\n\n## increment the variable of interest by h\ndat_hi &lt;- transform(dat, bill_length_mm = bill_length_mm + eps1)\n\n## model matrices: first 3 rows\nmm_lo &lt;- insight::get_modelmatrix(mod, data = dat)[1:3,]\nmm_hi &lt;- insight::get_modelmatrix(mod, data = dat_hi)[1:3,]\n\n## predictions\np_lo &lt;- linkinv(mm_lo %*% coef(mod))\np_hi &lt;- linkinv(mm_hi %*% coef(mod))\n\n## slopes\n(p_hi - p_lo) / eps1\n#&gt;         [,1]\n#&gt; 1 0.01797653\n#&gt; 2 0.03596304\n#&gt; 3 0.08490712\n\nTo get standard errors, we build a jacobian matrix where each column holds derivatives of the vector valued slope function, with respect to each of the coefficients. Using the same example:\n\nb_lo &lt;- b_hi &lt;- coef(mod)\nb_hi[1] &lt;- b_hi[1] + eps2\n\ndydx_lo &lt;- (linkinv(mm_hi %*% b_lo) - linkinv(mm_lo %*% b_lo)) / eps1\ndydx_hi &lt;- (linkinv(mm_hi %*% b_hi) - linkinv(mm_lo %*% b_hi)) / eps1\n(dydx_hi - dydx_lo) / eps2\n#&gt;         [,1]\n#&gt; 1 0.01600109\n#&gt; 2 0.02771394\n#&gt; 3 0.02275957\n\nThis gives us the first column of \\(J\\), which we can recover in full from the marginaleffects object attribute:\n\nJ &lt;- attr(s, \"jacobian\")\nJ\n#&gt;      (Intercept) bill_length_mm flipper_length_mm speciesChinstrap speciesGentoo bill_length_mm:flipper_length_mm\n#&gt; [1,]  0.01600109      0.6775622          2.897231                0             0                         122.6916\n#&gt; [2,]  0.02771394      1.1957935          5.153128                0             0                         222.4993\n#&gt; [3,]  0.02275957      1.1500800          4.440004                0             0                         224.0828\n\nTo build the full matrix, we would simply iterate through the coefficients, incrementing them one after the other. Finally, we get standard errors via:\n\nsqrt(diag(J %*% vcov(mod) %*% t(J)))\n#&gt; [1] 0.008677096 0.012611983 0.021317511\n\nWhich corresponds to our original standard errors:\n\nprint(s[, 1:5], digits = 7)\n#&gt; \n#&gt;            Term   Estimate  Std. Error        z\n#&gt;  bill_length_mm 0.01797653 0.008677096 2.071722\n#&gt;  bill_length_mm 0.03596304 0.012611983 2.851498\n#&gt;  bill_length_mm 0.08490712 0.021317511 3.982975\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic\n\nReverting to default settings:\n\noptions(marginaleffects_numDeriv = NULL)\n\nNote that our default results for this model are very similar ‚Äì but not exactly identical ‚Äì to those generated by the margins. As should be expected, the results in margins are also very sensitive to the value of eps for this model:\n\nlibrary(margins)\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.008727977 0.012567079 0.021293275\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-4, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.2269512 0.2255849 0.6636208\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-5, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.02317078 0.02928267 0.05480282",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/uncertainty.html#bayesian-estimates-and-credible-intervals",
    "href": "articles/uncertainty.html#bayesian-estimates-and-credible-intervals",
    "title": "\n31¬† Standard Errors\n",
    "section": "\n31.9 Bayesian estimates and credible intervals",
    "text": "31.9 Bayesian estimates and credible intervals\nSee the brms vignette for a discussion of bayesian estimates and credible intervals.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Standard Errors</span>"
    ]
  },
  {
    "objectID": "articles/supported_models.html",
    "href": "articles/supported_models.html",
    "title": "\n32¬† Supported Models\n",
    "section": "",
    "text": "This table shows the list of 88 supported model types. There are three main alternative software packages to compute such slopes (1) Stata‚Äôs margins command, (2) R‚Äôs margins::margins() function, and (3) R‚Äôs emmeans::emtrends() function. The test suite hosted on Github compares the numerical equivalence of results produced by marginaleffects::slopes() to those produced by all 3 alternative software packages:\n\n‚úì: a green check means that the results of at least one model are equal to a reasonable tolerance.\n‚úñ: a red cross means that the results are not identical; extra caution is warranted.\nU: a grey U means that computing slopes for a model type is unsupported by alternative packages, but supported by marginaleffects.\nAn empty cell means means that no comparison has been made yet.\n\nI am eager to add support for new models. Feel free to file a request or submit code on Github.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical equivalence\n\n\n\n\nSupported by marginaleffects\n\n\nStata\n\n\nmargins\n\n\nemtrends\n\n\n\nPackage\nFunction\ndY/dX\nSE\ndY/dX\nSE\ndY/dX\nSE\n\n\n\n\nstats\nlm\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nglm\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nnls\n\n\n\n\n\n\n\n\n\nloess\n\n\n‚úì\n\nU\nU\n\n\nAER\nivreg\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU\n\n\n\ntobit\n‚úì\n‚úì\nU\nU\n‚úì\n‚úì\n\n\nafex\nafex_aov\n\n\nU\nU\n‚úì\n‚úì\n\n\naod\nbetabin\n\n\nU\nU\nU\nU\n\n\nbetareg\nbetareg\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nbife\nbife\n\n\nU\nU\nU\nU\n\n\nbiglm\nbiglm\n\n\nU\nU\nU\nU\n\n\n\nbigglm\n\n\nU\nU\nU\nU\n\n\nblme\nblmer\n\n\n\n\n\n\n\n\n\nbglmer\n\n\n\n\n\n\n\n\nbrglm2\nbracl\n\n\nU\nU\nU\nU\n\n\n\nbrglmFit\n\n\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nbrnb\n\n\n‚úì\n‚úì\nU\nU\n\n\n\nbrmultinom\n\n\nU\nU\nU\nU\n\n\nbrms\nbrm\n\n\nU\nU\n‚úì\n‚úì\n\n\ncrch\ncrch\n\n\nU\nU\nU\nU\n\n\n\nhxlr\n\n\nU\nU\nU\nU\n\n\nDCchoice\noohbchoice\n\n\n\n\n\n\n\n\nestimatr\nlm_lin\n\n\n\n\n\n\n\n\n\nlm_robust\n‚úì\n‚úì\n‚úì\nU\n‚úì\n‚úì\n\n\n\niv_robust\n‚úì\n‚úì\nU\nU\nU\nU\n\n\nfixest\nfeols\n‚úì\n‚úì\nU\nU\nU\nU\n\n\n\nfeglm\n\n\nU\nU\nU\nU\n\n\n\nfenegbin\n\n\nU\nU\nU\nU\n\n\n\nfepois\n‚úì\n‚úì\nU\nU\nU\nU\n\n\ngam\ngam\n\n\nU\nU\n‚úì\n‚úì\n\n\ngamlss\ngamlss\n\n\nU\nU\n‚úì\n‚úì\n\n\ngeepack\ngeeglm\n\n\nU\nU\n‚úì\n‚úì\n\n\nglmmTMB\nglmmTMB\n\n\nU\nU\n‚úì\n‚úì\n\n\nglmx\nglmx\n\n\n‚úì\nU\nU\nU\n\n\nivreg\nivreg\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU\n\n\nmlr3\nLearner\n\n\n\n\n\n\n\n\nlme4\nlmer\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nglmer\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nglmer.nb\n\n\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nlmerTest\nlmer\n\n\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nlogistf\nlogistf\n\n\n\n\n\n\n\n\n\nflic\n\n\n\n\n\n\n\n\n\nflac\n\n\n\n\n\n\n\n\nMASS\nglmmPQL\n\n\nU\nU\n‚úì\n‚úì\n\n\n\nglm.nb\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\npolr\n‚úì\n‚úì\n‚úñ\n‚úñ\n‚úì\n‚úì\n\n\n\nrlm\n\n\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nmclogit\nmblogit\n\n\nU\nU\nU\nU\n\n\n\nmclogit\n\n\nU\nU\nU\nU\n\n\nMCMCglmm\nMCMCglmm\nU\nU\nU\nU\nU\nU\n\n\nmgcv\ngam\n\n\nU\nU\n‚úì\n‚úì\n\n\n\nbam\n\n\nU\nU\n‚úì\n‚úñ\n\n\nmhurdle\nmhurdle\n\n\n‚úì\n‚úì\nU\nU\n\n\nmlogit\nmlogit\n\n\nU\nU\nU\nU\n\n\nnlme\ngls\n\n\nU\nU\n‚úì\n‚úì\n\n\n\nlme\n\n\n\n\n\n\n\n\nnnet\nmultinom\n‚úì\n‚úì\nU\nU\nU\nU\n\n\nordbetareg\nordbetareg\n\n\nU\nU\n\n\n\n\nordinal\nclm\n‚úì\n‚úì\nU\nU\nU\nU\n\n\nplm\nplm\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU\n\n\nphylolm\nphylolm\n\n\n\n\n\n\n\n\n\nphyloglm\n\n\n\n\n\n\n\n\npscl\nhurdle\n\n\n‚úì\nU\n‚úì\n‚úñ\n\n\n\nhurdle\n\n\n‚úì\nU\n‚úì\n‚úñ\n\n\n\nzeroinfl\n‚úì\n‚úì\n‚úì\nU\n‚úì\n‚úì\n\n\nquantreg\nrq\n‚úì\n‚úì\nU\nU\n‚úì\n‚úì\n\n\nRchoice\nhetprob\n\n\n\n\n\n\n\n\n\nivpml\n\n\n\n\n\n\n\n\nrms\nols\n\n\n\n\n\n\n\n\n\nlrm\n\n\n\n\n\n\n\n\n\norm\n\n\n\n\n\n\n\n\nrobust\nlmRob\n\n\nU\nU\nU\nU\n\n\nrobustbase\nglmrob\n\n\n‚úì\n‚úì\nU\nU\n\n\n\nlmrob\n\n\n‚úì\n‚úì\nU\nU\n\n\nrobustlmm\nrlmer\n\n\nU\nU\n\n\n\n\nrstanarm\nstan_glm\n\n\n‚úñ\nU\n‚úì\n‚úì\n\n\nsampleSelection\nselection\n\n\nU\nU\nU\nU\n\n\n\nheckit\n\n\nU\nU\nU\nU\n\n\nscam\nscam\n\n\nU\nU\nU\nU\n\n\nspeedglm\nspeedglm\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU\n\n\n\nspeedlm\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU\n\n\nsurvey\nsvyglm\n\n\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\nsvyolr\n\n\n\n\n\n\n\n\nsurvival\nclogit\n\n\n\n\n\n\n\n\n\ncoxph\n‚úì\n‚úì\nU\nU\n‚úì\n‚úì\n\n\n\nsurvreg\n\n\n\n\n\n\n\n\ntobit1\ntobit1\n\n\n‚úì\n‚úì\nU\nU\n\n\ntruncreg\ntruncreg\n‚úì\n‚úì\n‚úì\n‚úì\nU\nU",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>32</span>¬† <span class='chapter-title'>Supported Models</span>"
    ]
  },
  {
    "objectID": "articles/tables.html#marginal-effects",
    "href": "articles/tables.html#marginal-effects",
    "title": "\n33¬† Tables\n",
    "section": "\n33.1 Marginal effects",
    "text": "33.1 Marginal effects\nWe can summarize the results of the comparisons() or slopes() functions using the modelsummary package.\n\nlibrary(modelsummary)\nlibrary(marginaleffects)\n\nmod &lt;- glm(am ~ wt + drat, family = binomial, data = mtcars)\nmfx &lt;- slopes(mod)\n\nmodelsummary(mfx)\n\n\n\n\n¬†(1)\n\n\n\ndrat\n0.278\n\n\n\n(0.168)\n\n\nwt\n‚àí0.217\n\n\n\n(0.080)\n\n\nNum.Obs.\n32\n\n\nAIC\n22.0\n\n\nBIC\n26.4\n\n\nLog.Lik.\n‚àí8.011\n\n\nF\n3.430\n\n\nRMSE\n0.28\n\n\n\n\n\nThe same results can be visualized with modelplot():\n\nmodelplot(mfx)",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "articles/tables.html#contrasts",
    "href": "articles/tables.html#contrasts",
    "title": "\n33¬† Tables\n",
    "section": "\n33.2 Contrasts",
    "text": "33.2 Contrasts\nWhen using the comparisons() function (or the slopes() function with categorical variables), the output will include two columns to uniquely identify the quantities of interest: term and contrast.\n\ndat &lt;- mtcars\ndat$gear &lt;- as.factor(dat$gear)\nmod &lt;- glm(vs ~ gear + mpg, data = dat, family = binomial)\n\ncmp &lt;- comparisons(mod)\nget_estimates(cmp)\n#&gt; # A tibble: 3 √ó 8\n#&gt;   term  contrast          estimate std.error statistic    p.value conf.low conf.high\n#&gt;   &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 gear  mean(4) - mean(3)   0.0372    0.137      0.272 0.785       -0.230     0.305 \n#&gt; 2 gear  mean(5) - mean(3)  -0.340     0.0988    -3.44  0.000588    -0.533    -0.146 \n#&gt; 3 mpg   mean(+1)            0.0608    0.0128     4.74  0.00000219   0.0356    0.0860\n\nWe can use the shape argument of the modelsummary function to structure the table properly:\n\nmodelsummary(cmp, shape = term + contrast ~ model)\n\n\n\n\n\n¬†(1)\n\n\n\ngear\nmean(4) - mean(3)\n0.037\n\n\n\nmean(4) - mean(3)\n(0.137)\n\n\n\nmean(5) - mean(3)\n‚àí0.340\n\n\n\nmean(5) - mean(3)\n(0.099)\n\n\nmpg\nmean(+1)\n0.061\n\n\n\nmean(+1)\n(0.013)\n\n\nNum.Obs.\n\n32\n\n\nAIC\n\n26.2\n\n\nBIC\n\n32.1\n\n\nLog.Lik.\n\n‚àí9.101\n\n\nF\n\n2.389\n\n\nRMSE\n\n0.31\n\n\n\n\n\nCross-contrasts can be a bit trickier, since there are multiple simultaneous groups. Consider this example:\n\nmod &lt;- lm(mpg ~ factor(cyl) + factor(gear), data = mtcars)\ncmp &lt;- comparisons(\n  mod,\n  variables = c(\"gear\", \"cyl\"),\n  cross = TRUE)\nget_estimates(cmp)\n#&gt; # A tibble: 4 √ó 9\n#&gt;   term  contrast_cyl      contrast_gear     estimate std.error statistic p.value conf.low conf.high\n#&gt;   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 cross mean(6) - mean(4) mean(4) - mean(3)    -5.33      2.77     -1.93 0.0542     -10.8  0.0953  \n#&gt; 2 cross mean(6) - mean(4) mean(5) - mean(3)    -5.16      2.63     -1.96 0.0500     -10.3  0.000165\n#&gt; 3 cross mean(8) - mean(4) mean(4) - mean(3)    -9.22      3.62     -2.55 0.0108     -16.3 -2.13    \n#&gt; 4 cross mean(8) - mean(4) mean(5) - mean(3)    -9.04      3.19     -2.84 0.00453    -15.3 -2.80\n\nAs we can see above, there are two relevant grouping columns: contrast_gear and contrast_cyl. We can simply plug those names in the shape argument:\n\nmodelsummary(\n  cmp,\n  shape = contrast_gear + contrast_cyl ~ model)\n\n\n\ngear\ncyl\n¬†(1)\n\n\n\nmean(4) - mean(3)\nmean(6) - mean(4)\n‚àí5.332\n\n\nmean(4) - mean(3)\nmean(6) - mean(4)\n(2.769)\n\n\nmean(4) - mean(3)\nmean(8) - mean(4)\n‚àí9.218\n\n\nmean(4) - mean(3)\nmean(8) - mean(4)\n(3.618)\n\n\nmean(5) - mean(3)\nmean(6) - mean(4)\n‚àí5.156\n\n\nmean(5) - mean(3)\nmean(6) - mean(4)\n(2.631)\n\n\nmean(5) - mean(3)\nmean(8) - mean(4)\n‚àí9.042\n\n\nmean(5) - mean(3)\nmean(8) - mean(4)\n(3.185)\n\n\nNum.Obs.\n\n32\n\n\nR2\n\n0.740\n\n\nR2 Adj.\n\n0.701\n\n\nAIC\n\n173.7\n\n\nBIC\n\n182.5\n\n\nLog.Lik.\n\n‚àí80.838\n\n\nF\n\n19.190\n\n\nRMSE\n\n3.03",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "articles/tables.html#marginal-means",
    "href": "articles/tables.html#marginal-means",
    "title": "\n33¬† Tables\n",
    "section": "\n33.3 Marginal means",
    "text": "33.3 Marginal means\n\nlibrary(\"marginaleffects\")\nlibrary(\"modelsummary\")\n\ndat &lt;- mtcars\ndat$cyl &lt;- as.factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ hp + cyl + am, data = dat)\nmm &lt;- marginal_means(mod)\n\nmodelsummary(mm,\n             title = \"Estimated Marginal Means\",\n             estimate = \"{estimate} ({std.error}){stars}\",\n             statistic = NULL,\n             group = term + value ~ model)\n\n\nEstimated Marginal Means\n\n\n\n¬†(1)\n\n\n\ncyl\n4\n22.885 (1.357)***\n\n\n\n6\n18.960 (1.073)***\n\n\n\n8\n19.351 (1.377)***\n\n\nam\nFALSE\n18.320 (0.785)***\n\n\n\nTRUE\n22.478 (0.834)***\n\n\nNum.Obs.\n\n32\n\n\nR2\n\n0.825\n\n\nR2 Adj.\n\n0.799\n\n\nAIC\n\n161.0\n\n\nBIC\n\n169.8\n\n\nLog.Lik.\n\n‚àí74.502\n\n\nF\n\n31.794\n\n\nRMSE\n\n2.48",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "articles/faq.html#stack-overflow-questions",
    "href": "articles/faq.html#stack-overflow-questions",
    "title": "\n34¬† FAQ\n",
    "section": "\n34.1 Stack Overflow questions",
    "text": "34.1 Stack Overflow questions\n\nplot_predictions() over a range of unobserved values\nPlot the marginal effects from a plm package model\nModels with demeaned, polynomials, or transformed variables",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "articles/faq.html#calling-marginaleffects-in-functions-loops-environments-or-after-re-assigning-variables",
    "href": "articles/faq.html#calling-marginaleffects-in-functions-loops-environments-or-after-re-assigning-variables",
    "title": "\n34¬† FAQ\n",
    "section": "\n34.2 Calling marginaleffects in functions, loops, environments, or after re-assigning variables",
    "text": "34.2 Calling marginaleffects in functions, loops, environments, or after re-assigning variables\nFunctions from the marginaleffects package can sometimes fail when they are called inside a function, loop, or other environments. To see why, it is important to know that marginaleffects often needs to operate on the original data that was used to fit the model. To extract this original data, we use the get_data() function from the insight package.\nIn most cases, get_data() can extract the data which is stored inside the model object created by the modeling package. However, some modeling packages do not save the original data in the model object (in order to save memory). In those cases, get_data() will parse the call to find the name of the data object, and will search for that data object in the global environment. When users fit models in a different environment (e.g., function calls), get_data() may not be able to retrieve the original data.\nA related problem can arise if users fit a model, but then assign a new value to the variable that used to store the dataset.\nRecommendations:\n\nSupply your dataset explicitly to the newdata argument of slopes() functions.\nAvoid assigning a new value to a variable that you use to store a dataset for model fitting.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#dev",
    "href": "articles/NEWS.html#dev",
    "title": "35¬† News",
    "section": "dev",
    "text": "dev\nNew supported packages:\n\ndbarts: https://cran.r-project.org/package=dbarts\nmvgam: https://nicholasjclark.github.io/mvgam/ Not available on CRAN yet, but this package maintains its own marginaleffects support function.\n\nBugs:\n\nCustom functions in the comparison argument of comparisons() did not supply the correct x vector length for bayesian models when the by argument is used. Thanks to @Sandhu-SS for report #931.\nAdd suport for two facet variables (through facet_grid) when plotting using condition",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section",
    "href": "articles/NEWS.html#section",
    "title": "35¬† News",
    "section": "0.16.0",
    "text": "0.16.0\nMachine learning support:\n\ntidymodels package\nmlr3 package\n\nMisc:\n\nNew vignettes:\n\nInverse Probability Weighting\nMachine Learning\nMatching\n\nAdd support for hypotheses() to inferences(). Thanks to @Tristan-Siegfried for code contribution #908.\nSupport survival::survreg(). Thanks to Carlisle Rainey for Report #911.\ncolumn_names argument in print.marginaleffects() to suppress the printed column names at the bottom of the printout.\nThe function supplied to the comparison argument of the comparisons() function can now operate on x and on newdata directly (e.g., to check the number of observations).\nMore informative errors from predict().\n\nBugs:\n\nSome gamlss models generated an error related to the what argument. Thanks to @DHLocke for Issue #933",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-1",
    "href": "articles/NEWS.html#section-1",
    "title": "35¬† News",
    "section": "0.15.1",
    "text": "0.15.1\n\nhypotheses(): The FUN argument handles group columns gracefully.\nNative support for Amelia for multiple imputation.\n\nDocumentation:\n\nNew section on ‚ÄúComplex aggregations‚Äù in the Hypothesis testing vignette.\n\nBug fix:\n\nResults of the predictions() function could be inaccurate when (a) running version 0.15.0, (b) type is NULL or invlink(link), (c) model is glm(), and (d) the hypothesis argument is non-numeric. Thanks to @strengejacke for report [#903](https://github.com/vincentarelbundock/marginaleffects/issues/903)",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-2",
    "href": "articles/NEWS.html#section-2",
    "title": "35¬† News",
    "section": "0.15.0",
    "text": "0.15.0\nNew:\n\nConformal prediction via inferences()\nhypothesis argument now accepts multiple string formulas.\nThe type argument now accepts an explicit invlink(link) value instead of silently back-transforming. Users are no longer pointed to type_dictionary. Instead, they should call their function with a bad type value, and they will obtain a list of valid types. The default type value is printed in the output. This is useful because the default type value is NULL, so the user often does not explicitly decide.\nAllow install with Rcpp 1.0.0 and greater.\n\nSupport new models:\n\nsurvey::svyolr()\n\nMisc:\n\ninferences(method=\"simulation\") uses the original point estimate rather than the mean of the simulation distribution. Issue #851.\nBetter documentation and error messages for newdata=NULL\nSome performance improvements for predictions() and marginalmeans() (#880, #882, @etiennebacher).\n\nBug fix:\n\nnewdata=\"median\" returned mean of binary variables. Thanks to @jkhanson1970 for report #896.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-3",
    "href": "articles/NEWS.html#section-3",
    "title": "35¬† News",
    "section": "0.14.0",
    "text": "0.14.0\nBreaking changes:\n\nRow order of the output changes for some objects. Rows are not sorted alphabetically by term, by, and variables explicitly supplied to datagrid. This can affect hypothesis tests computed using the b1, b2, b3, and other indices.\nNew procedure numderiv argument use a different procedure to select the step size used in the finite difference numeric derivative used to compute standard errors: abs(x) * sqrt(.Machine$double.eps). The numerical results may not be exactly identical to previous versions of marginaleffects, but the step size should be adequate in a broader variety of cases. Note that users can use the numderiv argument for more control on numeric differentiation, as documented.\nbife models are no longer supported pending investigation in weird results in the tests. Looking for volunteers write more thorough tests.\n\nNew:\n\nSupport: logistf package.\nSupport: DCchoice package.\nSupport: stats::nls\nhypotheses() can now accept raw data frame, which gives a lot of flexibility for custom contrasts and functions. See the Hypothesis vignette for an example.\nnumderiv argument allows users to use finite difference (center or forward) or Richardson‚Äôs method to compute the numerical derivatives used in the calculation of standard errors.\n\nBug fixes:\n\ninferences() supports the cross argument for comparisons() objects. Thanks to Kirill Solovev for report #856.\nsplines::bs() in formulas could produce incorrect results due to weirdness in stats::model.matrix(). Thanks to @chiungming for report #831.\nmgcv with ocat are now supported. Thanks to Lorenzo Fabbri for Issue #844.\nquantreg problem with rowid merge did not affect estimates but did not return the full original data. Issue #829.\nget_modeldata() extracts weights variable when available.\npredictions() is no longer broken in some inferences() calls. Issue #853\nInaccurate averaging with comparison=differenceavg some models where all predictors are categorical. Thanks to Karl Ove Hufthammer for report #865.\n\nMisc:\n\nMajor refactor to simplify the code base and make maintenance easier.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-4",
    "href": "articles/NEWS.html#section-4",
    "title": "35¬† News",
    "section": "0.13.0",
    "text": "0.13.0\nBreaking change:\n\nglmmTMB: Standard errors are no longer supported because they may have been erroneous. Follow Issue #810 on Github for developments: https://github.com/vincentarelbundock/marginaleffects/issues/810\n\nNew:\n\nhypothesis argument accepts wildcards: hypothesis = \"b*=b1\"\ns.value column in all output: Shannon transforms for p values. See Greenland (2019).\nmarginal_means supports mira (mice objects).\ncomparisons(): The variables arguments now accepts arbitrary numeric vectors of length equal to the number of rows in newdata. This allows users to specify fully custom treatment sizes. In the documentation examples, we show how to estimate the difference for a 1 standard deviation shift in a regressor, where the standard deviation is calculated on a group-wise basis.\ncomparisons(): the variables argument now accepts ‚Äúrevpairwise‚Äù, ‚Äúrevsequential‚Äù, ‚Äúrevreference‚Äù for factor and character variables.\ncomparisons(): the comparison argument now accept ‚Äúlift‚Äù and ‚Äúliftavg‚Äù.\n\nPerformance:\n\nComputing elasticities for linear models is now up to 30% faster (#787, @etiennebacher).\n\nBug fixes:\n\nBetter handling of environments when newdata is a function call. Thanks to @jcccf for report #814 and to @capnrefsmmat for the proposed fix using the rlang package.\nDegrees of freedom mismatch for joint hypothesis tests. Thanks to @snhansen for report #789.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-5",
    "href": "articles/NEWS.html#section-5",
    "title": "35¬† News",
    "section": "0.12.0",
    "text": "0.12.0\nBreaking change:\n\nRow order of output has changed for many calls, especially those using the by argument. This may break hypothesis tests conducted by indexing b1, b2, etc. This was necessary to fix Issue #776. Thanks to @marcora for the report.\n\nNew:\n\nhypotheses(): Joint hypothesis tests (F and Chi-square) with the joint and joint_test arguments.\nvcov.hypotheses method.\nwts is now available in plot_predictions(), plot_comparisons(), and plot_slopes().\n\nBug:\n\nWrong order of rows in bayesian models with by argument. Thanks to @shirdekel for report #782.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-6",
    "href": "articles/NEWS.html#section-6",
    "title": "35¬† News",
    "section": "0.11.2",
    "text": "0.11.2\n\nvcov() and coef() methods for marginaleffects objects.\nStrings in wts are accepted with the by argument.\npredictions() and avg_predictions() no longer use an automatic backtransformation for GLM models unless hypothesis is NULL.\nvcov() can be used to retrieve a full variance-covariance matrix from objects produced by comparisons(), slopes(), predictions(), or marginal_means() objects.\nWhen processing objects obtained using mice multiple imputation, the pooled model using mice::pool is attached to the model attribute of the output. This means that functions like modelsummary::modelsummary() will not erroneously report goodness-of-fit statistics from just a single model and will instead appropriately report the statistics for the pooled model. Thanks to @Tristan-Siegfried for PR #740.\nMore informative error messages on some prediction problems. Thanks to @andymilne for Report #751.\n\nPerformance:\n\ninferences() is now up to 17x faster and much more memory-efficient when method is \"boot\" or \"rsample\" (#770, #771, @etiennebacher).\n\nBugs:\n\nbrms models with nl=TRUE and a single predictor generated an error. Thanks to @Tristan-Siegried for Report #759.\navg_predictions(): Incorrect group-wise averaging when all predictors are categorical, the variables variable is used, and we are averaging with avg_ or the by argument. Thanks to BorgeJorge for report #766.\nBug when datagrid() when called inside a user-written function. Thanks to @NickCH-K for report #769 and to @capnrefsmmat for the diagnostics.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-7",
    "href": "articles/NEWS.html#section-7",
    "title": "35¬† News",
    "section": "0.11.1",
    "text": "0.11.1\nBreaking change:\n\nRow orders are now more consistent, but may have changed from previous version. This could affect results from hypothesis with b1, b2, ‚Ä¶ indexing.\n\nSupport new models:\n\nnlme::lme()\nphylolm::phylolm()\nphylolm::phyloglm()\n\nNew:\n\nVignette on 2x2 experimental designs. Thanks to Demetri Pananos.\ncomparisons() accepts data frames with two numeric columns (‚Äúlow‚Äù and ‚Äúhigh‚Äù) to specify fully customizable contrasts.\ndatagrid() gets a new by argument to create apply grid-making functions within groups.\nplot_*() gain a newdata argument for use with by.\n\nBug:\n\ncomparisons(comparison = \"lnratioavg\") ignored wts argument. Thanks to Demetri Pananos for report #737.\nordinal::clm(): incorrect standard errors when location and scale parameters are the same. Thanks to MrJerryTAO for report #718.\nIncorrect label for ‚Äú2sd‚Äù comparisons. Thanks to Andy Milne for report #720.\nInvalid factor levels in datagrid() means newdata argument gets ignored. Thanks to Josh Errickson for report #721.\nError in models with only categorical predictors and the by argument. Thanks to Sam Brilleman for report #723.\nElasticities are now supported for ordinal::clm() models. Thanks to MrJerryTAO for report #729.\nglmmTMB models with zero-inflated components are supported. Thanks to @Helsinki-Ronan and @strengejacke for report #734.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-8",
    "href": "articles/NEWS.html#section-8",
    "title": "35¬† News",
    "section": "0.11.0",
    "text": "0.11.0\nBreaking changes:\n\ntype column is replaced by type attribute.\npredictions() only works with officially supported model types (same list as comparisons() and slopes()).\n\nRenamed arguments (backward compatibility is preserved):\n\ntransform_pre -&gt; comparison\ntransform_post -&gt; transform\n\nNew:\n\np_adjust argument: Adjust p-values for multiple comparisons.\nequivalence argument available everywhere.\n\nPerformance:\n\nMuch faster results in avg_*() functions for models with only categorical predictors and many rows of data, using deduplication and weights instead of unit-level estimates.\nFaster predictions in lm() and glm() models using RcppEigen.\nBayesian models with many rows. Thanks to Etienne Bacher. #694\nFaster predictions, especially with standard errors and large datasets.\n\nBugs:\n\nMultiple imputation with mira objects was not pooling all datasets. Thanks to @Generalized for report #711.\nSupport for more models with offsets. Thanks to @mariofiorini for report #705.\nError on predictions() with by and wts. Thanks to Noah Greifer for report #695.\nafex: some models generated errors. Thanks to Daniel L√ºdecke for report #696.\ngroup column name is always forbidden. Thanks to Daniel L√ºdecke for report #697.\nBlank graphs in plot_comparisons() with a list in variables.\ntype=\"link\" produced an error with some categorical brms models. Thanks to @shirdekel for report #703.\nError on predictions(variables = ...) for glmmTMB models. Thanks to Daniel L√ºdecke for report #707.\nby with user-specified function in comparison and factor predictor did not aggregate correctly. Thanks to @joaotedde for report #715.\nordinal::clm: Support cum.prob and linear.predictor prediction types. Thanks to @MrJerryTAO for report #717.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-9",
    "href": "articles/NEWS.html#section-9",
    "title": "35¬† News",
    "section": "0.10.0",
    "text": "0.10.0\nPerformance:\n\n2-4x faster execution for many calls. Thanks to Etienne Bacher.\n\nNew models supported:\n\nMCMCglmm::MCMCglmm\nRchoice::hetprob\nRchoice::ivpml\nMultiple imputation using mice and any package which can return a list of imputed data frames (e.g., Amelia, missRanger, etc.)\n\nPlot improvements:\n\nNew by argument to display marginal estimates by subgroup.\nNew rug argument to display tick marks in the margins.\nNew points argument in plot_predictions() to display a scatter plot.\nNew gray argument to plot in grayscale using line types and shapes instead of color.\nThe effect argument is renamed to variables in plot_slopes() and plot_comparisons(). This improves consistency with the analogous slopes() and comparisons() functions.\nThe plotting vignette was re-written.\n\nOther:\n\nSupport multiple imputation with mice mira objects. The multiple imputation vignette was rewritten.\nThe variables_grid argument in marginal_means() is renamed newdata. Backward compatibility is maintained.\navg_*() returns an informative error when vcov is ‚Äúsatterthwaite‚Äù or ‚Äúkenward-roger‚Äù\n‚Äúsatterthwaite‚Äù and ‚Äúkenward-roger‚Äù are now supported when newdata is not NULL\nInformative error when hypothesis includes a b# larger than the available number of estimates.\navg_predictions(model, variables = \"x\") computes average counterfactual predictions by subgroups of x\ndatagrid() and plot_*() functions are faster in datasets with many extraneous columns.\nIn predictions(type = NULL) with glm() and Gam() we first make predictions on the link scale and then backtransform them. Setting type=\"response\" explicitly makes predictions directly on the response scale without backtransformation.\nStandard errors now supported for more glmmTMB models.\nUse the numDeriv package for numeric differentiation in the calculation of delta method standard error. A global option can now be passed to numDeriv::jacobian:\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = list(method = \"Richardson\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = NULL)\n\nPrint:\n\nPrint fewer significant digits.\nprint.marginaleffects now prints all columns supplied to newdata\nLess redundant labels when using hypothesis\n\nMany improvements to documentation.\n\nBugfixes:\n\nStandard errors could be inaccurate in models with non-linear components (and interactions) when some of the coefficients were very small. This was related to the step size used for numerical differentiation for the delta method. Issue #684.\navg_predictions(by =) did not work when the dataset included a column named term. Issue #683.\nbrms models with multivariate outcome collapsed categories in comparisons(). Issue #639.\nhypotheses() now works on lists and in calls to lapply(), purrr::map(), etc. Issue #660.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-10",
    "href": "articles/NEWS.html#section-10",
    "title": "35¬† News",
    "section": "0.9.0",
    "text": "0.9.0\nBreaking changes:\n\nAll functions return an estimate column instead of the function-specific predicted, comparisons, dydx, etc. This change only affects unit-level estimates, and not average estimates, which already used the estimate column name.\nThe transform_avg argument in tidy() deprecated. Use transform_post instead.\nplot_*(draw=FALSE) now return the actual variable names supplied to the condition argument, rather than the opaque ‚Äúcondition1‚Äù, ‚Äúcondition2‚Äù, etc.\n\nNew models supported:\n\nblme package.\n\nNew features:\n\nNew functions: avg_predictions(), avg_comparisons(), avg_slopes()\nEquivalence, non-inferiority, and non-superiority tests with the hypotheses() function and equivalence argument.\nNew experimental inferences() function: simulation-based inferences and bootstrap using the boot, rsample, and fwb package.\nNew df argument to set degrees of freedom manually for p and CI.\nPretty print() for all objects.\nby argument\n\nTRUE returns average (marginal) predictions, comparisons, or slopes.\nSupports bayesian models.\n\nhypothesis argument\n\nNumeric value sets the null used in calculating Z and p.\nExample: comparisons(mod, transform_pre = \"ratio\", hypothesis = 1)\n\nAll arguments from the main functions are now available through tidy(), and summary(): conf_level, transform_post, etc.\nBayesian posterior distribution summaries (median, mean, HDI, quantiles) can be customized using global options. See ?comparisons\n\nRenamed functions (backward-compatibility is maintained by keeping the old function names as aliases):\n\nmarginaleffects() -&gt; slopes()\nposteriordraws() -&gt; posterior_draws()\nmarginalmeans() -&gt; marginal_means()\nplot_cap() -&gt; plot_predictions()\nplot_cme() -&gt; plot_slopes()\nplot_cco() -&gt; plot_comparisons()\n\nBug fixes:\n\nIncorrect results: In 0.8.1, plot_*() the threenum and minmax labels did not correspond to the correct numeric values.\nFix corner case for slopes when the dataset includes infinite values.\nmlogit error with factors.\nThe vcov argument now accepts functions for most models.\n\nOther:\n\nRemoved major performance bottleneck for slopes()",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-11",
    "href": "articles/NEWS.html#section-11",
    "title": "35¬† News",
    "section": "0.8.1",
    "text": "0.8.1\n\ndeltamethod() can run hypothesis tests on objects produced by the comparisons(), marginaleffects(), predictions(), and marginalmeans() functions. This feature relies on match.call(), which means it may not always work when used programmatically, inside functions and nested environments. It is generally safer and more efficient to use the hypothesis argument.\nplot_cme() and plot_cco() accept lists with user-specified values for the regressors, and can display nice labels for shortcut string-functions like ‚Äúthreenum‚Äù or ‚Äúquartile‚Äù.\nposterior_draws: new shape argument to return MCMC draws in various formats, including the new rvar structure from the posterior package.\ntransform_avg function gets printed in summary() output.\ntransform_post and transform_avg support string shortcuts: ‚Äúexp‚Äù and ‚Äúln‚Äù\nAdded support for mlm models from lm(). Thanks to Noah Greifer.\n\nBug fixes:\n\nhypothesis argument with bayesian models and tidy() used to raise an error.\nMissing values for some regressors in the comparisons() output for brms models.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-12",
    "href": "articles/NEWS.html#section-12",
    "title": "35¬† News",
    "section": "0.8.0",
    "text": "0.8.0\nBreaking change:\n\nThe interaction argument is deprecated and replaced by the cross argument. This is to reduce ambiguity with respect to the interaction argument in emmeans, which does something completely different, akin to the difference-in-differences illustrated in the Interactions vignette.\n\n71 classes of models supported, including the new:\n\nrms::ols\nrms::lrm\nrms::orm\n\nNew features:\n\nPlots: plot_cme(), plot_cap(), and plot_cco() are now much more flexible in specifying the comparisons to display. The condition argument accepts lists, functions, and shortcuts for common reference values, such as ‚Äúminmax‚Äù, ‚Äúthreenum‚Äù, etc.\nvariables argument of the comparisons() function is more flexible:\n\nAccepts functions to specify custom differences in numeric variables (e.g., forward and backward differencing).\nCan specify pairs of factors to compare in the variables argument of the comparisons function.\n\nvariables argument of the predictions() function is more flexible:\n\nAccepts shortcut strings, functions, and vectors of arbitrary length.\n\nIntegrate out random effects in bayesian brms models (see Bayesian analysis vignette)\n\nNew vignettes:\n\nExperiments\nExtending marginal effects\nIntegrating out random effects in bayesian models\n\nBug fixes and minor improvements:\n\nThe default value of conf_level in summary() and tidy() is now NULL, which inherits the conf_level value in the original comparisons/marginaleffects/predictions calls.\nFix typo in function names for missing ‚Äúlnratioavgwts‚Äù\nInteractions with fixest::i() are parsed properly as categorical variables\nFor betareg objects, inference can now be done on all coefficients using deltamethod(). previously only the location coefficients were available.\nFor objects from crch package, a number of bugs have been fixed; standard errors should now be correct for deltamethod(), marginaleffects(), etc.\nFixed a bug in the tidy() function for glmmTMB models without random effects, which caused all t statistics to be identical.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-13",
    "href": "articles/NEWS.html#section-13",
    "title": "35¬† News",
    "section": "0.7.1",
    "text": "0.7.1\n\nNew supported model class: gamlss. Thanks to Marcio Augusto Diniz.\nmarginalmeans() accepts a wts argument with values: ‚Äúequal‚Äù, ‚Äúproportional‚Äù, ‚Äúcells‚Äù.\nby argument\n\naccepts data frames for complex groupings.\nin marginalmeans only accepts data frames.\naccepts ‚Äúgroup‚Äù to group by response level.\nworks with bayesian models.\n\nbyfun argument for the predictions() function to aggregate using different functions.\nhypothesis argument\n\nThe matrix column names are used as labels for hypothesis tests.\nBetter labels with ‚Äúsequential‚Äù, ‚Äúreference‚Äù, ‚Äúpairwise‚Äù.\nnew shortcuts ‚Äúrevpairwise‚Äù, ‚Äúrevsequential‚Äù, ‚Äúrevreference‚Äù\n\nwts argument is respected in by argument and with *avg shortcuts in the transform_pre argument.\ntidy.predictions() and tidy.marginalmeans() get a new transform_avg argument.\nNew vignettes:\n\nUnit-level contrasts in logistic regressions. Thanks to @arthur-albuquerque.\nPython Numpy models in marginaleffects. Thanks to @timpipeseek.\nBootstrap example in standard errors vignette.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-14",
    "href": "articles/NEWS.html#section-14",
    "title": "35¬† News",
    "section": "0.7.0",
    "text": "0.7.0\nBreaking changes:\n\nby is deprecated in summary() and tidy(). Use the same by argument in the main functions instead: comparisons(), marginaleffects(), predictions()\nCharacter vectors are no longer supported in the variables argument of the predictions() function. Use newdata=\"fivenum\" or ‚Äúgrid‚Äù, ‚Äúmean‚Äù, or ‚Äúmedian‚Äù instead.\n\nCritical bug fix:\n\nContrasts with interactions were incorrect in version 0.6.0. The error should have been obvious to most analysts in most cases (weird-looking alignment). Thanks to @vmikk.\n\nNew supported packages and models:\n\nsurvival::clogit\nbiglm: The main quantities can be computed, but not the delta method standard errors. See https://github.com/vincentarelbundock/marginaleffects/issues/387\n\nNew vignette:\n\nElasticity\nFrequently Asked Questions\n\nNew features:\n\nElasticity and semi-elasticity using the new slope argument in marginaleffects(): eyex, dyex, eydx\ndatagrid() accepts functions: datagrid(newdata = mtcars, hp = range, mpg = fivenum, wt = sd)\nNew datagridcf() function to create counterfactual datasets. This is a shortcut to the datagrid() function with default to grid_type = \"counterfactual\"\nNew by arguments in predictions(), comparisons(), marginaleffects()\nNew newdata shortcuts: ‚Äútukey‚Äù, ‚Äúgrid‚Äù\nNew string shortcuts for transform_pre in comparisons()\nmarginalmeans() now back transforms confidence intervals when possible.\nvcov argument string shortcuts are now case-insensitive\nThe default contrast in comparisons() for binary predictors is now a difference between 1 and 0, rather than +1 relative to baseline.\ndocumentation improvements",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-15",
    "href": "articles/NEWS.html#section-15",
    "title": "35¬† News",
    "section": "0.6.0",
    "text": "0.6.0\nNew supported packages and models:\n\ntidymodels objects of class tidy_model are supported if the fit engine is supported by marginaleffects.\n\nNew function:\n\ndeltamethod(): Hypothesis tests on functions of parameters\nplot_cco(): Plot conditional contrasts\n\nNew arguments:\n\nhypothesis for hypothesis tests and custom contrasts\ntransform_post in predictions()\nwts argument in predictions() only affects average predictions in tidy() or summary().\n\nNew or improved vignettes:\n\nHypothesis Tests and Custom Contrasts using the Delta Method: https://marginaleffects.com/articles/hypothesis.html\nMultiple Imputation: https://marginaleffects.com/articles/multiple_imputation.html\nCausal Inference with the g-Formula: https://marginaleffects.com/articles/gcomputation.html (Thanks to Rohan Kapre for the idea)\n\nDeprecated or renamed arguments:\n\ncontrast_factor and contrast_numeric arguments are deprecated in comparisons(). Use a named list in the variables argument instead. Backward compatibility is maintained.\nThe transform_post argument in tidy() and summary() is renamed to transform_avg to disambiguate against the argument of the same name in comparisons(). Backward compatibility is preserved.\n\nMisc:\n\ntidy.predictions() computes standard errors using the delta method for average predictions\nSupport gam models with matrix columns.\neps in marginaleffects() is now ‚Äúadaptive‚Äù by default: it equals 0.0001 multiplied the range of the predictor variable\ncomparisons() now supports ‚Äúlog of marginal odds ratio‚Äù in the transform_pre argument. Thanks to Noah Greifer.\nNew transform_pre shortcuts: dydx, expdydx\ntidy.predictions() computes standard errors and confidence intervals for linear models or GLM on the link scale.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-16",
    "href": "articles/NEWS.html#section-16",
    "title": "35¬† News",
    "section": "0.5.0",
    "text": "0.5.0\nBreaking changes:\n\ntype no longer accepts a character vector. Must be a single string.\nconf.int argument deprecated. Use vcov = FALSE instead.\n\nNew supported packages and models:\n\nmlogit\nmhurdle\ntobit1\nglmmTMB\n\nNew features:\n\ninteraction argument in comparisons() to compute interactions between contrasts (cross-contrasts).\nby argument in tidy() and summary() computes group-average marginal effects and comparisons.\ntransform_pre argument can define custom contrasts between adjusted predictions (e.g., log adjusted risk ratios). Available in comparisons().\ntransform_post argument allows back transformation before returning the final results. Available in comparisons(), marginalmeans(), summary(), tidy().\nThe variables argument of the comparisons() function accepts a named list to specify variable-specific contrast types.\nRobust standard errors with the vcov argument. This requires version 0.17.1 of the insight package.\n\nsandwich package shortcuts: vcov = \"HC3\", \"HC2\", \"NeweyWest\", and more.\nMixed effects models: vcov = \"satterthwaite\" or \"kenward-roger\"\nOne-sided formula to clusters: vcov = ~cluster_variable\nVariance-covariance matrix\nFunction which returns a named squared matrix\n\nmarginalmeans() allows interactions\nBayesian Model Averaging for brms models using type = \"average\". See vignette on the marginaleffects website.\neps argument for step size of numerical derivative\nmarginaleffects and comparisons now report confidence intervals by default.\nNew dependency on the data.table package yields substantial performance improvements.\nMore informative error messages and warnings\nBug fixes and performance improvements\n\nNew pages on the marginaleffects website: https://marginaleffects.com/\n\nAlternative software packages\nRobust standard errors (and more)\nPerformance tips\nTables and plots\nMultinomial Logit and Discrete Choice Models\nGeneralized Additive Models\nMixed effects models (Bayesian and Frequentist)\nTransformations and Custom Contrasts: Adjusted Risk Ratio Example\n\nArgument name changes (backward compatibility is preserved:\n\nEverywhere:\n\nconf.level -&gt; conf_level\n\ndatagrid():\n\nFUN.factor -&gt; FUN_factor (same for related arguments)\ngrid.type -&gt; grid_type",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-17",
    "href": "articles/NEWS.html#section-17",
    "title": "35¬† News",
    "section": "0.4.1",
    "text": "0.4.1\nNew supported packages and models:\n\nstats::loess\nsampleSelection::selection\nsampleSelection::heckit\n\nMisc:\n\nmgcv::bam models allow exclude argument.\nGam models allow include_smooth argument.\nNew tests\nBug fixes",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-18",
    "href": "articles/NEWS.html#section-18",
    "title": "35¬† News",
    "section": "0.4.0",
    "text": "0.4.0\nNew function:\n\ncomparisons() computes contrasts\n\nMisc:\n\nSpeed optimizations\npredictions() and plot_cap() include confidence intervals for linear models\nMore robust handling of in-formula functions: factor(), strata(), mo()\nDo not overwrite user‚Äôs ggplot2::theme_set() call",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-19",
    "href": "articles/NEWS.html#section-19",
    "title": "35¬† News",
    "section": "0.3.4",
    "text": "0.3.4\n\nBug fixes",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-20",
    "href": "articles/NEWS.html#section-20",
    "title": "35¬† News",
    "section": "0.3.3",
    "text": "0.3.3\nNew supported models:\n\nmclogit::mclogit\nrobust::lmRob\nrobustlmm::rlmer\nfixest confidence intervals in predictions\n\nMisc:\n\nSupport modelbased::visualisation_matrix in newdata without having to specify x explicitly.\ntidy.predictions() and summary.predictions() methods.\nDocumentation improvements.\nCRAN test fixes",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-21",
    "href": "articles/NEWS.html#section-21",
    "title": "35¬† News",
    "section": "0.3.2",
    "text": "0.3.2\nSupport for new models and packages:\n\nbrglm2::bracl\nmclogit::mblogit\nscam::scam\nlmerTest::lmer\n\nMisc:\n\nDrop numDeriv dependency, but make it available via a global option: options(‚Äúmarginaleffects_numDeriv‚Äù = list(method = ‚ÄúRichardson‚Äù, method.args = list(eps = 1e-5, d = 0.0001)))\nBugfixes\nDocumentation improvements\nCRAN tests",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-22",
    "href": "articles/NEWS.html#section-22",
    "title": "35¬† News",
    "section": "0.3.1",
    "text": "0.3.1\ndocumentation bugfix",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-23",
    "href": "articles/NEWS.html#section-23",
    "title": "35¬† News",
    "section": "0.3.0",
    "text": "0.3.0\nBreaking changes:\n\npredictions returns predictions for every observation in the original dataset instead of newdata=datagrid().\nmarginalmeans objects have new column names, as do the corresponding tidy and summary outputs.\n\nNew supported packages and models:\n\nbrms::brm\nrstanarm::stanglm\nbrglm2::brmultinom\nMASS::glmmPQL\naod::betabin\n\nMisc:\n\ndatagrid function supersedes typical and counterfactual with the grid.type argument. The typical and counterfactual functions will remain available and exported, but their use is not encouraged.\nposterior_draws function can be applied to a predictions or a marginaleffects object to extract draws from the posterior distribution.\nmarginalmeans standard errors are now computed using the delta method.\npredictions standard errors are now computed using the delta method when they are not available from insight::get_predicted.\nNew vignette on Bayesian models with brms\nNew vignette on Mixed effects models with lme4\nIf the data.table package is installed, marginaleffects will automatically use it to speed things up.\nContrast definition reported in a separate column of marginaleffects output.\nSafer handling of the type argument.\nComprehensive list of supported and tests models on the website.\nMany bug fixes\nMany new tests, including several against emmeans",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-24",
    "href": "articles/NEWS.html#section-24",
    "title": "35¬† News",
    "section": "0.2.0",
    "text": "0.2.0\nBreaking change:\n\ndata argument becomes newdata in all functions.\n\nNew supported packages and models:\n\nlme4:glmer.nb\nmgcv::gam\nordinal::clm\nmgcv\n\nmarginalmeans:\n\nNew variables_grid argument\n\npredictions:\n\nSupport mgcv\n\nplot_cap\n\nNew type argument\n\nMisc:\n\nNew validity checks and tests",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/NEWS.html#section-25",
    "href": "articles/NEWS.html#section-25",
    "title": "35¬† News",
    "section": "0.1.0",
    "text": "0.1.0\nFirst release. Bravo!\nThanks to Marco Avina Mendoza, Resul Umit, and all those who offered comments and suggestions.",
    "crumbs": [
      "Misc",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>News</span>"
    ]
  },
  {
    "objectID": "articles/functions.html",
    "href": "articles/functions.html",
    "title": "Functions",
    "section": "",
    "text": "Goal\nFunction\n\n\n\nPredictions\npredictions()\n\n\n\navg_predictions()\n\n\n\nplot_predictions()\n\n\nComparisons: Difference, Ratio, Odds, Lift, etc.\ncomparisons()\n\n\n\navg_comparisons()\n\n\n\nplot_comparisons()\n\n\nSlopes\nslopes()\n\n\n\navg_slopes()\n\n\n\nplot_slopes()\n\n\nMarginal Means\nmarginal_means()\n\n\nGrids\ndatagrid()\n\n\n\ndatagridcf()\n\n\nHypothesis & Equivalence\nhypotheses()\n\n\nBayes, Bootstrap, Simulation\nposterior_draws()\n\n\n\ninferences()\n\n\n\n\n\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2022. ‚ÄúA Gentle\nIntroduction to Conformal Prediction and Distribution-Free Uncertainty\nQuantification,‚Äù no. arXiv:2107.07511 (September). https://doi.org/10.48550/arXiv.2107.07511.\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2021.\n‚ÄúSurprise!‚Äù American Journal of Epidemiology 190\n(2): 191‚Äì93.\n\n\nDing, Tiffany, Anastasios N. Angelopoulos, Stephen Bates, Michael I.\nJordan, and Ryan J. Tibshirani. 2023. ‚ÄúClass-Conditional Conformal\nPrediction with Many Classes,‚Äù no. arXiv:2306.09335 (June). https://doi.org/10.48550/arXiv.2306.09335.\n\n\nRafi, Zad, and Sander Greenland. 2020. ‚ÄúSemantic and Cognitive\nTools to Aid Statistical Science: Replace Confidence and Significance by\nCompatibility and Surprise.‚Äù BMC Medical Research\nMethodology 20: 1‚Äì13.\n\n\nRothman, Kenneth J. 2021. ‚ÄúRothman Responds to\n‚ÄòSurprise!‚Äô‚Äù American Journal of\nEpidemiology 190 (2): 194‚Äì95.",
    "crumbs": [
      "Functions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#description",
    "href": "articles/reference/predictions.html#description",
    "title": "predictions",
    "section": "Description",
    "text": "Description\n\nOutcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \"reference grid\").\n\n\n\n\npredictions(): unit-level (conditional) estimates.\n\n\n\n\navg_predictions(): average (marginal) estimates.\n\n\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the predictions vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://marginaleffects.com/articles/predictions.html\n\n\n\n\nhttps://marginaleffects.com/",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#usage",
    "href": "articles/reference/predictions.html#usage",
    "title": "predictions",
    "section": "Usage",
    "text": "Usage\npredictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = FALSE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = TRUE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#arguments",
    "href": "articles/reference/predictions.html#arguments",
    "title": "predictions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nnewdata\n\n\n\nGrid of predictor values at which we evaluate predictions.\n\n\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\n\n\nNULL (default): Unit-level predictions for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Predictions at the Mean. Predictions when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Predictions at the Median. Predictions when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Predictions at Marginal Means. See Details section below.\n\n\n\n\n\"tukey\": Predictions at Tukey‚Äôs 5 numbers.\n\n\n\n\n\"grid\": Predictions on a grid of representative numbers (Tukey‚Äôs 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\n\n\n\nvariables\n\n\n\nCounterfactual variables.\n\n\n\n\nOutput:\n\n\n\n\npredictions(): The entire dataset is replicated once for each unique combination of variables, and predictions are made.\n\n\n\n\navg_predictions(): The entire dataset is replicated, predictions are made, and they are marginalized by variables categories.\n\n\n\n\nWarning: This can be expensive in large datasets.\n\n\n\n\nWarning: Users who need \"conditional\" predictions should use the newdata argument instead of variables.\n\n\n\n\n\n\nInput:\n\n\n\n\nNULL: computes one prediction per row of newdata\n\n\n\n\nCharacter vector: the dataset is replicated once of every combination of unique values of the variables identified in variables.\n\n\n\n\nNamed list: names identify the subset of variables of interest and their values. For numeric variables, the variables argument supports functions and string shortcuts:\n\n\n\n\nA function which returns a numeric value\n\n\n\n\nNumeric vector: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\n\"threenum\": mean and 1 standard deviation on both sides\n\n\n\n\n\"fivenum\": Tukey‚Äôs five numbers\n\n\n\n\n\n\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nby\n\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function‚Äôs documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\n\nbyfun\n\n\n\nA function such as mean() or sum() used to aggregate estimates within the subgroups defined by the by argument. NULL uses the mean() function. Must accept a numeric vector and return a single numeric value. This is sometimes used to take the sum or mean of predicted probabilities across outcome or predictor levels. See examples section.\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\ntransform\n\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\n\nhypothesis\n\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The ‚Å†b*‚Å† wildcard can be used to test hypotheses on all estimates. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n‚Å†b* / b1 = 1‚Å†\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\n\nequivalence\n\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\n\np_adjust\n\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\n\ndf\n\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n\nnumderiv\n\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\n\n\"richardson\": Richardson extrapolation method\n\n\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(‚Äúfdcenter‚Äù, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#value",
    "href": "articles/reference/predictions.html#value",
    "title": "predictions",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nestimate: predicted outcome\n\n\n\n\nstd.error: standard errors computed using the delta method.\n\n\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst‚Äôs intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.¬†(2020).\n\n\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nSee ?print.marginaleffects for printing options.",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#functions",
    "href": "articles/reference/predictions.html#functions",
    "title": "predictions",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_predictions(): Average predictions",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#standard-errors-using-the-delta-method",
    "href": "articles/reference/predictions.html#standard-errors-using-the-delta-method",
    "title": "predictions",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚Äúsimple‚Äù, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚ÄúRichardson‚Äù, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://marginaleffects.com/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://marginaleffects.com/articles/bootstrap.html",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#model-specific-arguments",
    "href": "articles/reference/predictions.html#model-specific-arguments",
    "title": "predictions",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#bayesian-posterior-summaries",
    "href": "articles/reference/predictions.html#bayesian-posterior-summaries",
    "title": "predictions",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúeti‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúhdi‚Äù)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmean‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmedian‚Äù)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the ‚Äúmarginaleffects_posterior_center‚Äù option (the median by default).",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#equivalence-inferiority-superiority",
    "href": "articles/reference/predictions.html#equivalence-inferiority-superiority",
    "title": "predictions",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#prediction-types",
    "href": "articles/reference/predictions.html#prediction-types",
    "title": "predictions",
    "section": "Prediction types",
    "text": "Prediction types\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\n\n\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=‚Äúinvlink(link)‚Äù will not always be equivalent to the average of estimates with type=‚Äúresponse‚Äù.\n\n\nSome of the most common type values are:\n\n\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#references",
    "href": "articles/reference/predictions.html#references",
    "title": "predictions",
    "section": "References",
    "text": "References\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106‚Äì114.\n\n\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191‚Äì93. https://doi.org/10.1093/aje/kwaa136",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/predictions.html#examples",
    "href": "articles/reference/predictions.html#examples",
    "title": "predictions",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n# Adjusted Prediction for every row of the original dataset\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\npred &lt;- predictions(mod)\nhead(pred)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n     20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n     26.4      0.962 27.5   &lt;0.001 549.0  24.5   28.3\n     20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n     15.9      0.992 16.0   &lt;0.001 190.0  14.0   17.9\n     20.2      1.219 16.5   &lt;0.001 201.8  17.8   22.5\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \nType:  response \n\n# Adjusted Predictions at User-Specified Values of the Regressors\npredictions(mod, newdata = datagrid(hp = c(100, 120), cyl = 4))\n\n\n  hp cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 100   4     26.2      0.986 26.6   &lt;0.001 516.6  24.3   28.2\n 120   4     25.8      1.110 23.2   &lt;0.001 393.8  23.6   27.9\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \nType:  response \n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp drat cyl am\n     22.0       1.29 17.0   &lt;0.001 214.0  19.4   24.5 123  3.7   6  1\n     18.2       1.27 14.3   &lt;0.001 151.9  15.7   20.7 123  3.7   6  0\n     25.5       1.32 19.3   &lt;0.001 274.0  23.0   28.1 123  3.7   4  1\n     21.8       1.54 14.1   &lt;0.001 148.3  18.8   24.8 123  3.7   4  0\n     22.6       2.14 10.6   &lt;0.001  84.2  18.4   26.8 123  3.7   8  1\n     18.9       1.73 10.9   &lt;0.001  89.0  15.5   22.3 123  3.7   8  0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \nType:  response \n\n# Average Adjusted Predictions (AAP)\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp * am * vs, mtcars)\n\navg_predictions(mod)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n     20.1      0.484 41.5   &lt;0.001 Inf  19.1     21\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\npredictions(mod, by = \"am\")\n\n\n am Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0     17.1      0.629 27.3   &lt;0.001 541.7  15.9   18.4\n  1     24.4      0.760 32.1   &lt;0.001 748.3  22.9   25.9\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Conditional Adjusted Predictions\nplot_predictions(mod, condition = \"hp\")\n\n\n\n# Counterfactual predictions with the `variables` argument\n# the `mtcars` dataset has 32 rows\n\nmod &lt;- lm(mpg ~ hp + am, data = mtcars)\np &lt;- predictions(mod)\nhead(p)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     25.4      0.818 31.0   &lt;0.001 700.5  23.8   27.0\n     25.4      0.818 31.0   &lt;0.001 700.5  23.8   27.0\n     26.4      0.850 31.1   &lt;0.001 701.1  24.7   28.1\n     20.1      0.775 25.9   &lt;0.001 490.0  18.6   21.6\n     16.3      0.677 24.0   &lt;0.001 421.6  15.0   17.6\n     20.4      0.796 25.6   &lt;0.001 478.6  18.8   22.0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am \nType:  response \n\nnrow(p)\n\n[1] 32\n\n# average counterfactual predictions\navg_predictions(mod, variables = \"am\")\n\n\n am Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0     17.9      0.676 26.6   &lt;0.001 513.7  16.6   19.3\n  1     23.2      0.822 28.3   &lt;0.001 581.2  21.6   24.8\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# counterfactual predictions obtained by replicating the entire for different\n# values of the predictors\np &lt;- predictions(mod, variables = list(hp = c(90, 110)))\nnrow(p)\n\n[1] 64\n\n# hypothesis test: is the prediction in the 1st row equal to the prediction in the 2nd row\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b1=b2     4.78      0.797 6   &lt;0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using row indices\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b1-b2=0     4.78      0.797 6   &lt;0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using numeric vector of weights\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error z Pr(&gt;|z|)    S 2.5 % 97.5 %\n custom     4.78      0.797 6   &lt;0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = lc)\n\n\n   Term wt Estimate Std. Error    z Pr(&gt;|z|)     S  2.5 % 97.5 % drat\n custom  2     4.78      0.797  6.0   &lt;0.001  28.9   3.22   6.35  3.6\n custom  3   115.21      3.647 31.6   &lt;0.001 725.0 108.07 122.36  3.6\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, drat, wt \nType:  response \n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\npredictions(mod, by = c(\"am\", \"vs\"))\n\n\n am vs Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0  0     15.0      0.791 19.0   &lt;0.001 265.7  13.5   16.6\n  0  1     20.7      1.036 20.0   &lt;0.001 294.0  18.7   22.8\n  1  0     19.7      1.119 17.7   &lt;0.001 229.3  17.6   21.9\n  1  1     28.4      1.036 27.4   &lt;0.001 546.3  26.3   30.4\n\nColumns: am, vs, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n# first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n\n\n Group Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n     3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n     3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n     3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n     3 4.04e-01   1.97e-01   2.0567   0.0397 4.7  1.90e-02 7.90e-01\n     3 1.00e+00   1.25e-03 802.4784   &lt;0.001 Inf  9.98e-01 1.00e+00\n     3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \nType:  probs \n\n# average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n\n\n Group Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n     3    0.469     0.0404 11.60   &lt;0.001 100.9 0.3895  0.548\n     4    0.375     0.0614  6.11   &lt;0.001  29.9 0.2546  0.495\n     5    0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n\nColumns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n\n\n  By Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n 3,4    0.422     0.0231 18.25   &lt;0.001 244.7 0.3766  0.467\n 5      0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \nType:  probs \n\n# sum of predicted probabilities for combined response levels\nmod &lt;- multinom(factor(cyl) ~ mpg + am, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    by = c(\"4,6\", \"4,6\", \"8\"),\n    group = as.character(c(4, 6, 8)))\npredictions(mod, newdata = \"mean\", byfun = sum, by = by)\n\n\n  By Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n 4,6   0.9158      0.121 7.549   &lt;0.001 44.4  0.678  1.154\n 8     0.0842      0.121 0.694    0.488  1.0 -0.154  0.322\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \nType:  probs",
    "crumbs": [
      "Functions",
      "predictions"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#description",
    "href": "articles/reference/comparisons.html#description",
    "title": "comparisons",
    "section": "Description",
    "text": "Description\n\nPredict the outcome variable at different regressor values (e.g., college graduates vs.¬†others), and compare those predictions by computing a difference, ratio, or some other function. comparisons() can return many quantities of interest, such as contrasts, differences, risk ratios, changes in log odds, lift, slopes, elasticities, etc.\n\n\n\n\ncomparisons(): unit-level (conditional) estimates.\n\n\n\n\navg_comparisons(): average (marginal) estimates.\n\n\n\n\nvariables identifies the focal regressors whose \"effect\" we are interested in. comparison determines how predictions with different regressor values are compared (difference, ratio, odds, etc.). The newdata argument and the datagrid() function control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the comparisons vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://marginaleffects.com/articles/comparisons.html\n\n\n\n\nhttps://marginaleffects.com/",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#usage",
    "href": "articles/reference/comparisons.html#usage",
    "title": "comparisons",
    "section": "Usage",
    "text": "Usage\ncomparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  comparison = \"difference\",\n  type = NULL,\n  vcov = TRUE,\n  by = FALSE,\n  conf_level = 0.95,\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  vcov = TRUE,\n  by = TRUE,\n  conf_level = 0.95,\n  comparison = \"difference\",\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#arguments",
    "href": "articles/reference/comparisons.html#arguments",
    "title": "comparisons",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nnewdata\n\n\n\nGrid of predictor values at which we evaluate the comparisons.\n\n\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\n\n\nNULL (default): Unit-level contrasts for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\n\n\ndata frame: Unit-level contrasts for each row of the newdata data frame.\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Contrasts at the Mean. Contrasts when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Contrasts at the Median. Contrasts when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Contrasts at Marginal Means.\n\n\n\n\n\"tukey\": Contrasts at Tukey‚Äôs 5 numbers.\n\n\n\n\n\"grid\": Contrasts on a grid of representative numbers (Tukey‚Äôs 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nnewdata = datagrid(mpg = fivenum): mpg variable held at Tukey‚Äôs five numbers (using the fivenum function), and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid documentation.\n\n\n\n\n\n\n\n\n\nvariables\n\n\n\nFocal variables\n\n\n\n\nNULL: compute comparisons for all the variables in the model object (can be slow).\n\n\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\nNamed list: names identify the subset of variables of interest, and values define the type of contrast to compute. Acceptable values depend on the variable type:\n\n\n\n\nFactor or character variables:\n\n\n\n\n\"reference\": Each factor level is compared to the factor reference (base) level\n\n\n\n\n\"all\": All combinations of observed levels\n\n\n\n\n\"sequential\": Each factor level is compared to the previous factor level\n\n\n\n\n\"pairwise\": Each factor level is compared to all other levels\n\n\n\n\n\"minmax\": The highest and lowest levels of a factor.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses.\n\n\n\n\nVector of length 2 with the two values to compare.\n\n\n\n\n\n\nLogical variables:\n\n\n\n\nNULL: contrast between TRUE and FALSE\n\n\n\n\n\n\nNumeric variables:\n\n\n\n\nNumeric of length 1: Contrast for a gap of x, computed at the observed value plus and minus x / 2. For example, estimating a +1 contrast compares adjusted predictions when the regressor is equal to its observed value minus 0.5 and its observed value plus 0.5.\n\n\n\n\nNumeric of length equal to the number of rows in newdata: Same as above, but the contrast can be customized for each row of newdata.\n\n\n\n\nNumeric vector of length 2: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\n\nData frame with the same number of rows as newdata, with two columns of \"low\" and \"high\" values to compare.\n\n\n\n\nFunction which accepts a numeric vector and returns a data frame with two columns of \"low\" and \"high\" values to compare. See examples below.\n\n\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\n\n\nExamples:\n\n\n\n\nvariables = list(gear = ‚Äúpairwise‚Äù, hp = 10)\n\n\n\n\nvariables = list(gear = ‚Äúsequential‚Äù, hp = c(100, 120))\n\n\n\n\nSee the Examples section below for more.\n\n\n\n\n\n\n\n\n\n\n\ncomparison\n\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\n\n\nstring: shortcuts to common contrast functions.\n\n\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nby\n\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function‚Äôs documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\ntransform\n\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\n\ncross\n\n\n\n\n\nFALSE: Contrasts represent the change in adjusted predictions when one predictor changes and all other variables are held constant.\n\n\n\n\nTRUE: Contrasts represent the changes in adjusted predictions when all the predictors specified in the variables argument are manipulated simultaneously (a \"cross-contrast\").\n\n\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\nhypothesis\n\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The ‚Å†b*‚Å† wildcard can be used to test hypotheses on all estimates. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n‚Å†b* / b1 = 1‚Å†\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\n\nequivalence\n\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\n\np_adjust\n\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\n\ndf\n\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n\neps\n\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\n\nnumderiv\n\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\n\n\"richardson\": Richardson extrapolation method\n\n\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(‚Äúfdcenter‚Äù, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#value",
    "href": "articles/reference/comparisons.html#value",
    "title": "comparisons",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nterm: the variable whose marginal effect is computed\n\n\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\n\n\nstd.error: standard errors computed by via the delta method.\n\n\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument.\n\n\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst‚Äôs intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.¬†(2020).\n\n\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nSee ?print.marginaleffects for printing options.",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#functions",
    "href": "articles/reference/comparisons.html#functions",
    "title": "comparisons",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_comparisons(): Average comparisons",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#standard-errors-using-the-delta-method",
    "href": "articles/reference/comparisons.html#standard-errors-using-the-delta-method",
    "title": "comparisons",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚Äúsimple‚Äù, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚ÄúRichardson‚Äù, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://marginaleffects.com/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://marginaleffects.com/articles/bootstrap.html",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#model-specific-arguments",
    "href": "articles/reference/comparisons.html#model-specific-arguments",
    "title": "comparisons",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#comparison-argument-functions",
    "href": "articles/reference/comparisons.html#comparison-argument-functions",
    "title": "comparisons",
    "section": "comparison argument functions",
    "text": "comparison argument functions\n\nThe following transformations can be applied by supplying one of the shortcut strings to the comparison argument. hi is a vector of adjusted predictions for the \"high\" side of the contrast. lo is a vector of adjusted predictions for the \"low\" side of the contrast. y is a vector of adjusted predictions for the original data. x is the predictor in the original data. eps is the step size to use to compute derivatives and elasticities.\n\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\ndifference\n\n\n(hi, lo) hi - lo\n\n\n\n\ndifferenceavg\n\n\n(hi, lo) mean(hi) - mean(lo)\n\n\n\n\ndydx\n\n\n(hi, lo, eps) (hi - lo)/eps\n\n\n\n\neyex\n\n\n(hi, lo, eps, y, x) (hi - lo)/eps * (x/y)\n\n\n\n\neydx\n\n\n(hi, lo, eps, y, x) ((hi - lo)/eps)/y\n\n\n\n\ndyex\n\n\n(hi, lo, eps, x) ((hi - lo)/eps) * x\n\n\n\n\ndydxavg\n\n\n(hi, lo, eps) mean((hi - lo)/eps)\n\n\n\n\neyexavg\n\n\n(hi, lo, eps, y, x) mean((hi - lo)/eps * (x/y))\n\n\n\n\neydxavg\n\n\n(hi, lo, eps, y, x) mean(((hi - lo)/eps)/y)\n\n\n\n\ndyexavg\n\n\n(hi, lo, eps, x) mean(((hi - lo)/eps) * x)\n\n\n\n\nratio\n\n\n(hi, lo) hi/lo\n\n\n\n\nratioavg\n\n\n(hi, lo) mean(hi)/mean(lo)\n\n\n\n\nlnratio\n\n\n(hi, lo) log(hi/lo)\n\n\n\n\nlnratioavg\n\n\n(hi, lo) log(mean(hi)/mean(lo))\n\n\n\n\nlnor\n\n\n(hi, lo) log((hi/(1 - hi))/(lo/(1 - lo)))\n\n\n\n\nlnoravg\n\n\n(hi, lo) log((mean(hi)/(1 - mean(hi)))/(mean(lo)/(1 - mean(lo))))\n\n\n\n\nlift\n\n\n(hi, lo) (hi - lo)/lo\n\n\n\n\nliftavg\n\n\n(hi, lo) (mean(hi) - mean(lo))/mean(lo)\n\n\n\n\nexpdydx\n\n\n(hi, lo, eps) ((exp(hi) - exp(lo))/exp(eps))/eps\n\n\n\n\nexpdydxavg\n\n\n(hi, lo, eps) mean(((exp(hi) - exp(lo))/exp(eps))/eps)",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#bayesian-posterior-summaries",
    "href": "articles/reference/comparisons.html#bayesian-posterior-summaries",
    "title": "comparisons",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúeti‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúhdi‚Äù)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmean‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmedian‚Äù)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the ‚Äúmarginaleffects_posterior_center‚Äù option (the median by default).",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#equivalence-inferiority-superiority",
    "href": "articles/reference/comparisons.html#equivalence-inferiority-superiority",
    "title": "comparisons",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#prediction-types",
    "href": "articles/reference/comparisons.html#prediction-types",
    "title": "comparisons",
    "section": "Prediction types",
    "text": "Prediction types\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\n\n\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=‚Äúinvlink(link)‚Äù will not always be equivalent to the average of estimates with type=‚Äúresponse‚Äù.\n\n\nSome of the most common type values are:\n\n\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#references",
    "href": "articles/reference/comparisons.html#references",
    "title": "comparisons",
    "section": "References",
    "text": "References\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106‚Äì114.\n\n\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191‚Äì93. https://doi.org/10.1093/aje/kwaa136",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/comparisons.html#examples",
    "href": "articles/reference/comparisons.html#examples",
    "title": "comparisons",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# Linear model\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01   &lt;0.001 14.0  -9.17  -3.15\n  cyl    8 - 4   -10.07       1.45 -6.93   &lt;0.001 37.8 -12.91  -7.22\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0 -9.17  -3.15\n  cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0  -9.17  -3.15\n  cyl    8 - 4   -10.07       1.45 -6.93  &lt; 0.001 37.8 -12.91  -7.22\n  cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# GLM with different scale types\nmod &lt;- glm(am ~ factor(gear), data = mtcars)\navg_comparisons(mod, type = \"response\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, type = \"link\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  link \n\n# Contrasts at the mean\ncomparisons(mod, newdata = \"mean\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % gear\n gear    4 - 3    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897    3\n gear    5 - 3    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307    3\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, gear \nType:  response \n\n# Contrasts between marginal means\ncomparisons(mod, newdata = \"marginalmeans\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n# Contrasts at user-specified values\ncomparisons(mod, newdata = datagrid(am = 0, gear = tmp$gear))\n\n\n Term Contrast gear Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n gear    4 - 3    3    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    4 - 3    4    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    4 - 3    5    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    5 - 3    3    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n gear    5 - 3    4    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n gear    5 - 3    5    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, gear, predicted_lo, predicted_hi, predicted \nType:  response \n\ncomparisons(mod, newdata = datagrid(am = unique, gear = max))\n\n\n Term Contrast gear Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n gear    4 - 3    5    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    4 - 3    5    0.667      0.117 5.68   &lt;0.001 26.1 0.436  0.897\n gear    5 - 3    5    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n gear    5 - 3    5    1.000      0.157 6.39   &lt;0.001 32.5 0.693  1.307\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, gear, predicted_lo, predicted_hi, predicted \nType:  response \n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\ncomparisons(m, variables = \"hp\", newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %  hp drat\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016 123  3.7\n cyl am\n   6  1\n   6  0\n   4  1\n   4  0\n   8  1\n   8  0\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, drat, cyl, am \nType:  response \n\n# Numeric contrasts\nmod &lt;- lm(mpg ~ hp, data = mtcars)\navg_comparisons(mod, variables = list(hp = 1))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp       +1  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(hp = 5))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n   hp       +5   -0.341     0.0506 -6.74   &lt;0.001 35.9 -0.44 -0.242\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(hp = c(90, 100)))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n   hp 100 - 90   -0.682      0.101 -6.74   &lt;0.001 35.9 -0.881 -0.484\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n   hp  Q3 - Q1     -5.7      0.845 -6.74   &lt;0.001 35.9 -7.35  -4.04\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n\n\n Term                Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 %\n   hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   &lt;0.001 35.9 -6.04\n 97.5 %\n  -3.32\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n\n\n Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n   hp Max - Min    -19.3       2.86 -6.74   &lt;0.001 35.9 -24.9  -13.7\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# using a function to specify a custom difference in one regressor\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (dat$hp - min(dat$hp)) / (max(dat$hp) - min(dat$hp)) + 1\nmodlog &lt;- lm(mpg ~ log(new_hp) + factor(cyl), data = dat)\nfdiff &lt;- \\(x) data.frame(x, x + 10)\navg_comparisons(modlog, variables = list(new_hp = fdiff))\n\n\n   Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n new_hp   custom    -1.97      0.711 -2.78  0.00547 7.5 -3.37 -0.581\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Adjusted Risk Ratio: see the contrasts vignette\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\navg_comparisons(mod, comparison = \"lnratioavg\", transform = exp)\n\n\n Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n  mpg mean(+1)     1.14   &lt;0.001 31.9  1.09   1.18\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n# Adjusted Risk Ratio: Manual specification of the `comparison`\navg_comparisons(\n     mod,\n     comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n     transform = exp)\n\n\n Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n  mpg       +1     1.14   &lt;0.001 31.9  1.09   1.18\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n# cross contrasts\nmod &lt;- lm(mpg ~ factor(cyl) * factor(gear) + hp, data = mtcars)\navg_comparisons(mod, variables = c(\"cyl\", \"gear\"), cross = TRUE)\n\n\n Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 % C: cyl C: gear\n   -0.631       3.40 -0.185    0.853 0.2 -7.30   6.04  6 - 4   4 - 3\n    2.678       4.62  0.580    0.562 0.8 -6.37  11.73  6 - 4   5 - 3\n    3.348       6.43  0.521    0.602 0.7 -9.25  15.95  8 - 4   4 - 3\n    5.525       5.87  0.942    0.346 1.5 -5.98  17.03  8 - 4   5 - 3\n\nColumns: term, contrast_cyl, contrast_gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# variable-specific contrasts\navg_comparisons(mod, variables = list(gear = \"sequential\", hp = 10))\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n gear    4 - 3    3.409      2.587  1.318   0.1876 2.4 -1.66  8.481\n gear    5 - 4    2.628      2.747  0.957   0.3387 1.6 -2.76  8.011\n hp      +10     -0.574      0.225 -2.552   0.0107 6.5 -1.02 -0.133\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n\n    Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n wt=drat    -6.23       1.05 -5.92   &lt;0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using row indices\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b1-b2=0     6.23       1.05 5.92   &lt;0.001 28.2  4.16   8.29\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using numeric vector of weights\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n custom     6.23       1.05 5.92   &lt;0.001 28.2  4.16   8.29\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n\n   Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n custom     6.23       1.05  5.92   &lt;0.001 28.2   4.16   8.29\n custom   -11.46       4.92 -2.33   0.0197  5.7 -21.10  -1.83\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Effect of a 1 group-wise standard deviation change\n# First we calculate the SD in each group of `cyl`\n# Second, we use that SD as the treatment size in the `variables` argument\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\ntmp &lt;- mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    mutate(hp_sd = sd(hp))\navg_comparisons(mod, variables = list(hp = tmp$hp_sd), by = \"cyl\")\n\n\n Term Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n   hp   custom   4   -0.678      0.435 -1.56    0.119 3.1 -1.53  0.174\n   hp   custom   6   -1.122      0.719 -1.56    0.119 3.1 -2.53  0.288\n   hp   custom   8   -0.818      0.525 -1.56    0.119 3.1 -1.85  0.210\n\nColumns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\ncomparisons(mod, by = TRUE)\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)    S  2.5 %  97.5 %\n   am    1 - 0   4.6980     1.0601  4.432   &lt;0.001 16.7  2.620  6.7758\n   hp    +1     -0.0688     0.0182 -3.780   &lt;0.001 12.6 -0.104 -0.0331\n   vs    1 - 0  -0.2943     2.3379 -0.126      0.9  0.2 -4.877  4.2879\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\navg_comparisons(mod, variables = \"hp\", by = c(\"vs\", \"am\"))\n\n\n Term Contrast vs am Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 %   97.5 %\n   hp mean(+1)  0  0  -0.0422     0.0248 -1.70  0.08879 3.5 -0.0907  0.00639\n   hp mean(+1)  0  1  -0.0368     0.0124 -2.97  0.00297 8.4 -0.0612 -0.01254\n   hp mean(+1)  1  0  -0.0994     0.0534 -1.86  0.06289 4.0 -0.2042  0.00534\n   hp mean(+1)  1  1  -0.1112     0.0463 -2.40  0.01645 5.9 -0.2020 -0.02034\n\nColumns: term, contrast, vs, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\nlibrary(nnet)\nmod &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\ncomparisons(mod, type = \"probs\", by = by)\n\n\n Term  By  Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 %  97.5 %\n  am  3,4 -0.222793    0.07959 -2.80  0.00512 7.6 -0.3788 -0.0668\n  mpg 3,4  0.000463    0.00579  0.08  0.93624 0.1 -0.0109  0.0118\n  vs  3,4  0.102102    0.07335  1.39  0.16394 2.6 -0.0417  0.2459\n  am  5    0.445585    0.15918  2.80  0.00512 7.6  0.1336  0.7576\n  mpg 5   -0.000927    0.01159 -0.08  0.93624 0.1 -0.0236  0.0218\n  vs  5   -0.204204    0.14671 -1.39  0.16394 2.6 -0.4917  0.0833\n\nColumns: term, by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs",
    "crumbs": [
      "Functions",
      "comparisons"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#description",
    "href": "articles/reference/slopes.html#description",
    "title": "slopes",
    "section": "Description",
    "text": "Description\n\nPartial derivative of the regression equation with respect to a regressor of interest.\n\n\n\n\nslopes(): unit-level (conditional) estimates.\n\n\n\n\navg_slopes(): average (marginal) estimates.\n\n\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the slopes vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://marginaleffects.com/articles/slopes.html\n\n\n\n\nhttps://marginaleffects.com/",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#usage",
    "href": "articles/reference/slopes.html#usage",
    "title": "slopes",
    "section": "Usage",
    "text": "Usage\nslopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = FALSE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = TRUE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#arguments",
    "href": "articles/reference/slopes.html#arguments",
    "title": "slopes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nnewdata\n\n\n\nGrid of predictor values at which we evaluate the slopes.\n\n\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\n\n\nNULL (default): Unit-level slopes for each observed value in the dataset (empirical distribution). The dataset is retrieved using insight::get_data(), which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Marginal Effects at the Mean. Slopes when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Marginal Effects at the Median. Slopes when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Marginal Effects at Marginal Means. See Details section below.\n\n\n\n\n\"tukey\": Marginal Effects at Tukey‚Äôs 5 numbers.\n\n\n\n\n\"grid\": Marginal Effects on a grid of representative numbers (Tukey‚Äôs 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\n\n\n\nvariables\n\n\n\nFocal variables\n\n\n\n\nNULL: compute slopes or comparisons for all the variables in the model object (can be slow).\n\n\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nby\n\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function‚Äôs documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\nslope\n\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\n\n\"dydx\": dY/dX\n\n\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\n\n\"eydx\": dY/dX * Y\n\n\n\n\n\"dyex\": dY/dX / X\n\n\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\nhypothesis\n\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The ‚Å†b*‚Å† wildcard can be used to test hypotheses on all estimates. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n‚Å†b* / b1 = 1‚Å†\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\n\nequivalence\n\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\n\np_adjust\n\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\n\ndf\n\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n\neps\n\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\n\nnumderiv\n\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\n\n\"richardson\": Richardson extrapolation method\n\n\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(‚Äúfdcenter‚Äù, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#details",
    "href": "articles/reference/slopes.html#details",
    "title": "slopes",
    "section": "Details",
    "text": "Details\n\nA \"slope\" or \"marginal effect\" is the partial derivative of the regression equation with respect to a variable in the model. This function uses automatic differentiation to compute slopes for a vast array of models, including non-linear models with transformations (e.g., polynomials). Uncertainty estimates are computed using the delta method.\n\n\nNumerical derivatives for the slopes function are calculated using a simple epsilon difference approach: \\(\\partial Y / \\partial X = (f(X + \\varepsilon/2) - f(X-\\varepsilon/2)) / \\varepsilon\\), where f is the predict() method associated with the model class, and \\(\\varepsilon\\) is determined by the eps argument.",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#value",
    "href": "articles/reference/slopes.html#value",
    "title": "slopes",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nterm: the variable whose marginal effect is computed\n\n\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\n\n\nstd.error: standard errors computed by via the delta method.\n\n\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\n\n\ns.value: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst‚Äôs intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.¬†(2020).\n\n\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nSee ?print.marginaleffects for printing options.",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#functions",
    "href": "articles/reference/slopes.html#functions",
    "title": "slopes",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_slopes(): Average slopes",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#standard-errors-using-the-delta-method",
    "href": "articles/reference/slopes.html#standard-errors-using-the-delta-method",
    "title": "slopes",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚Äúsimple‚Äù, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚ÄúRichardson‚Äù, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://marginaleffects.com/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://marginaleffects.com/articles/bootstrap.html",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#model-specific-arguments",
    "href": "articles/reference/slopes.html#model-specific-arguments",
    "title": "slopes",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#bayesian-posterior-summaries",
    "href": "articles/reference/slopes.html#bayesian-posterior-summaries",
    "title": "slopes",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúeti‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúhdi‚Äù)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmean‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmedian‚Äù)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the ‚Äúmarginaleffects_posterior_center‚Äù option (the median by default).",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#equivalence-inferiority-superiority",
    "href": "articles/reference/slopes.html#equivalence-inferiority-superiority",
    "title": "slopes",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#prediction-types",
    "href": "articles/reference/slopes.html#prediction-types",
    "title": "slopes",
    "section": "Prediction types",
    "text": "Prediction types\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\n\n\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=‚Äúinvlink(link)‚Äù will not always be equivalent to the average of estimates with type=‚Äúresponse‚Äù.\n\n\nSome of the most common type values are:\n\n\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#references",
    "href": "articles/reference/slopes.html#references",
    "title": "slopes",
    "section": "References",
    "text": "References\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106‚Äì114.\n\n\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191‚Äì93. https://doi.org/10.1093/aje/kwaa136",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/slopes.html#examples",
    "href": "articles/reference/slopes.html#examples",
    "title": "slopes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n\n\n\n# Unit-level (conditional) Marginal Effects\nmod &lt;- glm(am ~ hp * wt, data = mtcars, family = binomial)\nmfx &lt;- slopes(mod)\nhead(mfx)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S     2.5 %   97.5 %\n   hp 0.006983   0.005847 1.194    0.232 2.1 -0.004476 0.018442\n   hp 0.016404   0.012295 1.334    0.182 2.5 -0.007693 0.040501\n   hp 0.002828   0.003764 0.751    0.452 1.1 -0.004550 0.010206\n   hp 0.001935   0.002442 0.792    0.428 1.2 -0.002851 0.006721\n   hp 0.002993   0.003213 0.931    0.352 1.5 -0.003305 0.009291\n   hp 0.000148   0.000321 0.459    0.646 0.6 -0.000482 0.000778\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp, wt \nType:  response \n\n# Average Marginal Effect (AME)\navg_slopes(mod, by = TRUE)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %   97.5 %\n   hp  0.00265     0.0021  1.26   0.2069 2.3 -0.00147  0.00677\n   wt -0.43578     0.1435 -3.04   0.0024 8.7 -0.71712 -0.15445\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Marginal Effect at the Mean (MEM)\nslopes(mod, newdata = datagrid())\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 % 97.5 %\n   hp  0.00853    0.00824  1.03    0.301 1.7 -0.00763 0.0247\n   wt -1.74453    1.55594 -1.12    0.262 1.9 -4.79411 1.3051\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp, wt \nType:  response \n\n# Marginal Effect at User-Specified Values\n# Variables not explicitly included in `datagrid()` are held at their means\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term  hp Estimate Std. Error      z Pr(&gt;|z|)   S    2.5 %  97.5 %\n   hp 100  0.00117    0.00171  0.684    0.494 1.0 -0.00218 0.00451\n   hp 110  0.00190    0.00240  0.788    0.431 1.2 -0.00282 0.00661\n   wt 100 -0.19468    0.29895 -0.651    0.515 1.0 -0.78061 0.39126\n   wt 110 -0.33154    0.42907 -0.773    0.440 1.2 -1.17250 0.50942\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, am, wt \nType:  response \n\n# Group-Average Marginal Effects (G-AME)\n# Calculate marginal effects for each observation, and then take the average\n# marginal effect within each subset of observations with different observed\n# values for the `cyl` variable:\nmod2 &lt;- lm(mpg ~ hp * cyl, data = mtcars)\navg_slopes(mod2, variables = \"hp\", by = \"cyl\")\n\n\n Term    Contrast cyl Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 %  97.5 %\n   hp mean(dY/dX)   4  -0.0917     0.0353 -2.596  0.00943 6.7 -0.1610 -0.0225\n   hp mean(dY/dX)   6  -0.0523     0.0204 -2.561  0.01045 6.6 -0.0923 -0.0123\n   hp mean(dY/dX)   8  -0.0128     0.0143 -0.891  0.37280 1.4 -0.0409  0.0153\n\nColumns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n# Marginal Effects at User-Specified Values (counterfactual)\n# Variables not explicitly included in `datagrid()` are held at their\n# original values, and the whole dataset is duplicated once for each\n# combination of the values in `datagrid()`\nmfx &lt;- slopes(mod,\n              newdata = datagrid(hp = c(100, 110),\n              grid_type = \"counterfactual\"))\nhead(mfx)\n\n\n Term   wt  hp Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %  97.5 %\n   hp 2.62 100 0.012035    0.00994 1.211    0.226 2.1 -0.00744 0.03151\n   hp 2.62 110 0.006983    0.00585 1.194    0.232 2.1 -0.00448 0.01844\n   hp 2.88 100 0.014161    0.01051 1.347    0.178 2.5 -0.00644 0.03476\n   hp 2.88 110 0.016404    0.01229 1.334    0.182 2.5 -0.00769 0.04050\n   hp 2.32 100 0.001564    0.00220 0.712    0.476 1.1 -0.00274 0.00587\n   hp 2.32 110 0.000656    0.00118 0.557    0.577 0.8 -0.00165 0.00296\n\nColumns: rowid, rowidcf, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, hp, predicted_lo, predicted_hi, predicted \nType:  response \n\n# Heteroskedasticity robust standard errors\nmfx &lt;- slopes(mod, vcov = sandwich::vcovHC(mod))\nhead(mfx)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S     2.5 %   97.5 %\n   hp 0.006983   0.009047 0.772    0.440 1.2 -0.010748 0.024715\n   hp 0.016404   0.012419 1.321    0.187 2.4 -0.007936 0.040744\n   hp 0.002828   0.004876 0.580    0.562 0.8 -0.006728 0.012385\n   hp 0.001935   0.002035 0.951    0.342 1.5 -0.002054 0.005924\n   hp 0.002993   0.002928 1.022    0.307 1.7 -0.002747 0.008732\n   hp 0.000148   0.000235 0.629    0.529 0.9 -0.000312 0.000607\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp, wt \nType:  response \n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n\n    Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n wt=drat    -6.23       1.05 -5.92   &lt;0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using row indices\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b1-b2=0     6.23       1.05 5.92   &lt;0.001 28.2  4.16   8.29\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# same hypothesis test using numeric vector of weights\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n custom     6.23       1.05 5.92   &lt;0.001 28.2  4.16   8.29\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n\n       Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n Contrast A     6.23       1.05  5.92   &lt;0.001 28.2   4.16   8.29\n Contrast B   -11.46       4.92 -2.33   0.0197  5.7 -21.10  -1.83\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response",
    "crumbs": [
      "Functions",
      "slopes"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#description",
    "href": "articles/reference/marginal_means.html#description",
    "title": "marginal_means",
    "section": "Description",
    "text": "Description\n\nMarginal means are adjusted predictions, averaged across a grid of categorical predictors, holding other numeric predictors at their means. To learn more, read the marginal means vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\n\n\nhttps://marginaleffects.com/articles/marginalmeans.html\n\n\n\n\nhttps://marginaleffects.com/",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#usage",
    "href": "articles/reference/marginal_means.html#usage",
    "title": "marginal_means",
    "section": "Usage",
    "text": "Usage\nmarginal_means(\n  model,\n  variables = NULL,\n  newdata = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  transform = NULL,\n  cross = FALSE,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  wts = \"equal\",\n  by = NULL,\n  numderiv = \"fdforward\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#arguments",
    "href": "articles/reference/marginal_means.html#arguments",
    "title": "marginal_means",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nvariables\n\n\n\nFocal variables\n\n\n\n\nCharacter vector of variable names: compute marginal means for each category of the listed variables.\n\n\n\n\nNULL: calculate marginal means for all logical, character, or factor variables in the dataset used to fit model. Hint: Set cross=TRUE to compute marginal means for combinations of focal variables.\n\n\n\n\n\n\n\nnewdata\n\n\n\nGrid of predictor values over which we marginalize.\n\n\n\n\nWarning: Please avoid modifying your dataset between fitting the model and calling a marginaleffects function. This can sometimes lead to unexpected results.\n\n\n\n\nNULL create a grid with all combinations of all categorical predictors in the model. Warning: can be expensive.\n\n\n\n\nCharacter vector: subset of categorical variables to use when building the balanced grid of predictors. Other variables are held to their mean or mode.\n\n\n\n\nData frame: A data frame which includes all the predictors in the original model. The full dataset is replicated once for every combination of the focal variables in the variables argument, using the datagridcf() function.\n\n\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute marginal effects or contrasts. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\ntransform\n\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\n\ncross\n\n\n\nTRUE or FALSE\n\n\n\n\nFALSE (default): Marginal means are computed for each predictor individually.\n\n\n\n\nTRUE: Marginal means are computed for each combination of predictors specified in the variables argument.\n\n\n\n\n\n\n\nhypothesis\n\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The ‚Å†b*‚Å† wildcard can be used to test hypotheses on all estimates. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n‚Å†b* / b1 = 1‚Å†\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\n\nequivalence\n\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\n\np_adjust\n\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\n\ndf\n\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n\nwts\n\n\n\ncharacter value. Weights to use in the averaging.\n\n\n\n\n\"equal\": each combination of variables in newdata gets equal weight.\n\n\n\n\n\"cells\": each combination of values for the variables in the newdata gets a weight proportional to its frequency in the original data.\n\n\n\n\n\"proportional\": each combination of values for the variables in newdata ‚Äì except for those in the variables argument ‚Äì gets a weight proportional to its frequency in the original data.\n\n\n\n\n\n\n\nby\n\n\n\nCollapse marginal means into categories. Data frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\n\nnumderiv\n\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\n\n\"richardson\": Richardson extrapolation method\n\n\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(‚Äúfdcenter‚Äù, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#details",
    "href": "articles/reference/marginal_means.html#details",
    "title": "marginal_means",
    "section": "Details",
    "text": "Details\n\nThis function begins by calling the predictions function to obtain a grid of predictors, and adjusted predictions for each cell. The grid includes all combinations of the categorical variables listed in the variables and newdata arguments, or all combinations of the categorical variables used to fit the model if newdata is NULL. In the prediction grid, numeric variables are held at their means.\n\n\nAfter constructing the grid and filling the grid with adjusted predictions, marginal_means computes marginal means for the variables listed in the variables argument, by average across all categories in the grid.\n\n\nmarginal_means can only compute standard errors for linear models, or for predictions on the link scale, that is, with the type argument set to \"link\".\n\n\nThe marginaleffects website compares the output of this function to the popular emmeans package, which provides similar but more advanced functionality: https://marginaleffects.com/",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#value",
    "href": "articles/reference/marginal_means.html#value",
    "title": "marginal_means",
    "section": "Value",
    "text": "Value\n\nData frame of marginal means with one row per variable-value combination.",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#standard-errors-using-the-delta-method",
    "href": "articles/reference/marginal_means.html#standard-errors-using-the-delta-method",
    "title": "marginal_means",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚Äúsimple‚Äù, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = ‚ÄúRichardson‚Äù, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://marginaleffects.com/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://marginaleffects.com/articles/bootstrap.html",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#model-specific-arguments",
    "href": "articles/reference/marginal_means.html#model-specific-arguments",
    "title": "marginal_means",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#bayesian-posterior-summaries",
    "href": "articles/reference/marginal_means.html#bayesian-posterior-summaries",
    "title": "marginal_means",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúeti‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_interval‚Äù = ‚Äúhdi‚Äù)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmean‚Äù)\n\n\noptions(‚Äúmarginaleffects_posterior_center‚Äù = ‚Äúmedian‚Äù)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the ‚Äúmarginaleffects_posterior_center‚Äù option (the median by default).",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#equivalence-inferiority-superiority",
    "href": "articles/reference/marginal_means.html#equivalence-inferiority-superiority",
    "title": "marginal_means",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#prediction-types",
    "href": "articles/reference/marginal_means.html#prediction-types",
    "title": "marginal_means",
    "section": "Prediction types",
    "text": "Prediction types\n\nThe type argument determines the scale of the predictions used to compute quantities of interest with functions from the marginaleffects package. Admissible values for type depend on the model object. When users specify an incorrect value for type, marginaleffects will raise an informative error with a list of valid type values for the specific model object. The first entry in the list in that error message is the default type.\n\n\nThe invlink(link) is a special type defined by marginaleffects. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with type=‚Äúinvlink(link)‚Äù will not always be equivalent to the average of estimates with type=‚Äúresponse‚Äù.\n\n\nSome of the most common type values are:\n\n\nresponse, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#references",
    "href": "articles/reference/marginal_means.html#references",
    "title": "marginal_means",
    "section": "References",
    "text": "References\n\n\n\nGreenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106‚Äì114.\n\n\n\n\nCole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191‚Äì93. https://doi.org/10.1093/aje/kwaa136",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/marginal_means.html#examples",
    "href": "articles/reference/marginal_means.html#examples",
    "title": "marginal_means",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# simple marginal means for each level of `cyl`\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ carb + cyl + am, dat)\n\nmarginal_means(\n  mod,\n  variables = \"cyl\")\n\n\n Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  cyl     4 23.1       1.66 13.9   &lt;0.001 144.3  19.9   26.4\n  cyl     6 20.4       1.34 15.2   &lt;0.001 171.9  17.8   23.0\n  cyl     8 16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapse levels of cyl by averaging\nby &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  by = c(\"4 &amp; 6\", \"4 &amp; 6\", \"8\"))\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by)\n\n\n        By Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 4 &amp; 6 21.7       1.13 19.2   &lt;0.001 270.8  19.5   24.0\n 8         16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# pairwise differences between collapsed levels\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by,\n  hypothesis = \"pairwise\")\n\n\n          Term Mean Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n 4 &amp; 6 - 8 5.54       1.51 3.66   &lt;0.001 12.0  2.57    8.5\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# cross\nmarginal_means(mod,\n  variables = c(\"cyl\", \"carb\"),\n  cross = TRUE)\n\n\n Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 25.8       1.26 20.43   &lt;0.001 305.7 23.34   28.3\n 25.6       1.17 21.93   &lt;0.001 351.8 23.30   27.9\n 25.3       2.37 10.71   &lt;0.001  86.6 20.70   30.0\n 21.9       1.90 11.51   &lt;0.001  99.4 18.15   25.6\n 20.3       3.77  5.39   &lt;0.001  23.7 12.91   27.7\n 19.8       3.81  5.18   &lt;0.001  22.1 12.29   27.2\n 23.1       1.77 13.08   &lt;0.001 127.4 19.63   26.5\n 22.9       1.87 12.24   &lt;0.001 112.0 19.20   26.5\n 22.6       2.37  9.56   &lt;0.001  69.5 17.98   27.2\n 19.1       1.34 14.31   &lt;0.001 151.8 16.53   21.8\n 17.6       3.00  5.85   &lt;0.001  27.6 11.68   23.5\n 17.0       3.48  4.89   &lt;0.001  19.9 10.21   23.9\n 18.9       1.94  9.74   &lt;0.001  72.1 15.11   22.7\n 18.7       1.57 11.90   &lt;0.001 106.0 15.61   21.8\n 18.4       1.83 10.07   &lt;0.001  76.8 14.85   22.0\n 15.0       1.20 12.53   &lt;0.001 117.2 12.63   17.3\n 13.4       3.36  3.99   &lt;0.001  13.9  6.81   20.0\n 12.9       3.00  4.28   &lt;0.001  15.7  6.98   18.8\n\nResults averaged over levels of: am \nColumns: cyl, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# collapsed cross\nby &lt;- expand.grid(\n  cyl = unique(mtcars$cyl),\n  carb = unique(mtcars$carb))\nby$by &lt;- ifelse(\n  by$cyl == 4,\n  paste(\"Control:\", by$carb),\n  paste(\"Treatment:\", by$carb))\n\n\n# Convert numeric variables to categorical before fitting the model\ndat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$carb &lt;- as.factor(dat$carb)\nmod &lt;- lm(mpg ~ hp + am + carb, data = dat)\n\n# Compute and summarize marginal means\nmarginal_means(mod)\n\n\n Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n am   FALSE 17.9      1.244 14.37   &lt;0.001 153.0  15.4   20.3\n am   TRUE  23.1      0.974 23.72   &lt;0.001 410.9  21.2   25.0\n carb 1     22.0      1.345 16.35   &lt;0.001 197.2  19.4   24.6\n carb 2     21.5      1.025 20.95   &lt;0.001 321.5  19.5   23.5\n carb 3     20.6      1.780 11.55   &lt;0.001 100.1  17.1   24.0\n carb 4     18.8      1.042 18.06   &lt;0.001 239.9  16.8   20.9\n carb 6     18.5      3.019  6.12   &lt;0.001  30.0  12.6   24.4\n carb 8     21.6      4.055  5.33   &lt;0.001  23.3  13.7   29.6\n\nResults averaged over levels of: hp, am, carb \nColumns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Contrast between marginal means (carb2 - carb1), or \"is the 1st marginal means equal to the 2nd?\"\n# see the vignette on \"Hypothesis Tests and Custom Contrasts\" on the `marginaleffects` website.\nlc &lt;- c(-1, 1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = \"b2 = b1\")\n\n\n  Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b2=b1 -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n   Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n custom -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# Multiple custom contrasts\nlc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    -1, 1, 0, 0, 0, 0\n    ),\n  ncol = 2,\n  dimnames = list(NULL, c(\"A\", \"B\")))\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n Term   Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    A  1.199       6.15  0.195    0.845 0.2 -10.85  13.25\n    B -0.514       1.48 -0.348    0.728 0.5  -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response",
    "crumbs": [
      "Functions",
      "marginal_means"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#description",
    "href": "articles/reference/plot_predictions.html#description",
    "title": "plot_predictions",
    "section": "Description",
    "text": "Description\n\nPlot predictions on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal predictions, that is, predictions made on the original data, but averaged by subgroups. This is analogous to using the by argument in the predictions() function.\n\n\nThe condition argument is used to plot conditional predictions, that is, predictions made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a predictions() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below.\n\n\nSee the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\n\n\nhttps://marginaleffects.com",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#usage",
    "href": "articles/reference/plot_predictions.html#usage",
    "title": "plot_predictions",
    "section": "Usage",
    "text": "Usage\nplot_predictions(\n  model,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  transform = NULL,\n  points = 0,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#arguments",
    "href": "articles/reference/plot_predictions.html#arguments",
    "title": "plot_predictions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\ncondition\n\n\n\nConditional predictions\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey‚Äôs five numbers ?stats::fivenum\n\n\n\n\n\n\n\nby\n\n\n\nMarginal predictions\n\n\n\n\nCharacter vector (max length 3): Names of the categorical predictors to marginalize across.\n\n\n\n\n1: x-axis. 2: color. 3: facets.\n\n\n\n\n\n\n\nnewdata\n\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the predictions() function.\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\ntransform\n\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\n\npoints\n\n\n\nNumber between 0 and 1 which controls the transparency of raw data points. 0 (default) does not display any points.\n\n\n\n\n\nrug\n\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\n\ngray\n\n\n\nFALSE grayscale or color plot\n\n\n\n\n\ndraw\n\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#value",
    "href": "articles/reference/plot_predictions.html#value",
    "title": "plot_predictions",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object or data frame (if draw=FALSE)",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#model-specific-arguments",
    "href": "articles/reference/plot_predictions.html#model-specific-arguments",
    "title": "plot_predictions",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_predictions.html#examples",
    "href": "articles/reference/plot_predictions.html#examples",
    "title": "plot_predictions",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\nplot_predictions(mod, condition = \"wt\")\n\n\n\nmod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\nplot_predictions(mod, condition = c(\"hp\", \"wt\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = \"threenum\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = range))",
    "crumbs": [
      "Functions",
      "plot_predictions"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#description",
    "href": "articles/reference/plot_comparisons.html#description",
    "title": "plot_comparisons",
    "section": "Description",
    "text": "Description\n\nPlot comparisons on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal comparisons, that is, comparisons made on the original data, but averaged by subgroups. This is analogous to using the by argument in the comparisons() function.\n\n\nThe condition argument is used to plot conditional comparisons, that is, comparisons made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a comparisons() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\n\n\nhttps://marginaleffects.com",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#usage",
    "href": "articles/reference/plot_comparisons.html#usage",
    "title": "plot_comparisons",
    "section": "Usage",
    "text": "Usage\nplot_comparisons(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  comparison = \"difference\",\n  transform = NULL,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#arguments",
    "href": "articles/reference/plot_comparisons.html#arguments",
    "title": "plot_comparisons",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nvariables\n\n\n\nName of the variable whose contrast we want to plot on the y-axis.\n\n\n\n\n\ncondition\n\n\n\nConditional slopes\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey‚Äôs five numbers ?stats::fivenum.\n\n\n\n\n\n\n\nby\n\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function‚Äôs documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\n\nnewdata\n\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the comparisons() function.\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\ncomparison\n\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\n\n\nstring: shortcuts to common contrast functions.\n\n\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts\n\n\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\n\ntransform\n\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\n\nrug\n\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\n\ngray\n\n\n\nFALSE grayscale or color plot\n\n\n\n\n\ndraw\n\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#value",
    "href": "articles/reference/plot_comparisons.html#value",
    "title": "plot_comparisons",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#model-specific-arguments",
    "href": "articles/reference/plot_comparisons.html#model-specific-arguments",
    "title": "plot_comparisons",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_comparisons.html#examples",
    "href": "articles/reference/plot_comparisons.html#examples",
    "title": "plot_comparisons",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_comparisons(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))",
    "crumbs": [
      "Functions",
      "plot_comparisons"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#description",
    "href": "articles/reference/plot_slopes.html#description",
    "title": "plot_slopes",
    "section": "Description",
    "text": "Description\n\nPlot slopes on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal slopes, that is, slopes made on the original data, but averaged by subgroups. This is analogous to using the by argument in the slopes() function.\n\n\nThe condition argument is used to plot conditional slopes, that is, slopes made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a slopes() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://marginaleffects.com/articles/plot.html\n\n\n\n\nhttps://marginaleffects.com",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#usage",
    "href": "articles/reference/plot_slopes.html#usage",
    "title": "plot_slopes",
    "section": "Usage",
    "text": "Usage\nplot_slopes(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  slope = \"dydx\",\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#arguments",
    "href": "articles/reference/plot_slopes.html#arguments",
    "title": "plot_slopes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nvariables\n\n\n\nName of the variable whose marginal effect (slope) we want to plot on the y-axis.\n\n\n\n\n\ncondition\n\n\n\nConditional slopes\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey‚Äôs five numbers ?stats::fivenum.\n\n\n\n\n\n\n\nby\n\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\nFor more complex aggregations, you can use the FUN argument of the hypotheses() function. See that function‚Äôs documentation and the Hypothesis Test vignettes on the marginaleffects website.\n\n\n\n\n\n\n\nnewdata\n\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the slopes() function.\n\n\n\n\n\ntype\n\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the first entry in the error message is used by default.\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\nwts\n\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ‚Å†avg_*()‚Å† or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\n\nslope\n\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\n\n\"dydx\": dY/dX\n\n\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\n\n\"eydx\": dY/dX * Y\n\n\n\n\n\"dyex\": dY/dX / X\n\n\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\n\nrug\n\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\n\ngray\n\n\n\nFALSE grayscale or color plot\n\n\n\n\n\ndraw\n\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#value",
    "href": "articles/reference/plot_slopes.html#value",
    "title": "plot_slopes",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#model-specific-arguments",
    "href": "articles/reference/plot_slopes.html#model-specific-arguments",
    "title": "plot_slopes",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/plot_slopes.html#examples",
    "href": "articles/reference/plot_slopes.html#examples",
    "title": "plot_slopes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))",
    "crumbs": [
      "Functions",
      "plot_slopes"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#description",
    "href": "articles/reference/datagrid.html#description",
    "title": "datagrid",
    "section": "Description",
    "text": "Description\n\nGenerate a data grid of user-specified values for use in the newdata argument of the predictions(), comparisons(), and slopes() functions. This is useful to define where in the predictor space we want to evaluate the quantities of interest. Ex: the predicted outcome or slope for a 37 year old college graduate.\n\n\n\n\ndatagrid() generates data frames with combinations of \"typical\" or user-supplied predictor values.\n\n\n\n\ndatagridcf() generates \"counter-factual\" data frames, by replicating the entire dataset once for every combination of predictor values supplied by the user.",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#usage",
    "href": "articles/reference/datagrid.html#usage",
    "title": "datagrid",
    "section": "Usage",
    "text": "Usage\ndatagrid(\n  ...,\n  model = NULL,\n  newdata = NULL,\n  by = NULL,\n  FUN_character = get_mode,\n  FUN_factor = get_mode,\n  FUN_logical = get_mode,\n  FUN_numeric = function(x) mean(x, na.rm = TRUE),\n  FUN_integer = function(x) round(mean(x, na.rm = TRUE)),\n  FUN_other = function(x) mean(x, na.rm = TRUE),\n  grid_type = \"typical\"\n)\n\ndatagridcf(..., model = NULL, newdata = NULL)",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#arguments",
    "href": "articles/reference/datagrid.html#arguments",
    "title": "datagrid",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n‚Ä¶\n\n\n\nnamed arguments with vectors of values or functions for user-specified variables.\n\n\n\n\nFunctions are applied to the variable in the model dataset or newdata, and must return a vector of the appropriate type.\n\n\n\n\nCharacter vectors are automatically transformed to factors if necessary. +The output will include all combinations of these variables (see Examples below.)\n\n\n\n\n\n\n\nmodel\n\n\n\nModel object\n\n\n\n\n\nnewdata\n\n\n\ndata.frame (one and only one of the model and newdata arguments can be used.)\n\n\n\n\n\nby\n\n\n\ncharacter vector with grouping variables within which ‚Å†FUN_*‚Å† functions are applied to create \"sub-grids\" with unspecified variables.\n\n\n\n\n\nFUN_character\n\n\n\nthe function to be applied to character variables.\n\n\n\n\n\nFUN_factor\n\n\n\nthe function to be applied to factor variables.\n\n\n\n\n\nFUN_logical\n\n\n\nthe function to be applied to logical variables.\n\n\n\n\n\nFUN_numeric\n\n\n\nthe function to be applied to numeric variables.\n\n\n\n\n\nFUN_integer\n\n\n\nthe function to be applied to integer variables.\n\n\n\n\n\nFUN_other\n\n\n\nthe function to be applied to other variable types.\n\n\n\n\n\ngrid_type\n\n\n\ncharacter\n\n\n\n\n\"typical\": variables whose values are not explicitly specified by the user in ‚Ä¶ are set to their mean or mode, or to the output of the functions supplied to FUN_type arguments.\n\n\n\n\n\"counterfactual\": the entire dataset is duplicated for each combination of the variable values specified in ‚Ä¶. Variables not explicitly supplied to datagrid() are set to their observed values in the original dataset.",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#details",
    "href": "articles/reference/datagrid.html#details",
    "title": "datagrid",
    "section": "Details",
    "text": "Details\n\nIf datagrid is used in a predictions(), comparisons(), or slopes() call as the newdata argument, the model is automatically inserted in the model argument of datagrid() call, and users do not need to specify either the model or newdata arguments.\n\n\nIf users supply a model, the data used to fit that model is retrieved using the insight::get_data function.",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#value",
    "href": "articles/reference/datagrid.html#value",
    "title": "datagrid",
    "section": "Value",
    "text": "Value\n\nA data.frame in which each row corresponds to one combination of the named predictors supplied by the user via the ‚Ä¶ dots. Variables which are not explicitly defined are held at their mean or mode.",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#functions",
    "href": "articles/reference/datagrid.html#functions",
    "title": "datagrid",
    "section": "Functions",
    "text": "Functions\n\n\n\ndatagridcf(): Counterfactual data grid",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/datagrid.html#examples",
    "href": "articles/reference/datagrid.html#examples",
    "title": "datagrid",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n# The output only has 2 rows, and all the variables except `hp` are at their\n# mean or mode.\ndatagrid(newdata = mtcars, hp = c(100, 110))\n\n       mpg    cyl     disp     drat      wt     qsec     vs      am   gear\n1 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n2 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n    carb  hp\n1 2.8125 100\n2 2.8125 110\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndatagrid(model = mod, hp = c(100, 110))\n\n       mpg  hp\n1 20.09062 100\n2 20.09062 110\n\n# Use in `marginaleffects` to compute \"Typical Marginal Effects\". When used\n# in `slopes()` or `predictions()` we do not need to specify the\n#`model` or `newdata` arguments.\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp 100  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp 110  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# datagrid accepts functions\ndatagrid(hp = range, cyl = unique, newdata = mtcars)\n\n       mpg     disp     drat      wt     qsec     vs      am   gear   carb  hp\n1 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n2 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n3 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n4 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n5 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n6 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n  cyl\n1   6\n2   4\n3   8\n4   6\n5   4\n6   8\n\ncomparisons(mod, newdata = datagrid(hp = fivenum))\n\n\n Term Contrast  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp       +1  52  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1  96  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 123  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 180  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 335  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response \n\n# The full dataset is duplicated with each observation given counterfactual\n# values of 100 and 110 for the `hp` variable. The original `mtcars` includes\n# 32 rows, so the resulting dataset includes 64 rows.\ndg &lt;- datagrid(newdata = mtcars, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64\n\n# We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndg &lt;- datagrid(model = mod, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64",
    "crumbs": [
      "Functions",
      "datagrid"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#description",
    "href": "articles/reference/hypotheses.html#description",
    "title": "hypotheses",
    "section": "Description",
    "text": "Description\n\nUncertainty estimates are calculated as first-order approximate standard errors for linear or non-linear functions of a vector of random variables with known or estimated covariance matrix. In that sense, hypotheses emulates the behavior of the excellent and well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models; requires fewer dependencies; expands the range of tests to equivalence and superiority/inferiority; and offers convenience features like robust standard errors.\n\n\nTo learn more, read the hypothesis tests vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\n\n\nhttps://marginaleffects.com/articles/hypothesis.html\n\n\n\n\nhttps://marginaleffects.com/\n\n\n\n\nWarning #1: Tests are conducted directly on the scale defined by the type argument. For some models, it can make sense to conduct hypothesis or equivalence tests on the ‚Äúlink‚Äù scale instead of the ‚Äúresponse‚Äù scale which is often the default.\n\n\nWarning #2: For hypothesis tests on objects produced by the marginaleffects package, it is safer to use the hypothesis argument of the original function. Using hypotheses() may not work in certain environments, in lists, or when working programmatically with *apply style functions.\n\n\nWarning #3: The tests assume that the hypothesis expression is (approximately) normally distributed, which for non-linear functions of the parameters may not be realistic. More reliable confidence intervals can be obtained using the inferences() function with method = ‚Äúboot‚Äù.",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#usage",
    "href": "articles/reference/hypotheses.html#usage",
    "title": "hypotheses",
    "section": "Usage",
    "text": "Usage\nhypotheses(\n  model,\n  hypothesis = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  df = Inf,\n  equivalence = NULL,\n  joint = FALSE,\n  joint_test = \"f\",\n  FUN = NULL,\n  numderiv = \"fdforward\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#arguments",
    "href": "articles/reference/hypotheses.html#arguments",
    "title": "hypotheses",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\n\nModel object or object generated by the comparisons(), slopes(), predictions(), or marginal_means() functions.\n\n\n\n\n\nhypothesis\n\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. The ‚Å†b*‚Å† wildcard can be used to test hypotheses on all estimates. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n‚Å†b* / b1 = 1‚Å†\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html\n\n\n\n\n\n\n\nvcov\n\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: ‚ÄúHC‚Äù, ‚ÄúHC0‚Äù, ‚ÄúHC1‚Äù, ‚ÄúHC2‚Äù, ‚ÄúHC3‚Äù, ‚ÄúHC4‚Äù, ‚ÄúHC4m‚Äù, ‚ÄúHC5‚Äù. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: ‚ÄúHAC‚Äù\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: ‚ÄúNeweyWest‚Äù, ‚ÄúKernHAC‚Äù, ‚ÄúOPG‚Äù. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\n\nconf_level\n\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\n\ndf\n\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n\nequivalence\n\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\n\njoint\n\n\n\nJoint test of statistical significance. The null hypothesis value can be set using the hypothesis argument.\n\n\n\n\nFALSE: Hypotheses are not tested jointly.\n\n\n\n\nTRUE: All parameters are tested jointly.\n\n\n\n\nString: A regular expression to match parameters to be tested jointly. grep(joint, perl = TRUE)\n\n\n\n\nCharacter vector of parameter names to be tested. Characters refer to the names of the vector returned by coef(object).\n\n\n\n\nInteger vector of indices. Which parameters positions to test jointly.\n\n\n\n\n\n\n\njoint_test\n\n\n\nA character string specifying the type of test, either \"f\" or \"chisq\". The null hypothesis is set by the hypothesis argument, with default null equal to 0 for all parameters.\n\n\n\n\n\nFUN\n\n\n\nNULL or function.\n\n\n\n\nNULL (default): hypothesis test on a model‚Äôs coefficients, or on the quantities estimated by one of the marginaleffects package functions.\n\n\n\n\nFunction which accepts a model object and returns a numeric vector or a data.frame with two columns called term and estimate. This argument can be useful when users want to conduct a hypothesis test on an arbitrary function of quantities held in a model object. See examples below.\n\n\n\n\n\n\n\nnumderiv\n\n\n\nstring or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.\n\n\n\n\n\"fdforward\": finite difference method with forward differences\n\n\n\n\n\"fdcenter\": finite difference method with central differences (default)\n\n\n\n\n\"richardson\": Richardson extrapolation method\n\n\n\n\nExtra arguments can be specified by passing a list to the numDeriv argument, with the name of the method first and named arguments following, ex: numderiv=list(‚Äúfdcenter‚Äù, eps = 1e-5). When an unknown argument is used, marginaleffects prints the list of valid arguments for each method.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments.",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#joint-hypothesis-tests",
    "href": "articles/reference/hypotheses.html#joint-hypothesis-tests",
    "title": "hypotheses",
    "section": "Joint hypothesis tests",
    "text": "Joint hypothesis tests\n\nThe test statistic for the joint Wald test is calculated as (R * theta_hat - r)‚Äô * inv(R * V_hat * R‚Äô) * (R * theta_hat - r) / Q, where theta_hat is the vector of estimated parameters, V_hat is the estimated covariance matrix, R is a Q x P matrix for testing Q hypotheses on P parameters, r is a Q x 1 vector for the null hypothesis, and Q is the number of rows in R. If the test is a Chi-squared test, the test statistic is not normalized.\n\n\nThe p-value is then calculated based on either the F-distribution (for F-test) or the Chi-squared distribution (for Chi-squared test). For the F-test, the degrees of freedom are Q and (n - P), where n is the sample size and P is the number of parameters. For the Chi-squared test, the degrees of freedom are Q.",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#equivalence-inferiority-superiority",
    "href": "articles/reference/hypotheses.html#equivalence-inferiority-superiority",
    "title": "hypotheses",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n\\(\\theta\\) is an estimate, \\(\\sigma_\\theta\\) its estimated standard error, and \\([a, b]\\) are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\leq a\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &gt; a\\)\n\n\n\n\n\\(t=(\\theta - a)/\\sigma_\\theta\\)\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\n\\(H_0\\): \\(\\theta \\geq b\\)\n\n\n\n\n\\(H_1\\): \\(\\theta &lt; b\\)\n\n\n\n\n\\(t=(\\theta - b)/\\sigma_\\theta\\)\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature.",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/hypotheses.html#examples",
    "href": "articles/reference/hypotheses.html#examples",
    "title": "hypotheses",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\n# When `FUN` and `hypotheses` are `NULL`, `hypotheses()` returns a data.frame of parameters\nhypotheses(mod)\n\n\n         Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Test of equality between coefficients\nhypotheses(mod, hypothesis = \"hp = wt\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Non-linear function\nhypotheses(mod, hypothesis = \"exp(hp + wt) = 0.1\")\n\n\n               Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Robust standard errors\nhypotheses(mod, hypothesis = \"hp = wt\", vcov = \"HC3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# b1, b2, ... shortcuts can be used to identify the position of the\n# parameters of interest in the output of FUN\nhypotheses(mod, hypothesis = \"b2 = b3\")\n\n\n    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# wildcard\nhypotheses(mod, hypothesis = \"b* / b2 = 1\")\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n b1 / b2 = 1    -1551      764.0 -2.03   0.0423 4.6 -3048.9    -54\n b2 / b2 = 1        0         NA    NA       NA  NA      NA     NA\n b3 / b2 = 1      137       78.1  1.75   0.0804 3.6   -16.6    290\n b4 / b2 = 1      144      111.0  1.30   0.1938 2.4   -73.3    362\n b5 / b2 = 1      137      151.9  0.90   0.3679 1.4  -161.0    435\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# term names with special characters have to be enclosed in backticks\nhypotheses(mod, hypothesis = \"`factor(cyl)6` = `factor(cyl)8`\")\n\n\n                            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %\n `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41\n 97.5 %\n   3.07\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmod2 &lt;- lm(mpg ~ hp * drat, data = mtcars)\nhypotheses(mod2, hypothesis = \"`hp:drat` = drat\")\n\n\n             Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n `hp:drat` = drat    -6.08       2.89 -2.1   0.0357 4.8 -11.8 -0.405\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# predictions(), comparisons(), and slopes()\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\ncmp &lt;- comparisons(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n b1=b2   -0.288      0.125 -2.3   0.0212 5.6 -0.533 -0.043\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nmfx &lt;- slopes(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b2 = 0.2\")\n\n\n   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n b2=0.2    0.101      0.131 0.772     0.44 1.2 -0.156  0.359\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\npre &lt;- predictions(mod, newdata = datagrid(hp = 110, mpg = c(30, 35)))\nhypotheses(pre, hypothesis = \"b1 = b2\")\n\n\n  Term  Estimate Std. Error      z Pr(&gt;|z|)   S     2.5 %   97.5 %\n b1=b2 -3.57e-05   0.000172 -0.207    0.836 0.3 -0.000373 0.000302\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n# The `FUN` argument can be used to compute standard errors for fitted values\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) predict(x, type = \"link\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n    1   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    2   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    3    0.233      0.781  0.299    0.765 0.4 -1.30  1.764\n    4   -0.595      0.647 -0.919    0.358 1.5 -1.86  0.674\n    5   -0.418      0.647 -0.645    0.519 0.9 -1.69  0.851\n    6   -5.026      2.195 -2.290    0.022 5.5 -9.33 -0.725\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nf &lt;- function(x) predict(x, type = \"response\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    1  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    2  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    3  0.55803     0.1926 2.898  0.00376 8.1  0.1806 0.9355\n    4  0.35560     0.1483 2.398  0.01648 5.9  0.0650 0.6462\n    5  0.39710     0.1550 2.562  0.01041 6.6  0.0933 0.7009\n    6  0.00652     0.0142 0.459  0.64653 0.6 -0.0213 0.0344\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Complex aggregation\n# Step 1: Collapse predicted probabilities by outcome level, for each individual\n# Step 2: Take the mean of the collapsed probabilities by group and `cyl`\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\n\naggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 &amp; 4\", \"5\")) |&gt;\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n\n\n     Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n 3 &amp; 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n 3 &amp; 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n 3 &amp; 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n 5            6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n 5            4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n 5            8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n\nColumns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Equivalence, non-inferiority, and non-superiority tests\nmod &lt;- lm(mpg ~ hp + factor(gear), data = mtcars)\np &lt;- predictions(mod, newdata = \"median\")\nhypotheses(p, equivalence = c(17, 18))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf)\n     19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404\n p (Equiv)  hp gear\n     0.951 123    3\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\nmfx &lt;- avg_slopes(mod, variables = \"hp\")\nhypotheses(mfx, equivalence = c(-.1, .1))\n\n\n Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup)\n   hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001\n p (NonInf) p (Equiv)\n    0.00135   0.00135\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\ncmp &lt;- avg_comparisons(mod, variables = \"gear\", hypothesis = \"pairwise\")\nhypotheses(cmp, equivalence = c(0, 10))\n\n\n              Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n (4 - 3) - (5 - 3)    -3.94       2.05 -1.92   0.0543 4.2 -7.95 0.0727\n p (NonSup) p (NonInf) p (Equiv)\n     &lt;0.001      0.973     0.973\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response \n\n# joint hypotheses: character vector\nmodel &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n\n\n\nJoint hypothesis test:\nas.factor(cyl)6:hp = 0\nas.factor(cyl)8:hp = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 2.11    0.142    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n as.factor(cyl)6:hp = 0\n as.factor(cyl)8:hp = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 5.7  0.00197    4   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.12  0.00665    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 1\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 6.84  0.00411    2   26\n\nColumns: statistic, p.value, df1, df2 \n\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 2\n \n    F Pr(&gt;|F|) Df 1 Df 2\n 7.47  0.00273    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n cyl 6 - 4 = 0\n cyl 8 - 4 = 0\n \n   F Pr(&gt;|F|) Df 1 Df 2\n 1.6    0.221    2   26\n\nColumns: statistic, p.value, df1, df2",
    "crumbs": [
      "Functions",
      "hypotheses"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#description",
    "href": "articles/reference/inferences.html#description",
    "title": "inferences",
    "section": "Description",
    "text": "Description\n\nWarning: This function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other marginaleffects functions.\n\n\nApply this function to a marginaleffects object to change the inferential method used to compute uncertainty estimates.",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#usage",
    "href": "articles/reference/inferences.html#usage",
    "title": "inferences",
    "section": "Usage",
    "text": "Usage\ninferences(\n  x,\n  method,\n  R = 1000,\n  conf_type = \"perc\",\n  conformal_test = NULL,\n  conformal_calibration = NULL,\n  conformal_score = \"residual_abs\",\n  ...\n)",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#arguments",
    "href": "articles/reference/inferences.html#arguments",
    "title": "inferences",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nx\n\n\n\nObject produced by one of the core marginaleffects functions.\n\n\n\n\n\nmethod\n\n\n\nString\n\n\n\n\n\"delta\": delta method standard errors\n\n\n\n\n\"boot\" package\n\n\n\n\n\"fwb\": fractional weighted bootstrap\n\n\n\n\n\"rsample\" package\n\n\n\n\n\"simulation\" from a multivariate normal distribution (Krinsky & Robb, 1986)\n\n\n\n\n\"mi\" multiple imputation for missing data\n\n\n\n\n\"conformal_split\": prediction intervals using split conformal prediction (see Angelopoulos & Bates, 2022)\n\n\n\n\n\"conformal_cv+\": prediction intervals using cross-validation+ conformal prediction (see Barber et al., 2020)\n\n\n\n\n\n\n\nR\n\n\n\nNumber of resamples, simulations, or cross-validation folds.\n\n\n\n\n\nconf_type\n\n\n\nString: type of bootstrap interval to construct.\n\n\n\n\nboot: \"perc\", \"norm\", \"basic\", or \"bca\"\n\n\n\n\nfwb: \"perc\", \"norm\", \"basic\", \"bc\", or \"bca\"\n\n\n\n\nrsample: \"perc\" or \"bca\"\n\n\n\n\nsimulation: argument ignored.\n\n\n\n\n\n\n\nconformal_test\n\n\n\nData frame of test data for conformal prediction.\n\n\n\n\n\nconformal_calibration\n\n\n\nData frame of calibration data for split conformal prediction (‚Å†method=‚Äúconformal_split‚Å†).\n\n\n\n\n\nconformal_score\n\n\n\nString. Warning: The type argument in predictions() must generate predictions which are on the same scale as the outcome variable. Typically, this means that type must be \"response\" or \"probs\".\n\n\n\n\n\"residual_abs\" or \"residual_sq\" for regression tasks (numeric outcome)\n\n\n\n\n\"softmax\" for classification tasks (when predictions() returns a group columns, such as multinomial or ordinal logit models.\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\n\n\nIf method=‚Äúboot‚Äù, additional arguments are passed to boot::boot().\n\n\n\n\nIf method=‚Äúfwb‚Äù, additional arguments are passed to fwb::fwb().\n\n\n\n\nIf method=‚Äúrsample‚Äù, additional arguments are passed to rsample::bootstraps().\n\n\n\n\nAdditional arguments are ignored for all other methods.",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#details",
    "href": "articles/reference/inferences.html#details",
    "title": "inferences",
    "section": "Details",
    "text": "Details\n\nWhen method=‚Äúsimulation‚Äù, we conduct simulation-based inference following the method discussed in Krinsky & Robb (1986):\n\n\n\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model‚Äôs estimated coefficients and variance equal to the model‚Äôs variance-covariance matrix (classical, \"HC3\", or other).\n\n\n\n\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, slopes, or hypotheses.\n\n\n\n\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\n\n\n\nWhen method=‚Äúfwb‚Äù, drawn weights are supplied to the model fitting function‚Äôs weights argument; if the model doesn‚Äôt accept non-integer weights, this method should not be used. If weights were included in the original model fit, they are extracted by weights() and multiplied by the drawn weights. These weights are supplied to the wts argument of the estimation function (e.g., comparisons()).",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#value",
    "href": "articles/reference/inferences.html#value",
    "title": "inferences",
    "section": "Value",
    "text": "Value\n\nA marginaleffects object with simulation or bootstrap resamples and objects attached.",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#references",
    "href": "articles/reference/inferences.html#references",
    "title": "inferences",
    "section": "References",
    "text": "References\n\nKrinsky, I., and A. L. Robb. 1986. ‚ÄúOn Approximating the Statistical Properties of Elasticities.‚Äù Review of Economics and Statistics 68 (4): 715‚Äì9.\n\n\nKing, Gary, Michael Tomz, and Jason Wittenberg. \"Making the most of statistical analyses: Improving interpretation and presentation.\" American journal of political science (2000): 347-361\n\n\nDowd, Bryan E., William H. Greene, and Edward C. Norton. \"Computation of standard errors.\" Health services research 49.2 (2014): 731-750.\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2022. \"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.\" arXiv. https://doi.org/10.48550/arXiv.2107.07511.\n\n\nBarber, Rina Foygel, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. 2020. ‚ÄúPredictive Inference with the Jackknife+.‚Äù arXiv. http://arxiv.org/abs/1905.02928.",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/inferences.html#examples",
    "href": "articles/reference/inferences.html#examples",
    "title": "inferences",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nlibrary(magrittr)\nset.seed(1024)\nmod &lt;- lm(Sepal.Length ~ Sepal.Width * Species, data = iris)\n\n# bootstrap\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"boot\")\n\n\n    Species Estimate Std. Error 2.5 % 97.5 %\n setosa         5.01     0.0344  4.93   5.07\n versicolor     5.94     0.0619  5.82   6.07\n virginica      6.59     0.0788  6.44   6.75\n\nColumns: Species, estimate, std.error, conf.low, conf.high \nType:  response \n\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"rsample\")\n\n\n    Species Estimate 2.5 % 97.5 %\n setosa         5.01  4.94   5.07\n versicolor     5.94  5.82   6.06\n virginica      6.59  6.44   6.75\n\nColumns: Species, estimate, conf.low, conf.high \nType:  response \n\n# Fractional (bayesian) bootstrap\navg_slopes(mod, by = \"Species\") %&gt;%\n  inferences(method = \"fwb\") %&gt;%\n  posterior_draws(\"rvar\") %&gt;%\n  data.frame()\n\n         term                        contrast    Species  estimate predicted_lo\n1 Sepal.Width                     mean(dY/dX)     setosa 0.6904897     5.055715\n2 Sepal.Width                     mean(dY/dX) versicolor 0.8650777     6.307983\n3 Sepal.Width                     mean(dY/dX)  virginica 0.9015345     6.881900\n4     Species mean(versicolor) - mean(setosa)     setosa 1.4992211     5.055715\n5     Species mean(versicolor) - mean(setosa) versicolor 1.3843422     4.848568\n6     Species mean(versicolor) - mean(setosa)  virginica 1.4199582     4.917617\n7     Species  mean(virginica) - mean(setosa)     setosa 1.9912967     5.055715\n8     Species  mean(virginica) - mean(setosa) versicolor 1.8524292     4.848568\n9     Species  mean(virginica) - mean(setosa)  virginica 1.8954823     4.917617\n  predicted_hi predicted  std.error  conf.low conf.high         rvar\n1     5.055881  5.055715 0.08102709 0.5487008 0.8583514 0.69 ¬± 0.081\n2     6.308191  6.307983 0.20712193 0.4808519 1.2809997 0.88 ¬± 0.207\n3     6.882117  6.881900 0.23704875 0.3867165 1.3338278 0.89 ¬± 0.237\n4     6.567507  5.055715 0.15422309 1.2037516 1.7935150 1.50 ¬± 0.154\n5     6.307983  6.307983 0.08693716 1.2216215 1.5524924 1.38 ¬± 0.087\n6     6.394491  6.881900 0.08908202 1.2437332 1.5875035 1.42 ¬± 0.089\n7     7.062207  5.055715 0.12038738 1.7300974 2.2013722 1.99 ¬± 0.120\n8     6.791747  6.307983 0.12456281 1.6287253 2.1275295 1.86 ¬± 0.125\n9     6.881900  6.881900 0.09656870 1.7196001 2.0992510 1.90 ¬± 0.097\n\n# Simulation-based inference\nslopes(mod) %&gt;%\n  inferences(method = \"simulation\") %&gt;%\n  head()\n\n\n        Term Contrast Estimate Std. Error 2.5 % 97.5 %\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n Sepal.Width    dY/dX     0.69      0.166 0.392   1.02\n\nColumns: rowid, term, contrast, estimate, std.error, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx, Sepal.Length, Sepal.Width, Species \nType:  response",
    "crumbs": [
      "Functions",
      "inferences"
    ]
  },
  {
    "objectID": "articles/reference/posterior_draws.html#description",
    "href": "articles/reference/posterior_draws.html#description",
    "title": "posterior_draws",
    "section": "Description",
    "text": "Description\n\nExtract Posterior Draws or Bootstrap Resamples from marginaleffects Objects",
    "crumbs": [
      "Functions",
      "posterior_draws"
    ]
  },
  {
    "objectID": "articles/reference/posterior_draws.html#usage",
    "href": "articles/reference/posterior_draws.html#usage",
    "title": "posterior_draws",
    "section": "Usage",
    "text": "Usage\nposterior_draws(x, shape = \"long\")",
    "crumbs": [
      "Functions",
      "posterior_draws"
    ]
  },
  {
    "objectID": "articles/reference/posterior_draws.html#arguments",
    "href": "articles/reference/posterior_draws.html#arguments",
    "title": "posterior_draws",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nx\n\n\n\nAn object produced by a marginaleffects package function, such as predictions(), avg_slopes(), hypotheses(), etc.\n\n\n\n\n\nshape\n\n\n\nstring indicating the shape of the output format:\n\n\n\n\n\"long\": long format data frame\n\n\n\n\n\"DxP\": Matrix with draws as rows and parameters as columns\n\n\n\n\n\"PxD\": Matrix with draws as rows and parameters as columns\n\n\n\n\n\"rvar\": Random variable datatype (see posterior package documentation).",
    "crumbs": [
      "Functions",
      "posterior_draws"
    ]
  },
  {
    "objectID": "articles/reference/posterior_draws.html#value",
    "href": "articles/reference/posterior_draws.html#value",
    "title": "posterior_draws",
    "section": "Value",
    "text": "Value\n\nA data.frame with drawid and draw columns.",
    "crumbs": [
      "Functions",
      "posterior_draws"
    ]
  }
]