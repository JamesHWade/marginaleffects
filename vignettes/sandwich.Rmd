---
title: "Standard Errors and Hypothesis Tests"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Standard Errors and Hypothesis Tests}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
# this vignette is in .Rbuildignore because lme4 is not available on old CRAN
# test machines.

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 9,
  fig.asp = .4,
  out.width = "100%",
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

The code in this vignette requires version 0.5.0 of `marginaleffects` and 0.17.1 of `insight`.

Table of Contents:

* Robust standard errors
* Sattherthwaite and Kenward-Roger corrections for mixed effects models
* Delta method
* Standard errors for average marginal effects and contrasts

# Robust standard errors 

All the functions in the `marginaleffects` package can compute robust standard errors on the fly for any model type supported by [the `sandwich` package.](https://sandwich.r-forge.r-project.org/) The `vcov` argument supports string shortcuts like `"HC3"`, a one-sided formula to request clustered standard errors, variance-covariance matrices, or functions which return such matrices. Here are a few examples.

Adjusted predictions with classical or heteroskedasticity-robust standard errors:

```{r}
library(marginaleffects)
library(patchwork)
mod <- lm(mpg ~ hp, data = mtcars)

p <- predictions(mod)
head(p, 2)

p <- predictions(mod, vcov = "HC3")
head(p, 2)
```

Marginal effects with cluster-robust standard errors:

```{r}
mfx <- marginaleffects(mod, vcov = ~cyl)
summary(mfx)
```

Comparing adjusted predictions with classical and robust standard errors:

```{r, fig.asp = .4}
p1 <- plot_cap(mod, condition = "hp")
p2 <- plot_cap(mod, condition = "hp", vcov = "HC3")
p1 + p2
```

# Mixed effects models: Satterthwaite and Kenward-Roger corrections

For linear mixed effects models we can apply the Satterthwaite and Kenward-Roger corrections in the same way as above:

```{r, message = FALSE}
library(marginaleffects)
library(patchwork)
library(lme4)

dat <- mtcars
dat$cyl <- factor(dat$cyl)
dat$am <- as.logical(dat$am)
mod <- lmer(mpg ~ hp + am + (1 | cyl), data = dat)
```

Marginal effects at the mean with classical standard errors and z-statistic:

```{r}
marginaleffects(mod, newdata = "mean")
```

Marginal effects at the mean with Kenward-Roger adjusted variance-covariance and degrees of freedom:

```{r}
marginaleffects(mod,
                newdata = "mean",
                vcov = "kenward-roger")
```

We can use the same option in any of the package's core functions, including:

```{r}
plot_cap(mod, condition = "hp", vcov = "satterthwaite")
```

# Delta Method

All the standard errors generated by the `marginaleffects` package are estimated using the delta method. Mathematical treatments of this method can be found in most statistics textbooks [and on Wikipedia.](https://en.wikipedia.org/wiki/Delta_method) Roughly speaking, the delta method allows us to approximate the distribution of a smooth function of an asymptotically normal estimator. 

Concretely, this allows us to generate standard errors around functions of a model's coefficient estimates. Predictions, contrasts, marginal effects, and marginal means are functions of the coefficients, so we can use the delta method to estimate standard errors around all of those quantities. Since there are a lot of mathematical treatments available elsewhere, this vignette focuses on the "implementation" in `marginaleffects`.

Consider the case of the `marginalmeans()` function. When a user calls this function, they obtain a vector of marginal means. To estimate standard errors around this vector:

1. Take the numerical derivative of the marginal means vector with respect to the first coefficient in the model:
   - Compute marginal means with the original model: $f(\beta)$
   - Increment the first (and only the first) coefficient held inside the model object by a small amount, and compute marginal means again: $f(\beta+\varepsilon)$
   -  Calculate: $\frac{f(\beta+\varepsilon) - f(\beta)}{\varepsilon}$
2. Repeat step 1 for every coefficient in the model to construct a $J$ matrix.
3. Extract the variance-covariance matrix of the coefficient estimates: $V$
4. Standard errors are the square root of the diagonal of $JVJ'$

The main function used to compute standard errors in `marginaleffects` is here: https://github.com/vincentarelbundock/marginaleffects/blob/main/R/get_se_delta.R

The `predictions()` function behaves slightly differently. For GLM and mixed effects models, `predictions()` function delegates the computation of standard errors and confidence intervals to the `get_predicted()` function of the `insight` package. The benefit is that `insight` tries to compute predictions and confidence intervals on the link scale, and then transforms them to the response scale. This ensures that, for example, confidence intervals around predicted probabilities do not stretch outside the $[0,1]$ interval.

This type of transformation is not done automatically in the case of `marginalmeans()`, `comparisons()` or `marginaleffects()` function. Similar results can be achieved in `marginalmeans()` by using the `type="link"` argument and by supplying an appropriate transformation function to the `transform_post` argument (available in version 0.5.1 of the package). The `insight::link_inverse()` function could be helpful here.

# Standard Error of the Average Marginal Effect or Contrast

The `summary()` and `tidy()` functions compute the average marginal effect (or contrast) when they are applied to an object produced by `marginaleffects()` (or `comparisons()`). This is done in 3 steps:

1. Extract the Jacobian used to compute unit-level standard errors.
2. Take the average of that Jacobian.
3. Estimate the standard error of the average marginal effect as the square root of the diagonal of J'VJ, where V is the variance-covariance matrix.

As [explained succinctly on Stack Exchange:](https://stats.stackexchange.com/a/331311/4874)

> we want the variance of the Average Marginal Effect (AME) and hence our transformed function is: $AME =  \frac{1}{N} \sum_{i=1}^N g_i(x_i,\hat{\beta})$ Then using the delta method we have $Var \left( g(\hat{\beta}) \right) = J_g' \Omega_{\hat{\beta}} J_g$ where $\Omega_{\hat{\beta}} = Var(\hat{\beta})$ and $J_g' = \frac{\partial\left[\frac{1}{N}\sum_{i=1}^N g (x_i,\hat{\beta})\right]}{\partial \hat\beta} = \frac{1}{N}{\left[\sum_{i=1}^N \frac{\partial \left (g (x_i,\hat{\beta})\right)}{\partial \hat\beta}\right]}$ Which justifies using the "average Jacobian" in the delta method to calculate variance of the AME.

References:

* Dowd, Bryan E, William H Greene, and Edward C Norton. “Computation of Standard Errors.” Health Services Research 49, no. 2 (April 2014): 731–50. https://doi.org/10.1111/1475-6773.12122.
* https://stats.stackexchange.com/questions/283831/delta-method-for-marginal-effects-of-generalized-linear-model?rq=1


