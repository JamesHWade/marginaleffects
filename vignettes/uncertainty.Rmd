---
title: "Standard Errors and Confidence Intervals"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Standard Errors and Confidence Intervals}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
options(width = 1000)
# this vignette is in .Rbuildignore because lme4 is not available on old CRAN
# test machines.

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 9,
  fig.asp = .4,
  out.width = "100%",
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

# Delta Method

All the standard errors generated by the `slopes()`, `comparisons()`, and `hypotheses()` functions of this package package are estimated using the delta method. Mathematical treatments of this method can be found in most statistics textbooks [and on Wikipedia.](https://en.wikipedia.org/wiki/Delta_method) Roughly speaking, the delta method allows us to approximate the distribution of a smooth function of an asymptotically normal estimator. 

Concretely, this allows us to generate standard errors around functions of a model's coefficient estimates. Predictions, contrasts, marginal effects, and marginal means are functions of the coefficients, so we can use the delta method to estimate standard errors around all of those quantities. Since there are a lot of mathematical treatments available elsewhere, this vignette focuses on the "implementation" in `marginaleffects`.

Consider the case of the `marginal_means()` function. When a user calls this function, they obtain a vector of marginal means. To estimate standard errors around this vector:

1. Take the numerical derivative of the marginal means vector with respect to the first coefficient in the model:
   - Compute marginal means with the original model: $f(\beta)$
   - Increment the first (and only the first) coefficient held inside the model object by a small amount, and compute marginal means again: $f(\beta+\varepsilon)$
   -  Calculate: $\frac{f(\beta+\varepsilon) - f(\beta)}{\varepsilon}$
2. Repeat step 1 for every coefficient in the model to construct a $J$ matrix.
3. Extract the variance-covariance matrix of the coefficient estimates: $V$
4. Standard errors are the square root of the diagonal of $JVJ'$

The main function used to compute standard errors in `marginaleffects` is here: https://github.com/vincentarelbundock/marginaleffects/blob/main/R/get_se_delta.R

# Standard errors and intervals for `slopes()` and `comparisons()`

All standard errors for the `slopes()` and `comparisons()` functions are computed using the delta method, as described above.

# Standard errors and intervals for `marginal_means()` and `predictions()`

The `marginal_means()` and `predictions()` function can compute the confidence intervals in two ways. If the following conditions hold:

* The user sets: `type = "response"`
* The model class is `glm`
* The `transform_post` argument is `NULL`

then `marginal_means()` and `predictions()` will first compute estimates on the link scale, and then back transform them using the inverse link function supplied by `insight::link_inverse(model)` function.

In all other cases, standard errors are computed using the delta method as described above.

# Robust standard errors 

All the functions in the `marginaleffects` package can compute robust standard errors on the fly for any model type supported by [the `sandwich` package.](https://sandwich.r-forge.r-project.org/) The `vcov` argument supports string shortcuts like `"HC3"`, a one-sided formula to request clustered standard errors, variance-covariance matrices, or functions which return such matrices. Here are a few examples.

Adjusted predictions with classical or heteroskedasticity-robust standard errors:

```{r}
library(marginaleffects)
library(patchwork)
mod <- lm(mpg ~ hp, data = mtcars)

p <- predictions(mod)
head(p, 2)

p <- predictions(mod, vcov = "HC3")
head(p, 2)
```

Marginal effects with cluster-robust standard errors:

```{r}
avg_slopes(mod, vcov = ~cyl)
```

Comparing adjusted predictions with classical and robust standard errors:

```{r, fig.asp = .4}
p1 <- plot_predictions(mod, condition = "hp")
p2 <- plot_predictions(mod, condition = "hp", vcov = "HC3")
p1 + p2
```


# Simulation-based inference

`marginaleffects` offers an *experimental* `inferences` function to conduct simulation-based inference following the strategy proposed by Krinsky & Robb (1986):

1. Draw `iter` sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model's estimated coefficients and variance equal to the model's variance-covariance matrix (classical, "HC3", or other).
2. Use the `iter` sets of coefficients to compute `iter` sets of estimands: predictions, comparisons, or slopes.
3. Take quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.

Here are a few examples:

```{r, warning = FALSE}
library(marginaleffects)
library(ggplot2)
library(ggdist)

mod <- glm(vs ~ hp * wt + factor(gear), data = mtcars, family = binomial)

mod |> predictions() |> inferences(method = "simulation")

mod |> avg_slopes(vcov = ~gear) |> inferences(method = "simulation")
```

Since simulation based inference generates `iter` estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the `posterior_draws()` function, and plot their distributions using packages like`ggplot2` and `ggdist`:

```{r}
mod |>
  avg_comparisons(variables = "gear") |>
  inferences(method = "simulation") |>
  posterior_draws("rvar") |>
  ggplot(aes(y = contrast, xdist = rvar)) +
  stat_slabinterval()
```


# Bootstrap

It is easy to use the bootstrap as an alternative strategy to compute standard errors and confidence intervals. Several `R` packages can help us achieve this, including the long-established `boot` package:

```{r}
library(boot)
set.seed(123)

bootfun <- function(data, indices, ...) {
    d <- data[indices, ]
    mod <- lm(mpg ~ am + hp + factor(cyl), data = d)
    cmp <- comparisons(mod, newdata = d, vcov = FALSE, variables = "am")
    tidy(cmp)$estimate
}

b <- boot(data = mtcars, statistic = bootfun, R = 1000)

b
boot.ci(b, type = "perc")
```

Note that, in the code above, we set `vcov=FALSE` to avoid computation of delta method standard errors and speed things up.

Compare to the delta method standard errors:

```{r}
mod <- lm(mpg ~ am + hp + factor(cyl), data = mtcars)
avg_comparisons(mod, variables = "am")
```

# Mixed effects models: Satterthwaite and Kenward-Roger corrections

For linear mixed effects models we can apply the Satterthwaite and Kenward-Roger corrections in the same way as above:

```{r, message = FALSE}
library(marginaleffects)
library(patchwork)
library(lme4)

dat <- mtcars
dat$cyl <- factor(dat$cyl)
dat$am <- as.logical(dat$am)
mod <- lmer(mpg ~ hp + am + (1 | cyl), data = dat)
```

Marginal effects at the mean with classical standard errors and z-statistic:

```{r}
slopes(mod, newdata = "mean")
```

Marginal effects at the mean with Kenward-Roger adjusted variance-covariance and degrees of freedom:

```{r}
slopes(mod,
                newdata = "mean",
                vcov = "kenward-roger")
```

We can use the same option in any of the package's core functions, including:

```{r}
plot_predictions(mod, condition = "hp", vcov = "satterthwaite")
```

# Numerical derivatives

Numerical derivatives are used to obtain both first and second derivatives: (1) estimate the partial derivatives in the `slopes()` function, (2) estimate standard errors using the delta method for the vast majority of quantities generated by this package.

The slopes are always computed using a centered finite difference approach:

$$\frac{f(x + \varepsilon / 2) - f(x - \varepsilon / 2)}{\varepsilon}$$

Users can adjust the size of $\varepsilon$ using the `eps` argument. For example:

```{r}
library(marginaleffects)
dat <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv")
dat$large_penguin <- ifelse(dat$body_mass_g > median(dat$body_mass_g, na.rm = TRUE), 1, 0)
mod <- glm(large_penguin ~ bill_length_mm * flipper_length_mm + species,
           data = dat, family = binomial)

Q
pkgload::load_all()

summary(margins::margins(mod, variables = "bill_length_mm", eps = 1e-4))$SE
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-4)$std.error

summary(margins::margins(mod, variables = "bill_length_mm", eps = 1e-7))
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-7)

summary(margins::margins(mod, variables = "bill_length_mm"))
avg_slopes(mod, variables = "bill_length_mm")

library(margins)

#### Unit SEs
nd <- datagrid(model = mod)

# default
margins::margins(mod, data = nd, variables = "bill_length_mm", unit_ses = TRUE)$SE_dydx_bill_length_mm
slopes(mod, newdata = nd, variables = "bill_length_mm")$std.error

ma <- mm <- NULL
for (i in -(1:10)) {
  ma <- c(ma, margins::margins(mod, data = nd, variables = "bill_length_mm", unit_ses = TRUE, eps = 10^i)$SE_dydx_bill_length_mm)
  mm <- c(mm, slopes(mod, newdata = nd, variables = "bill_length_mm", eps = 10^i)$std.error)
}
data.frame(ma, mm)

Q
pkgload::load_all()

nd <- head(dat, 1)
slopes(mod, newdata = nd, variables = "bill_length_mm", eps = 10^-8) |> attr("jacobian") -> Jmm

margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, data = nd, eps = 1e-8) |> attr("jacobian") -> Jma

slopes(mod, newdata = nd, variables = "bill_length_mm", eps = 10^-8)

margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, data = nd, eps = 1e-8) |> data.frame()

b <- beps <- coef(mod)
beps[1] <- beps[1] + 1e-8 
#beps %*% 
mm <- model.matrix(mod)[1, , drop = FALSE]
(insight::link_inverse(mod)(mm %*% beps) - insight::link_inverse(mod)(mm %*% b)) / 1e-8


sqrt(Jmm %*% vcov(mod) %*% t(Jmm))
sqrt(Jma %*% vcov(mod) %*% t(Jma))

margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, data = nd, eps = 1e-8)$SE_dydx_bill_length_mm |>
  attr("jacobian")
   print()

|> attr("jacobian")

(k$predicted_hi - k$predicted_lo) / 10^-8

margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, data = nd, eps = 1e-15)

# 1e-4
pkgload::load_all()
margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, eps = 1e-4)$SE_dydx_bill_length_mm |> head()
slopes(mod, variables = "bill_length_mm", eps = 1e-4)$std.error |> head()

summary(margins::margins(mod, variables = "bill_length_mm", eps = 1e-5))$SE
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-5)$std.error

slopes(mod, variables = "bill_length_mm") |> head() |> data.frame()


# 1e-7
margins::margins(mod, variables = "bill_length_mm", unit_ses = TRUE, eps = 1e-7)$SE_dydx_bill_length_mm |> head()
slopes(mod, variables = "bill_length_mm", eps = 1e-7)$std.error |> head()

summary(margins(mod, variables = "bill_length_mm"))$SE |> print()
avg_slopes(mod, variables = "bill_length_mm")$std.error |> print()

for (i in -(1:10)) {
  summary(margins(mod, variables = "bill_length_mm", eps = 10^i))$SE |> print()
}

pkgload::load_all()
for (i in -(1:10)) {
  avg_slopes(mod, variables = "bill_length_mm", eps = 10^i)$std.error |> print()
}

margin
```

When the user does not specify the value of `eps`, `marginaleffects` uses a simple heuristic: `eps` is set to `1e-4` times the range of the variable with respect to which we are taking the derivative. In many (but not all) cases, the choice of `eps` for the first derivative will not affect the results too much.

To compute the second derivatives needed to estimate standard errors, the default approach is the forward finite difference approach:

$$\frac{f(x + \varepsilon) - f(x)}{\varepsilon}$$

The forward difference approach is cheaper than the centered difference -- which is useful in models with many parameters -- but it can also be a bit less accurate. By default, `marginaleffects` uses a similar heuristic as above to set the step size: `max(1e-8, 1e-4*min(abs(coef(model)))`.

Standard errors can be very sensitive to the choice of step size, especially in models with non-linear components and/or interactions. Analysts are strongly encouraged to explore different values of the `eps` argument. The first standard error printed below is generated by the default step size, and the others with different powers of 10:

```{r}
for (eps in c(list(NULL), 10^-(4:10))) {
  print(avg_slopes(mod, variables = "bill_length_mm", eps = eps)$std.error)
}
```

`marginaleffects` also allows users to delegate the computation of second derivatives to the `numDeriv` package, which is more flexible. To do this, we set the `marginaleffects_numDeriv` global option to a list of arguments, which are passed to the `numDeriv::jacobian()` function. The `marginaleffects_numDeriv`

```{r}
avg_slopes(mod, variables = "bill_length_mm")$std.error
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-1)$std.error
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-4)$std.error
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-7)$std.error
avg_slopes(mod, variables = "bill_length_mm", eps = 1e-10)$std.error

summary(margins::margins(mod, variables = "bill_length_mm", eps = 1e-4))
summary(margins::margins(mod, variables = "bill_length_mm", eps = 1e-7))

pkgload::load_all()
avg_slopes(mod)

options(marginaleffects_diff_richardson = NULL)
options(marginaleffects_diff_richardson = list())
```


# Bayesian estimates and credible intervals

[See the `brms` vignette](https://vincentarelbundock.github.io/marginaleffects/reference/brms.html) for a discussion of bayesian estimates and credible intervals.


<!--
# Uncertainty around unit-level or average estimates

Note that it is normal to observe that confidence intervals around average estimates tend to be tighter than those around unit-level estimates:

```{r}
library(marginaleffects)
mod <- lm(mpg ~ hp * qsec, mtcars)

# On average CIs around unit-level estimates are:
pre1 <- predictions(mod)
as.numeric(mean(pre1$conf.high - pre1$conf.low))

# The CI of the average estimate is:
pre2 <- tidy(predictions(mod))
as.numeric(pre2$conf.high - pre2$conf.low)

# On average CIs around unit-level estimates are:
cmp1 <- comparisons(mod, variables = "hp")
mean(cmp1$conf.high - cmp1$conf.low)

# The CI of the average estimate is:
cmp2 <- comparisons(mod, variables = "hp", transform_pre = "differenceavg")
cmp2$conf.high - cmp2$conf.low
```

Relevant?
https://en.wikipedia.org/wiki/Jensen%27s_inequality
https://math.stackexchange.com/questions/34009/is-the-variance-of-a-convex-function-a-convex-function/34021#34021?newreg=698ab30a481945878307935ac5d6b629


NO LONGER RELEVANT WITH THE AVERAGE() + RECALL() REFACTOR SINCE WE COMPUTE SEs DIRECTLY ON THE MARGINS W

## Standard Error of the Average Marginal Effect or Contrast

The `avg_*()` functions computes the average marginal effect (or contrast) when they are applied to an object produced by `slopes()` (or `comparisons()`). This is done in 3 steps:

1. Extract the Jacobian used to compute unit-level standard errors.
2. Take the average of that Jacobian.
3. Estimate the standard error of the average marginal effect as the square root of the diagonal of J'VJ, where V is the variance-covariance matrix.

As [explained succinctly on Stack Exchange:](https://stats.stackexchange.com/a/331311/4874)

> we want the variance of the Average Marginal Effect (AME) and hence our transformed function is: $AME =  \frac{1}{N} \sum_{i=1}^N g_i(x_i,\hat{\beta})$ Then using the delta method we have $Var \left( g(\hat{\beta}) \right) = J_g' \Omega_{\hat{\beta}} J_g$ where $\Omega_{\hat{\beta}} = Var(\hat{\beta})$ and $J_g' = \frac{\partial\left[\frac{1}{N}\sum_{i=1}^N g (x_i,\hat{\beta})\right]}{\partial \hat\beta} = \frac{1}{N}{\left[\sum_{i=1}^N \frac{\partial \left (g (x_i,\hat{\beta})\right)}{\partial \hat\beta}\right]}$ Which justifies using the "average Jacobian" in the delta method to calculate variance of the AME.

References:

* Dowd, Bryan E, William H Greene, and Edward C Norton. “Computation of Standard Errors.” Health Services Research 49, no. 2 (April 2014): 731–50. https://doi.org/10.1111/1475-6773.12122.
* https://stats.stackexchange.com/questions/283831/delta-method-for-marginal-effects-of-generalized-linear-model?rq=1
-->